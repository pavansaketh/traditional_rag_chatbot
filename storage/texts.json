[
  "Received 18 June 2024, accepted 9 July 2024, date of publication 11 July 2024, date of current version 23 July 2024.\nDigital Object Identifier 10.1 109/ACCESS.2024.3426955\nDeep Learning for Credit Card Fraud Detection:\nA Review of Algorithms, Challenges,\nand Solutions\nIBOMOIYE DOMOR MIENYE\n , (Member, IEEE), AND NOBERT JERE\nDepartment of Information Technology, Walter Sisulu University, Buffalo City Campus, East London 5200, South Africa\nCorresponding author: Ibomoiye Domor Mienye (imienye@wsu.ac.za)\nABSTRACT Deep learning (DL), a branch of machine learning (ML), is the core technology in today’s\ntechnological advancements and innovations. Deep learning-based approaches are the state-of-the-art\nmethods used to analyse and detect complex patterns in large datasets, such as credit card transactions.\nHowever, most credit card fraud models in the literature are based on traditional ML algorithms, and recently,",
  "However, most credit card fraud models in the literature are based on traditional ML algorithms, and recently,\nthere has been a rise in applications based on deep learning techniques. This study reviews the recent\nDL-based literature and presents a concise description and performance comparison of the widely used DL\ntechniques, including convolutional neural network (CNN), simple recurrent neural network (RNN), long\nshort-term memory (LSTM), and gated recurrent unit (GRU). Additionally, an attempt is made to discuss\nsuitable performance metrics, common challenges encountered when training credit card fraud models using\nDL architectures and potential solutions, which are lacking in previous studies and would benefit deep\nlearning researchers and practitioners. Meanwhile, the experimental results and analysis using a real-world\ndataset indicate the robustness of the deep learning architectures in credit card fraud detection.",
  "dataset indicate the robustness of the deep learning architectures in credit card fraud detection.\nINDEX TERMS Credit card, CNN, deep learning, fraud detection, GRU, LSTM, machine learning.\nI. INTRODUCTION\nCredit card transactions have grown due to the rapid\ntechnological advancements and convenience of electronic\nservices [1], [2]. Consequently, there has been an increase in\nsecurity issues, such as credit card fraud, which has become\na significant concern for both financial institutions and cus-\ntomers [3], [4]. According to the Nielsen report, losses from\ncredit card fraud in 2019, 2020, and 2021 were approximately\n28.65, 28.50, and 32.34 billion dollars, respectively [5], [6],\n[7]. Additionally, losses due to credit card fraud globally have\ntripled in the last decade, from 9.84 billion dollars in 2011 to\n32.34 billion dollars in 2021 [8].\nMachine learning (ML) methods have been widely used\nfor credit card fraud detection (CCFD), achieving state-of-",
  "32.34 billion dollars in 2021 [8].\nMachine learning (ML) methods have been widely used\nfor credit card fraud detection (CCFD), achieving state-of-\nthe-art performances [9], [10], [11]. ML algorithms can be\nclassified into supervised, unsupervised, semi-supervised,\nor reinforcement learning [12]. The most widely used ML\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Tony Thomas.\nmethod for identifying credit card fraud is the supervised\nlearning (SL) method [13]. Supervised learning entails\ntraining an ML algorithm using a dataset where each data\npoint has a label. The label indicates the specific class the data\npoint belongs to, such as fraud or not fraud. SL techniques\ntend to learn the relationship between the input features\n(or independent variables) and the output labels (dependent\nvariables).\nSeveral studies have demonstrated the ability of neural\nnetworks to identify fraudulent transactions in complex",
  "(or independent variables) and the output labels (dependent\nvariables).\nSeveral studies have demonstrated the ability of neural\nnetworks to identify fraudulent transactions in complex\ncredit card data [14], [15]. A neural network is a type of\nmachine learning with a learning process that mimics the\nhuman brain and can be supervised or unsupervised [16].\nNeural networks with multiple layers in the network\nalso called deep learning (DL), can progressively extract\nhigher-level features and analyse complex patterns with\nenhanced predictions. DL approaches have been used to\nidentify fraudulent transactions in credit card data. For\nexample, Mienye and Sun [17] developed an approach for\ncredit card fraud detection using a stacked ensemble of\nVOLUME 12, 2024",
  "2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/ 96893",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nlong short-term memory (LSTM) and gated recurrent unit\n(GRU) networks, with a multilayer perceptron (MLP) as the\nbase learner. The DL-based ensemble performed excellently\ncompared to other ML algorithms and the individual DL\narchitectures. Similarly, Esenogho et al. [18] proposed a\nDL-based approach for credit card fraud detection using the\nLSTM neural network as the base learner in the adaptive\nboosting (AdaBoost) implementation, achieving excellent\nclassification performance. Additionally, different studies\nhave used convolutional neural networks [19], [20].\nMeanwhile, recurrent neural networks (RNN) and their\nvariants, such as LSTM and GRU, are the most widely used\nDL-based networks for modelling and analysing credit card\ntransactions due to their ability to learn sequential data and\ndetect temporal relationships [21], [22], [23]. The LSTM\nnetwork is useful for learning long-term dependencies in a",
  "transactions due to their ability to learn sequential data and\ndetect temporal relationships [21], [22], [23]. The LSTM\nnetwork is useful for learning long-term dependencies in a\nsequence. It is powerful because it can remember information\nfrom previous time steps and selectively forget or update that\ninformation as new inputs are processed. Like LSTM, GRU\ncan selectively update or forget data from earlier time steps\ndue to its gating mechanism, making it suitable for time series\nmodelling.\nHowever, despite the robustness of deep learning tech-\nniques, there are certain benefits and limitations in using\nthem for credit card fraud detection. Therefore, this study\naims to review the application and role of deep learning in\ncredit card fraud detection. The significance of this study\nlies in its comprehensive review of the current state of\ndeep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.",
  "lies in its comprehensive review of the current state of\ndeep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.\nBy systematically analyzing various deep learning techniques\nand their performance, this study provides valuable insights\nfor researchers and practitioners. The main objectives and\ncontributions of this review are as follows:\n• A review of the most current research on credit card\nfraud detection, focusing on deep learning techniques.\n• A concise but comprehensive overview of the main\ndeep learning techniques used in CCFD and their\nperformance comparison.\n• A detailed evaluation of the widely used performance\nevaluation metrics, focusing on their suitability for\nCCFD.\n• An in-depth analysis of existing challenges in using\nDL-based techniques for credit card fraud detection,\npotential solutions, and research directions.\nFurthermore, to ensure a comprehensive and unbiased",
  "DL-based techniques for credit card fraud detection,\npotential solutions, and research directions.\nFurthermore, to ensure a comprehensive and unbiased\nreview, a systematic approach was employed for literature\ngathering. The literature selection began with a compre-\nhensive search of multiple academic databases, including\nIEEE Xplore, SpringerLink, ScienceDirect, and Google\nScholar. Keywords such as ‘‘credit card fraud detection,’’\n‘‘deep learning,’’ ‘‘CNN,’’ ‘‘RNN,’’ ‘‘LSTM,’’ and ‘‘GRU’’\nwere used to identify relevant studies. We included articles\npublished between 2015 and 2024 to capture the most recent\nadvancements and trends in the field. Studies were further\nfiltered based on relevance, impact, and their contribution to\nthe understanding of deep learning applications in credit card\nfraud detection.\nThe rest of the paper is structured as follows: section II\npresents a discussion of credit card fraud detection and related",
  "fraud detection.\nThe rest of the paper is structured as follows: section II\npresents a discussion of credit card fraud detection and related\nreviews. Section III discusses publicly available credit card\ndatasets, and Section IV presents a comprehensive overview\nof deep learning and relevant architectures. Section V\ndiscusses widely used performance evaluation metrics and\ntheir suitability for credit card fraud detection. Section VI\npresents a review of recent studies that applied deep learning\nfor credit card fraud detection. Section VII presents the\nexperimental results and analysis of the various DL methods,\nand Section VIII discusses the challenges of using DL to\ndetect credit card fraud and possible solutions. Section IX\npresents a general discussion and future research directions,\nand Section X concludes the study and highlights future\nresearch directions.\nII. RELATED WORKS\nA. CREDIT CARD FRAUD DETECTION\nCredit card fraud occurs when an unauthorised user obtains",
  "and Section X concludes the study and highlights future\nresearch directions.\nII. RELATED WORKS\nA. CREDIT CARD FRAUD DETECTION\nCredit card fraud occurs when an unauthorised user obtains\naccess to someone’s credit card details and performs trans-\nactions. It is an inclusive term for fraud committed via a\nbank card, including credit and debit cards [24]. Although\nthe transactions are frequently carried out online, they can be\ncarried out using the actual card when misplaced or stolen.\nFraudsters use different methods to obtain the cardholder’s\ninformation, including phishing, where a fraudster poses as\na financial official to coerce a user into disclosing personal\ninformation, and skimmers use an interface to an automated\nteller machine (ATM) or point-of-sale device that can read a\ncard directly [25], [26].\nDetecting credit card fraud is essential in ensuring the\nsecurity of consumers’ finances and financial information.\nThe two main approaches to detecting fraudulent activity",
  "card directly [25], [26].\nDetecting credit card fraud is essential in ensuring the\nsecurity of consumers’ finances and financial information.\nThe two main approaches to detecting fraudulent activity\nare automated systems and manual investigation. While\nautomated systems rely on algorithms and machine learning\ntechniques to identify patterns of fraudulent behaviour,\nmanual investigation involves human intervention to analyse\nsuspicious activities and gather evidence. Automated systems\nare more popular due to their ability to process large volumes\nof data quickly and efficiently, and they utilize advanced\nstatistical and machine learning models [27].\nFurthermore, machine learning algorithms, including neu-\nral networks, are widely employed for detecting credit\ncard fraud. For example, Mienye and Sun [28] proposed\na CCFD method using hybrid feature selection based on\ngenetic algorithm and information gain, and the learning\nalgorithm was the extreme learning machine (ELM). The",
  "a CCFD method using hybrid feature selection based on\ngenetic algorithm and information gain, and the learning\nalgorithm was the extreme learning machine (ELM). The\ngenetic algorithm’s fitness function employed in the study\nwas the geometric mean, which was used to tackle the\nclass imbalance problem, leading to improved classification\nperformance. Similarly, Karthik et al. [29] proposed a hybrid\nensemble approach for credit card fraud detection to solve\nthe imbalance class problem. The study combined boosting\n96894 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nand bagging methods, i.e., adaptive boosting (AdaBoost) and\nrandom forest, respectively, achieving superior performance\ncompared to the individual classifiers.\nRandhawa et al. [30] developed a hybrid ensemble based\non majority voting and adaptive boosting. They compared\nthe performance with some single classifiers, including\ndecision tree, support vector machines (SVM), and naïve\nBayes. The proposed hybrid method achieved the best\nMatthews Correlation Coefficient (MCC) score of 1. Other\nexamples of ML algorithms in credit card fraud detection\ninclude random forest [31], XGBoost [32], convolutional\nneural network (CNN) [33], [34], RNN [35], LSTM [22],\n[36], [37], GRU [38], and bidirectional gated recurrent unit\n(BiGRU) [39].\nMeanwhile, apprehending credit card scammers often\nrelies on the availability and quality of data. Law enforce-\nment agencies and financial institutions utilize transactional",
  "(BiGRU) [39].\nMeanwhile, apprehending credit card scammers often\nrelies on the availability and quality of data. Law enforce-\nment agencies and financial institutions utilize transactional\ndata, along with advanced machine learning algorithms,\nto identify suspicious patterns indicative of fraud [24].\nCollaboration between banks and cybersecurity firms enables\nreal-time monitoring and alerts, which are crucial in catching\nfraudsters. For instance, data-sharing agreements allow for\nthe aggregation of data across different banks, providing\na broader view of fraudulent activities. This collaborative\neffort enhances the detection and prevention of credit\ncard fraud, thereby increasing the chances of apprehending\nscammers [40]. Furthermore, anonymized and synthetic data\ngeneration techniques can be used to augment training\ndatasets, allowing models to better generalize and detect\nnew types of fraud, ultimately aiding in the apprehension of\nscammers.\nB. RELATED REVIEWS",
  "datasets, allowing models to better generalize and detect\nnew types of fraud, ultimately aiding in the apprehension of\nscammers.\nB. RELATED REVIEWS\nSeveral research works have examined fraud detection in\nmany reviews and surveys that have appeared in peer-\nreviewed articles. For instance, Modi and Dayma [41]\npresented reviews regarding the application of ML in\ndetecting credit card fraud. Lucas and Jurgovsky [42]\nexamined the difficulties in detecting credit card fraud. They\nconcentrated on methods proposed to handle the concept drift\nand imbalance problems, which are two major challenges\nfaced when analysing credit card transaction data. Concept\ndrift occurs when the statistical properties of the data used\nto train an ML model change over time. As a result, the\nmodel may function differently than intended or produce\nless accurate predictions. The review provided a detailed\ndiscussion of concept drift, imbalance classification and\napproaches to handling them.",
  "less accurate predictions. The review provided a detailed\ndiscussion of concept drift, imbalance classification and\napproaches to handling them.\nAl-Hashedi and Magalingam [43] provided a broad review\nof fraud detection, including insurance, credit card, and\nother financial fraud. The review described the ML methods\nused for the different fraud detection problems. Additionally,\ndatasets and performance evaluation metrics were discussed.\nAlso, the article lists the benefits and drawbacks of each ML\nmethod. Nevertheless, the review is limited to the following\nML techniques: SVM, logistic regression, artificial neural\nnetwork, k-nearest neighbor (KNN), GA, Bayesian network,\ndecision tree, fuzzy logic, and hidden Markov model.\nPopat and Chaudhary [44] examined several ML-based\nCCFD studies, focusing on the difficulties encountered by\nthe ML models when detecting fraud. The methods studied\ninclude logistic regression, SVM, decision tree, ANN, and",
  "CCFD studies, focusing on the difficulties encountered by\nthe ML models when detecting fraud. The methods studied\ninclude logistic regression, SVM, decision tree, ANN, and\nBayesian Belief Network.. Ryman-Tubb et al. [45] conducted\na review and analysed current techniques for detecting card\nfraud via transactional volumes. The methods reviewed\ninclude SVM, KNN, CNN, MLP, decision tree, and random\nforest. Pandey et al. [46] reviewed CCFD, focusing on the\ndifferent types and statistics of credit card fraud in India.\nAlamri and Ykhlef [47] presented a survey of credit\ncard fraud detection studies that employed sampling tech-\nniques after identifying the imbalance class problem as\nthe main challenge researchers face when building CCFD\nmodels. The study considered oversampling techniques, such\nas synthetic minority oversampling technique (SMOTE)\nand Borderline-SMOTE, undersampling methods, such as\nrandom undersampling (RUS) technique and Tomek links,",
  "as synthetic minority oversampling technique (SMOTE)\nand Borderline-SMOTE, undersampling methods, such as\nrandom undersampling (RUS) technique and Tomek links,\nand hybrid sampling methods, such as SMOTE-ENN and\nSMOTE-Tomek links. The study identified hybrid sampling\nmethods as more efficient in handling the imbalance class\nproblem in CCFD, while noting that oversampling techniques\ncan lead to overfitting and undersampling can discard\nessential samples.\nWhile several reviews examine credit card fraud detection\nsystems, most of them have a very narrow scope, such\nas those focusing on sampling techniques [47], where the\nauthors specifically reviewed studies that aimed to solve\nthe imbalance problem in credit card fraud detection using\nresampling methods, showing the importance of effective\ndata resampling. Meanwhile, some of the reviews have a\nbroad scope, including [41], [43], and [44]. While they\ntouched on vital areas of fraud detection, they have some",
  "data resampling. Meanwhile, some of the reviews have a\nbroad scope, including [41], [43], and [44]. While they\ntouched on vital areas of fraud detection, they have some\nlimitations. For instance, Modi and Dayma [41] focused on\nperformance evaluation of the machine learning algorithms\nwithout delving deep into the inner workings of the algo-\nrithms, Al-Hashedi and Magalingam [43] only surveyed the\nperiod 2009 to 2019, and Popat and Chaudhary [44] reviewed\nselected ML algorithms. Meanwhile, credit card fraud has\nincreased considerably in recent years, and considering the\nrobustness of deep learning methods in different areas,\nit has become imperative to explore their applicability and\nperformance in credit card fraud detection. Therefore, this\nstudy aims to review deep learning methods and their\nperformance in detecting credit card fraud. In addition, this\nreview also covers specific gaps in related reviews, such\nas identifying and reviewing suitable evaluation metrics,",
  "performance in detecting credit card fraud. In addition, this\nreview also covers specific gaps in related reviews, such\nas identifying and reviewing suitable evaluation metrics,\nchallenges in building CCFD models, and potential solutions.\nIII. CREDIT CARD DATASETS\nDue to privacy and security concerns, credit card datasets\nare not easily accessible. However, there are a few publicly\navailable credit card datasets that are used for fraud detection,\nVOLUME 12, 2024 96895",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nand they are described in this section. Meanwhile, other\npublicly available credit card datasets are not considered in\nthis section since they were not curated for fraud detection.\nFor example, the Taiwan and Australian credit card datasets\nwere designed for credit card default and risk prediction. The\nfraud detection datasets include the following:\n• European credit card dataset: The European credit card\ndataset [48] has been widely used by researchers in\nbuilding robust CCFD models. This dataset contains\n284,807 transactions from European countries, which\nhave been labeled as either normal or fraudulent. Each\ntransaction includes 28 features, such as time of the\ntransaction, amount, and various anonymized variables.\nThe dataset has become a benchmark for evaluating\nthe performance of fraud detection algorithms. Of the\n284,807 transactions, only a tiny fraction (0.17%)",
  "The dataset has become a benchmark for evaluating\nthe performance of fraud detection algorithms. Of the\n284,807 transactions, only a tiny fraction (0.17%)\nbelong to the positive class (i.e., fraud transactions),\nwhile the majority class (99.83%) represents the neg-\native class or legitimate transactions. This imbalanced\ndistribution poses a significant challenge for many\nmachine learning algorithms and requires careful con-\nsideration during model development. The dataset was\nreleased in 2013, and while it is older, it remains relevant\nfor current research due to its comprehensive nature.\n• Brazilian credit card dataset: This dataset was obtained\nfrom a large Brazilian bank, and it contains 374,823\ntransactions [29]. The fraud samples comprise 3.74% of\nthe records. Each record in the dataset has 17 numerical\nfeatures, including merchant category code, post/zip\ncode of current and previous transactions, current\ntransaction amount, previous transaction amount, trans-",
  "features, including merchant category code, post/zip\ncode of current and previous transactions, current\ntransaction amount, previous transaction amount, trans-\naction type (card present), credit limit, card type (e.g.,\nMastercard, Visa, Diners), local/international transac-\ntion, previous transaction fraud score, and time since\nlast transaction. Despite its age, this dataset provides\nvaluable insights into transaction patterns and fraud\ndetection.\n• IEEE-CIS Fraud Detection Dataset: Released in 2019,\nthe IEEE-CIS dataset is one of the more recent publicly\navailable datasets [49]. It contains transaction data\nprovided by Vesta Corporation and includes a mix of\nfraud and non-fraud transactions over a period. The\ndataset consists of two files: one with identity infor-\nmation and another with transaction details, comprising\napproximately 590,000 transactions. Features include\ndevice type, device information, card information,\ntransaction amount, and timestamp. The dataset is",
  "approximately 590,000 transactions. Features include\ndevice type, device information, card information,\ntransaction amount, and timestamp. The dataset is\nhighly imbalanced, with a small fraction representing\nfraudulent transactions.\n• PaySim Synthetic Dataset: PaySim is a synthetic dataset\ngenerated using a simulation based on real transaction\ndata [50]. Although it is not real-world data, it was\ncreated to mimic the transaction behaviors found in a\nreal financial institution. Released in 2017, the dataset\nincludes features such as transaction type, amount,\nbalance, and origin and destination accounts. PaySim is\nvaluable for its realistic simulation of fraud scenarios,\nand it contains over 6 million transactions.\nIV. OVERVIEW OF DEEP LEARNING\nIn this Section, an overview of DL is presented, including\na detailed description of the widely used DL architectures.\nDeep learning, a branch of ML, maps input data to\nnew representations or generates predictions using neural",
  "a detailed description of the widely used DL architectures.\nDeep learning, a branch of ML, maps input data to\nnew representations or generates predictions using neural\nnetworks [51]. Meanwhile, neural networks consist of\ninterconnected neurons with weighted connections. The\nneuron converts its input into a single output by summing its\nweighted inputs using a non-linear activation function [52].\nThe network’s weight parameters are modified using gradient\ndescent optimisation, reducing the loss function, i.e., the\ndiscrepancy between the expected and actual outputs. A neu-\nral network can have one or more hidden layers. A neural\nnetwork with one hidden layer is often referred to as a shallow\nnetwork, while a network with many hidden layers is called\na deep neural network (DNN). Figure 9 shows a general\nshallow neural network (or simple ANN) and deep neural\nnetwork architectures, where the latter has multiple hidden\nlayers.\nFurthermore, deep learning is a broader term used to",
  "shallow neural network (or simple ANN) and deep neural\nnetwork architectures, where the latter has multiple hidden\nlayers.\nFurthermore, deep learning is a broader term used to\ndescribe ML techniques that are based on neural networks\nwith many layers (deep architectures). Deep learning can\nbe unsupervised, semi-supervised, or supervised [54]. Deep\nlearning methods perform better than shallow machine\nlearning methods in most applications with big and high-\ndimensional data [55], [56]. Additionally, the ability of deep\nlearning to achieve excellent performance when the data\nincreases sets it apart from conventional machine learning.\nBecause DL architectures can handle massive datasets to\ncreate an efficient data-driven model, it is beneficial when\nworking with large volumes of data, such as credit card\ntransaction data [56], [57].\nDeep-learning architectures include DNN, RNN, CNN,\ntransformers, and deep reinforcement learning. These DL",
  "working with large volumes of data, such as credit card\ntransaction data [56], [57].\nDeep-learning architectures include DNN, RNN, CNN,\ntransformers, and deep reinforcement learning. These DL\narchitectures have produced results that are as good as\nhuman expert performance and sometimes outperforming\nthe human experts in domains such as image recognition,\nnatural language processing, computer vision, and speech\nrecognition. Meanwhile, RNNs are well-suited for sequential\ndata modelling, such as credit card transactions, and are a\nsignificant focus of this study. Though RNNs can model\nsequence data effectively, they are challenging to train due\nto issues with vanishing and exploding gradients [58], which\nled Hochreiter and Schmidhuber [59] to develop the LSTM\nto tackle the vanishing gradients problem effectively. The\nGRU, first presented in [60], manages the data and performs\nLSTM-like tasks without requiring an additional memory",
  "to tackle the vanishing gradients problem effectively. The\nGRU, first presented in [60], manages the data and performs\nLSTM-like tasks without requiring an additional memory\nunit. The bidirectional variants of these networks are unique\nvariations that enable the system to forecast the current state\nmore accurately by utilising data from subsequent time steps\nin addition to earlier time steps. The following subsections\npresent brief but concise overview of these deep learning\nmethods.\n96896 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nFIGURE 1. Simple ANN vs deep learning [53].\nA. MULTILAYER PERCEPTRON\nMultilayer Perceptron is a feedforward artificial neural\nnetwork for supervised learning problems. MLP is considered\nthe foundation network of deep learning or deep neural\nnetworks. It is a fully connected network comprising an input\nlayer where input data is received, one or more hidden layers\nthat serve as the neural network’s computational engine, and\nan output layer that makes the prediction based on the given\ninputs [61]. Furthermore, backpropagation, a supervised\nlearning algorithm, is widely utilised for training the MLP.\nThe backpropagation is considered as the primary building\nblock of network networks. Meanwhile, the widely used error\nfunction in the MLP is the mean squared error, represented as\nfollows:\nE = 1\n2\nn∑\ni=1\n|| pi − ti ||2 (1)\nwhere n represents the sample size, while pi and ti represent",
  "function in the MLP is the mean squared error, represented as\nfollows:\nE = 1\n2\nn∑\ni=1\n|| pi − ti ||2 (1)\nwhere n represents the sample size, while pi and ti represent\nthe predicted and actual outputs for the i − th sample.\nMeanwhile, the MLP network uses activation functions to\ndetermine its output, and examples of the activation functions\ninclude Softmax, rectified linear unit (ReLu), Sigmoid, and\nhyperbolic tangent (Tanh) [62]. In training the MLP, different\noptimisation techniques can be used, including stochastic\ngradient descent (SGD) and adaptive moment estimation\n(Adam). Lastly, the MLP hyperparameters mainly need to\nbe tuned for optimal performance, and these hyperparameters\ninclude the number of neurons, hidden layers, and iterations.\nB. CONVOLUTIONAL NEURAL NETWORK\nThe convolutional neural network is a well-known deep\nlearning architecture with wide applications in image\nrecognition [63], [64], [65], achieving state-of-the-art per-",
  "The convolutional neural network is a well-known deep\nlearning architecture with wide applications in image\nrecognition [63], [64], [65], achieving state-of-the-art per-\nformances, and has recently been applied in several cCCFD\nmodels [66], [67]. It consists of neurons that have learnable\nweights and biases. Meanwhile, the CNN’s hidden layers\nare made up of convolutional, pooling, and fully connected\nlayers [68]. A CNN showing this multi-layer architecture\nis shown in Figure 2. The convolutional layer, CNN’s core\ncomponent, uses learnable filters to compute a convolution\noperation on the input. A set of feature maps is produced\nafter the convolution operation. The pooling layer is utilized\nto reduce the feature maps’ spatial dimensions [69]. After\nthe feature extraction and downsampling by the convolutional\nand pooling layers, respectively, their output is mapped by the\nfully connected layers to the final output of the CNN [70]. For",
  "the feature extraction and downsampling by the convolutional\nand pooling layers, respectively, their output is mapped by the\nfully connected layers to the final output of the CNN [70]. For\na classification problem like CCFD, this mapping returns the\nprobability for each class (fraud or not fraud).\nC. SIMPLE RNN\nConventional neural networks assume that each unit in the\ninput vectors is independent. As a result, sequential data\ncannot be predicted by the typical neural network. However,\nrecurrent neural networks are built to have time series\nmemory, making them suitable for processing sequential\ndata [72]. They can effectively model temporal dependencies\nin the data. Figure 3 shows a simple RNN architecture.\nThe autoregressive architecture of RNNs allows them to\nmaintain a hidden state that can capture information from\nprior time steps. This feature is significant when working\nwith sequential data like credit card transaction records. In the",
  "maintain a hidden state that can capture information from\nprior time steps. This feature is significant when working\nwith sequential data like credit card transaction records. In the\nsimple RNN implementation, the current hidden state ht is\ncomputed according to :\nht = tanh(Uxt + Wht−1) (2)\nwhere U is the matrix of trainable weights for the input xt ,\nW is the matrix of trainable weights for the previous hidden\nstate ht−1, and tanh is the activation function applied element-\nwise.\nD. LONG SHORT-TERM MEMORY\nThe LSTM network is a well-known RNN variant that\nwas developed primarily to solve the vanishing gradient\nissue associated with the simple RNN, which made it\nchallenging to identify long-term dependencies in the data.\nLSTMs have unique gating mechanisms that enable them to\nstore information better over longer sequences, as shown in\nFigure 4.\nVOLUME 12, 2024 96897",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nFIGURE 2. CNN Architecture [71].\nFIGURE 3. The architecture of Simple RNN.\nFIGURE 4. Architecture of the LSTM network [73].\nLSTM models have performed excellently in many time\nseries prediction applications, including credit card fraud\ndetection. It comprises three types of gates: forget, output,\nand input [18]. An LSTM’s forget gate decides what data\nfrom the previous hidden state should be kept or discarded.\nDuring training, the LSTM can focus on relevant inputs\nbetter and maintain a steady gradient by selectively ignoring\ncertain information. Only necessary information is added\nto the cell state because the input gate regulates new data\nflow into the cell state. The output gate then uses the\nupdated cell state and the input data to determine what\ninformation should be transmitted to the next block. The\ngating mechanism enables LSTM to extract the sequence’s\nFIGURE 5. A stacked LSTM.",
  "updated cell state and the input data to determine what\ninformation should be transmitted to the next block. The\ngating mechanism enables LSTM to extract the sequence’s\nFIGURE 5. A stacked LSTM.\nlong-term properties. Assuming xt is the input, the following\nfunctions are computed by the LSTM cell :\nit = σ(Vixt + Wiht−1 + bi) (3)\nft = σ(Vf xt + Wf h(t−1) + bf ) (4)\n˜ct = tanh(Vcxt + Wch(t−1) + bc) (5)\nct = ft ⊗ c(t−1) + it ⊗ ˜ct (6)\not = σ(Voxt + Woh(t−1) + bo) (7)\nht = ot ⊗ tanh(ct ) (8)\nwhere it , ft , ct , and ot denote the input, forget, cell, and output\ngates, respectively. Meanwhile, V∗, W∗, and b∗ represent the\nlearnable parameters, while h∗ represents the hidden state.\nFurthermore, σ is the sigmoid activation function and ⊗\ndenotes the element-wise product [74].\nThe standard LSTM network is made up of one hidden\nLSTM layer and a feedforward output later, but it can be\nextended to have many hidden layers and every layer to have",
  "The standard LSTM network is made up of one hidden\nLSTM layer and a feedforward output later, but it can be\nextended to have many hidden layers and every layer to have\nseveral memory cells, and this is called a stacked LSTM\nnetwork. By stacking the LSTM hidden layers, the network\nbecomes deeper, which is important because the success\nof deep learning models has been linked to how deep the\nnetwork is [75]. A general block diagram of a stacked LSTM\nis shown in Figure 5.\nE. GATED RECURRENT UNIT\nThe GRU was introduced by Cho et al. [60]. They also have\ngating mechanisms that aid in managing the information\nflow inside the network but without an output gate. A model\ncan be trained using the GRU to keep previous information\nor remove irrelevant information. The GRU architecture is\nshown in Figure 6. In contrast to LSTMs, GRUs have a\nsimpler architecture with just two gates: an update gate zt and\na reset gate rt . The reset gate regulates how much the previous",
  "shown in Figure 6. In contrast to LSTMs, GRUs have a\nsimpler architecture with just two gates: an update gate zt and\na reset gate rt . The reset gate regulates how much the previous\nhidden state influences the current hidden state, whereas the\nupdate gate controls how much the prior hidden state is\nkept [76]. With a less complex and more computationally\n96898 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nefficient network than LSTMs, GRUs are able to capture\nlong-term dependencies in sequential data more effectively\nbecause of this gating mechanism. The functions that a GRU\ncell computes are as follows :\nrt = σ(Vr xt + Wr h(t−1) + br ) (9)\nzt = σ(Vzxt + Wzh(t−1) + bz) (10)\n˜ct = tanh(Vcxt + Wc(rt ⊗ ht−1) + bc) (11)\nct = (1 − zt ) ⊗ c(t−1) + zt ⊗ ˜ct (12)\nht = ct (13)\nwhere Wr , Wz, Vr , and Vz are weight matrices while br and\nbz represent the bias vectors [77].\nFIGURE 6. Architecture of the GRU network.\nF. BIDIRECTIONAL LONG SHORT-TERM MEMORY\nUnidirectional Long Short-Term Memory, or LSTM, only\nstores past data since its inputs are limited to the past and must\nbe fed in the correct order. On the other hand, a bidirectional\nlong short-term memory network (BiLSTM) combines two\nhidden states to process the inputs in both forward and\nbackward directions, as shown in Figure 7. This feature",
  "long short-term memory network (BiLSTM) combines two\nhidden states to process the inputs in both forward and\nbackward directions, as shown in Figure 7. This feature\nallows the network to store information from incoming\ninputs, guaranteeing that information about previous and\nupcoming states is always accessible. In other words,\na BiLSTM is precisely like an LSTM, except that it employs\nhistorical and future data to compute the weights [17]. The\nBiLSTM’s network structure comprises two LSTMs with\nopposite information propagation directions. At each time\nunit, the current pre-hidden state output and post-hidden\nstate output are derived and recorded. The BiLSTM’s output\nvalue is then determined by connecting the two hidden states.\nThe mathematical formulation of the BiLSTM network is as\nfollows:\nhf\nt = LSTM(xt , hf\nt−1) (14)\nhb\nt = LSTM(xt , hb\nt−1) (15)\not = Wf .hf\nt + Wb.hb\nt + b (16)\nwhere LSTM(·) represent the mapping of the LSTM layers,",
  "follows:\nhf\nt = LSTM(xt , hf\nt−1) (14)\nhb\nt = LSTM(xt , hb\nt−1) (15)\not = Wf .hf\nt + Wb.hb\nt + b (16)\nwhere LSTM(·) represent the mapping of the LSTM layers,\nwhile Wf and Wb are the weight matrix of the forward and\nbackward LSTM layers, and b is the output layer’s deviation\nvector. Furthermore, BiLSTM models are substantially more\nefficient in natural language processing and can outperform\nconventional unidirectional LSTMs in time series predic-\ntion [78]. Because of its dual model architecture, BiLSTMs\nhave the drawback of requiring longer training times.\nFIGURE 7. The architecture of the BiLSTM network [78].\nG. BIDIRECTIONAL GATED RECURRENT UNIT\nBidirectional gated recurrent unit (Bi-GRU) is a variant\nof the popular GRU recurrent neural network designed to\nimprove temporal modelling accuracy. It is an example\nof a bi-directional RNN, meaning it can process input\nsequences in both forward and backward directions. In recent\nyears, Bi-GRU has become a popular choice for temporal",
  "of a bi-directional RNN, meaning it can process input\nsequences in both forward and backward directions. In recent\nyears, Bi-GRU has become a popular choice for temporal\nmodelling in deep learning applications. It is an efficient\nmodel that combines forward and backwards information\npropagation to improve accuracy when predicting future\nevents or sequences. Its main strengths over GRU are its\nability to capture bidirectional dependencies and its improved\nperformance in tasks involving long-term dependencies.\nIn the Bi-GRU implementation, the input sequences are\ncomputed in both directions using two sublayers, modelling\nforward and backward hidden sequences, which are com-\nbined to obtain the current hidden state ht and output ot of the\nnetwork [79]. The mathematical formulation is represented\nby the following :\nhf\nt = GRU(xt , hf\nt−1) (17)\nhb\nt = GRU(xt , hb\nt−1) (18)\nht = Wf .hf\nt + Wb.hb\nt ) (19)\not = φ(W oht ) (20)\nwhere hf\nt and hb\nt represent the forward and backward",
  "by the following :\nhf\nt = GRU(xt , hf\nt−1) (17)\nhb\nt = GRU(xt , hb\nt−1) (18)\nht = Wf .hf\nt + Wb.hb\nt ) (19)\not = φ(W oht ) (20)\nwhere hf\nt and hb\nt represent the forward and backward\nhidden sequences, the GRU function denotes the nonlinear\ntransformation of the input, W o represents the weight\ncoefficient in the network’s hidden and output layers, and φ\ndenotes the activation function applied to the output layer.\nH. TRANSFORMER MODELS\nWhile traditional deep learning architectures, such as CNNs,\nLSTM, and GRU, have been widely used in fraud detection\nand have achieved excellent performance, they have limita-\ntions, particularly in capturing long-range dependencies and\nprocessing large-scale datasets efficiently. Transformer mod-\nels have recently gained attention in the field of credit card\nVOLUME 12, 2024 96899",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nfraud detection due to their robust performance in sequence\nmodelling and anomaly detection tasks. Unlike traditional\nRNNs and CNNs, Transformers employs a self-attention\nmechanism that allows them to weigh the importance of\ndifferent parts of an input sequence dynamically. The core\ncomponent of a Transformer model is the self-attention\nmechanism, which computes attention scores for each pair of\ntokens in the input sequence [80]. The attention mechanism\nis defined as follows :\nAttention(Q, K, V ) = softmax\n(QKT\n√dk\n)\nV (21)\nwhere Q (queries), K (keys), and V (values) are the input\nmatrices, and dk is the dimension of the keys. The softmax\nfunction ensures that the attention scores sum to one,\nhighlighting the most relevant tokens [81]. Meanwhile, the\nTransformer model uses multiple self-attention heads to\ncapture different aspects of the relationships within the input\nsequence:",
  "highlighting the most relevant tokens [81]. Meanwhile, the\nTransformer model uses multiple self-attention heads to\ncapture different aspects of the relationships within the input\nsequence:\nMultiHead(Q, K, V )=Concat(head1, head2, . . . ,headh)W O\n(22)\nwhere each attention head head i is computed as :\nheadi = Attention(QW Q\ni , KW K\ni , VW V\ni ) (23)\nwhere W Q\ni , W K\ni , and W V\ni are learned projection matrices,\nand W O is the output projection matrix. Furthermore, Trans-\nformers can be pre-trained on large datasets and fine-tuned\non specific fraud detection tasks, leveraging transfer learning\nto improve performance. The pre-training phase typically\ninvolves learning general representations from a large corpus\nof transaction data, while the fine-tuning phase adapts these\nrepresentations to the specific characteristics of fraudulent\ntransactions.\nLtotal = Lpre-train + λLfine-tune (24)\nwhere Lpre-train is the loss during the pre-training phase,",
  "representations to the specific characteristics of fraudulent\ntransactions.\nLtotal = Lpre-train + λLfine-tune (24)\nwhere Lpre-train is the loss during the pre-training phase,\nLfine-tune is the loss during the fine-tuning phase, and λ is a\nweighting factor.\nV. PERFORMANCE EVALUATION METRICS\nAn essential step in ensuring effective credit card fraud\ndetection is the performance metrics used to assess the\nmodel’s prediction performance. This section discusses\nmetrics that are widely applied and their suitability in\ncredit card fraud detection. The confusion matrix provides a\nsummary of the binary classification results, and it is shown\nin Table 1, indicating true Positive (TP), true negative (TN),\nfalse position (FP), and false negative (FN).\nAccuracy = TP + TN\nTP + TN + FP + FN (25)\nErrorRate = 1 − Accuracy (26)\nWhen assessing the performance of ML models, the\nmost commonly utilised measures are accuracy and error\nTABLE 1. Confusion matrix.",
  "TP + TN + FP + FN (25)\nErrorRate = 1 − Accuracy (26)\nWhen assessing the performance of ML models, the\nmost commonly utilised measures are accuracy and error\nTABLE 1. Confusion matrix.\nrate [82], [83], i.e., Equation 25 and Equation 26, respectively.\nHowever, when dealing with CCFD, which is mostly an\nimbalance classification task, neither is sufficient because the\nmajority class, or the non-fraud class, dominates the final\nvalue. Hence, a naïve classifier can achieve 99% accuracy\nby labelling all samples as not fraud when given input data\nwhere the positive class distribution makes up only 1% of the\ndata set. There would be no actual benefit to such a model.\nOther commonly used metrics are sensitivity, specificity, and\nprecision, and their mathematical formulations are shown\nbelow:\nPrecision = TP\nTP + FP (27)\nSensitivity = TP\nTP + FN (28)\nSpecificity = TN\nTN + FP (29)\nPrecision represents the fraction of positively predicted\nsamples that are classified correctly. Because precision",
  "TP + FP (27)\nSensitivity = TP\nTP + FN (28)\nSpecificity = TN\nTN + FP (29)\nPrecision represents the fraction of positively predicted\nsamples that are classified correctly. Because precision\ntakes into account the number of negative instances that\nare wrongly predicted as positive, it is sensitive to class\nimbalance [84], [85]. However, precision on its own is\ninadequate as it does not reveal how many positive instances\nwere mistakenly classified as negative. Sensitivity, also\nknown as the true positive rate (TPR), quantifies the\nproportion of the positive instances that the classifier\naccurately predicted to be positive. The class imbalance has\nno effect on sensitivity since it solely depends on the positive\nclass. Meanwhile, the number of negative instances that are\nincorrectly classified as positive is not taken into account by\nsensitivity. Specificity, also called true negative rate (TNR),\nquantifies the proportion of the negative instances that were",
  "incorrectly classified as positive is not taken into account by\nsensitivity. Specificity, also called true negative rate (TNR),\nquantifies the proportion of the negative instances that were\nclassified correctly. Furthermore, there are other metrics\nthat tend to combine earlier discussed metrics to obtain a\nmore comprehensive evaluation of the model performance,\nincluding F-measure, G-mean, and balance accuracy. Their\nmathematical formulations are shown as follows :\nF − Measure = 2XPrecisionXSensitivity\nPrecision + Sensitivity (30)\nG − Mean =\n√\nSensitivityXSpecificity (31)\nBalancedAccuracy = Sensitivity + Specificity\n2 (32)\nF-measure is the harmonic mean of sensitivity and preci-\nsion. It presents a way to combine sensitivity and precision\ninto one metric that captures the properties of both metrics.\nThe geometric mean (G-mean) combines specificity and\nsensitivity into one metric that considers a balance between\nboth minority and majority class performances. Like G-mean,",
  "The geometric mean (G-mean) combines specificity and\nsensitivity into one metric that considers a balance between\nboth minority and majority class performances. Like G-mean,\nthe balanced accuracy metric computes a metric sensitive to\n96900 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nthe minority class instances by combining TPR and TNR.\nWhen evaluating the performance of classifiers trained with\nimbalanced data, F-measure, G-Mean, and balanced accuracy\nare better metrics compared to accuracy and error rate [86].\nFurthermore, the receiver operating characteristic (ROC)\ncurve, the area under the ROC curve (AUC), and the\nprecision-recall curve are other important metrics. The ROC\ncurve is a plot of the true positive rate versus the false positive\nrate at various classification thresholds, and it demonstrates\nthe ability of a classifier to distinguish between the positive\nand negative classes [65]. The AUC is a summary of the ROC\ncurve. It has a value range of 0 to 1, with 1 indicating that all of\nthe classifier’s predictions are accurate and 0 indicating that\nall of the predictions are incorrect. The precision-recall (PR)\ncurve demonstrates the tradeoff between precision and recall",
  "the classifier’s predictions are accurate and 0 indicating that\nall of the predictions are incorrect. The precision-recall (PR)\ncurve demonstrates the tradeoff between precision and recall\nat various thresholds [87]. Since high precision indicates a\nlow false positive rate, while high recall indicates a low false\nnegative rate, an area under the precision-recall curve with a\nhigh value indicates high recall and precision values.\nIn imbalance classification tasks, such as credit card\nfraud detection, the ROC curve can be misleading because\na small number of correct or wrong classifications can\nlead to a significant change in the ROC curve or AUC\nvalue. Meanwhile, the PR curve focuses on the minority\nclass, making it a more suitable metric for imbalance\nclassification [88]. Hence, the PR curve, together with\nother metrics previously discussed, is recommended for\nimbalanced credit card fraud detection.\nVI. DEEP LEARNING APPLICATIONS IN CREDIT CARD\nFRAUD DETECTION",
  "other metrics previously discussed, is recommended for\nimbalanced credit card fraud detection.\nVI. DEEP LEARNING APPLICATIONS IN CREDIT CARD\nFRAUD DETECTION\nBenchaji et al. [23] developed a CCFD model via sequential\nmodelling of the credit card data using deep LSTM neural\nnetworks and attention mechanisms. The proposed approach\ntakes into account the sequential nature of the credit card data\nand enables the classifier to determine which transactions\nin the input sequence are the most significant. Specifically,\nin the proposed approach, the LSTM was used to ensure\nsequential modelling of the data, the attention mechanism\nwas employed to improve the performance of the LSTM, and\nthe uniform manifold approximation and projection (UMAP)\nwas introduced to select the most significant attributes. The\nmodels yielded good performance with an accuracy of 96.7%.\nSimilarly, Femila et al. [89] developed a credit card fraud\ndetection model with the aim of lowering losses caused by",
  "models yielded good performance with an accuracy of 96.7%.\nSimilarly, Femila et al. [89] developed a credit card fraud\ndetection model with the aim of lowering losses caused by\ncredit card fraud. This study aimed to identify credit card\nfraud using an LSTM model. An attention mechanism was\nalso incorporated to boost the LSTM’s performance since\nmodels with such a structure have shown to be effective\nin sequence modelling. Other classifiers like SVM, naive\nBayes, and ANN were contrasted with the LSTM, and the\nexperimental results showed that the LSTM yielded robust\noutcomes, including an accuracy of 100%.\nNajadat et al. [90] developed a model based on BiLSTM\nand BiGRU with MaxPooling layers. Meanwhile, the dataset\nwas preprocessed using three resampling techniques: random\noversampling, random undersampling, and SMOTE. The\nstudy compared the performance of the deep learning-based\nclassifier and other ML classifiers, including logistic regres-",
  "oversampling, random undersampling, and SMOTE. The\nstudy compared the performance of the deep learning-based\nclassifier and other ML classifiers, including logistic regres-\nsion, random forest, voting, naïve base, AdaBoost, and\ndecision tree. When random oversampling was applied, the\nproposed BiLSTM-BiGRU obtained excellent performance,\nwith an AUC of 91.4%.\nForough and Momtazi [91] developed a credit card fraud\ndetection model that considers the sequential structure of\ncredit card transactions. The method used LSTM models\nas base classifiers in an ensemble implementation, where a\nfeed-forward neural network (FFNN) was used as the voting\nmechanism. The proposed LSTM ensemble outperformed\nother ML and DL techniques when experimented on two\ncredit card datasets. Specifically, the proposed ensemble\nachieved an AUC of 0.879 and 0.88 on the European and\nBrazilian datasets.\nAurna et al. [92] proposed a federated learning (FL)\nbased CCFD approach in order to protect the privacy of",
  "achieved an AUC of 0.879 and 0.88 on the European and\nBrazilian datasets.\nAurna et al. [92] proposed a federated learning (FL)\nbased CCFD approach in order to protect the privacy of\nsensitive credit card data. This allows the model to be trained\nwithout exposing credit card information to third parties on\nthe cloud. The study considered three deep learning models\nbased on LSTM, MLP, and CNN. The influence on the\nconventional centralised and FL systems is then examined\nusing four different sampling procedures to address the data\nimbalance problem. The proposed approach was compared\nwith other well-performing methods in the literature, and\nthe experimental results show that the proposed method\nobtains excellent performance with accuracies for CNN,\nMLP, and LSTM models being 99.51%, 98.77%, and 98.20%,\nrespectively.\nXie et al. [93] developed a time-aware attention-based\ninteractive LSTM (TAI-LSTM) method for credit card\nfraud detection. The method contains two time-aware gates,",
  "respectively.\nXie et al. [93] developed a time-aware attention-based\ninteractive LSTM (TAI-LSTM) method for credit card\nfraud detection. The method contains two time-aware gates,\na time-aware attention module and an interaction module.\nThe approach was built to capture the customer’s long\nand short-term spending behaviour and detect behavioural\nchanges over time. The time-aware attention model aims to\nextract behavioural information from the sequential credit\ncard data, while the interactive module aims to acquire\nmore thorough and logical representations. The findings\ndemonstrate that the learned representation can accurately\ndifferentiate between fraudulent and genuine behaviours and\nthat the suggested approach outperforms similar methods\nwith a sensitivity of 99.6%.\nSehrawat and Singh [21] used an auto-encoder with\nLSTM and GRU neural networks to detect credit card\nfraud. In the proposed approach, the autoencoder performed\nrepresentation learning from the data, which was achieved",
  "LSTM and GRU neural networks to detect credit card\nfraud. In the proposed approach, the autoencoder performed\nrepresentation learning from the data, which was achieved\nby excluding the class labels. The auto-encoder’s output\ncombined with the class labels were supplied as input to the\nLSTM and GRU models to detect fraud. The LSTM obtained\na classification accuracy of 99.1%.\nAjitha et al. [94] compared the performance of a CNN\nmodel with other ML algorithms, including XGBoost, SVM,\nrandom forest, KNN, logistic regression, and decision. The\nVOLUME 12, 2024 96901",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nCNN model consists of one flattened layer, one fully\nconnected layer, and two convolutional layers with the\nReLu activation function. The experimental results indicated\nthat the CNN obtained a classification accuracy of 97.2%,\noutperforming the other classifiers.\nYousuf Ali et al. [95] developed DL models combined with\nthe SMOTE oversampling approach to forecast credit card\nfraud. The paper employed three widely used deep learning\narchitectures: LSTM, CNN, and a DNN. The experimental\nresults showed that the CNN model achieved a significant\nincrease in accuracy after the SMOTE-based resampling,\nespecially in detecting fraud instances. The CNN obtained\nan accuracy of 99.9%. The study concluded that the CNN\narchitecture can aid in reducing financial losses due to credit\ncard fraud. Gambo et al. [19] also employed CNN for credit\ncard fraud detection, combining it with the adaptive synthetic",
  "architecture can aid in reducing financial losses due to credit\ncard fraud. Gambo et al. [19] also employed CNN for credit\ncard fraud detection, combining it with the adaptive synthetic\n(ADASYN) sampling method. After the resampling of the\ncredit card dataset, the CNN model achieved an accuracy\nof 99.8%.\nMizher and Nassif [96] presented credit card fraud\ndetection models based on the convolutional neural network\ntechnique and two machine learning algorithms: SVM\nand random forest. Using highly skewed real-world credit\ncard data, the models were assessed and contrasted, and\nthe random forest achieved the best performance with\nan accuracy of 99.7%. Meanwhile, the CNN obtained an\naccuracy of 93.5%.\nFurthermore, recent advancements in deep learning have\nseen the rise of Transformer-based models, which have\nrevolutionized various fields, including natural language\nprocessing and, more recently, fraud detection. Transformers,\nsuch as the Bidirectional Encoder Representations from",
  "revolutionized various fields, including natural language\nprocessing and, more recently, fraud detection. Transformers,\nsuch as the Bidirectional Encoder Representations from\nTransformers (BERT) model and its variants, have demon-\nstrated exceptional performance in sequence modelling and\nanomaly detection tasks due to their ability to capture\nlong-range dependencies and contextual information effec-\ntively. The application of Transformer models in credit card\nfraud detection offers several advantages over traditional\nDL architectures like LSTM and CNN. For example, their\nself-attention mechanism allows them to focus on the\nmost relevant parts of a transaction sequence, improving\nthe accuracy of fraud detection. Studies such as those\nof Igbal and Amin [97] and Tang and Liu [98] have\nexplored the application of Transformers in credit card fraud\ndetection, showing promising results, with accuracy of 100%\nand 98.98%, respectively. Table 2 summarises the deep",
  "explored the application of Transformers in credit card fraud\ndetection, showing promising results, with accuracy of 100%\nand 98.98%, respectively. Table 2 summarises the deep\nlearning methods reviewed in this study, comparing their\nperformances based on the accuracy metric.\nVII. EXPERIMENTAL RESULTS AND ANALYSIS\nIn this section, we analyze the performance of the MLP\nand deep learning architectures trained with the European\ncredit card dataset. The DL techniques include CNN, simple\nRNN, LSTM, GRU, BiLSTM, and BiGRU. To ensure a fair\ncomparison, the models were trained using the parameters\nlisted in Table 4. These parameters were chosen based on\ntheir widespread use in the literature and their effectiveness\nin similar tasks. To ensure robust evaluation, the models\nwere trained and validated using k-fold cross-validation.\nSpecifically, we used 5-fold cross-validation, where the\ndataset was randomly partitioned into five equal-sized\nsubsets. Each subset was used as a validation set once,",
  "Specifically, we used 5-fold cross-validation, where the\ndataset was randomly partitioned into five equal-sized\nsubsets. Each subset was used as a validation set once,\nwhile the remaining four subsets were used for training.\nThis process was repeated five times, and the performance\nmetrics were averaged over the five folds to provide a reliable\nestimate of model performance.\nTable 4 and Figure 8 show the performance of the various\nmodels in terms of accuracy, sensitivity, specificity, precision,\nand F-measure. Additionally, Figure 9 shows the ROC curves\nof the models. The results reveal interesting insights into the\nbehaviour of the models, especially regarding the near-perfect\naccuracy and specificity values compared to the relatively\nlower sensitivity and F-measure.\nFIGURE 8. Performance comparison.\nFIGURE 9. ROC curves of the various models.\nFirstly, all the models performed well in classifying the\nmajority class (non-fraud samples), indicated by the high",
  "FIGURE 8. Performance comparison.\nFIGURE 9. ROC curves of the various models.\nFirstly, all the models performed well in classifying the\nmajority class (non-fraud samples), indicated by the high\naccuracy and specificity scores. Specificity and accuracy\nmetrics reflect the model’s performance on the majority\nclass samples. Precisely, specificity measures the correctly\nclassified non-fraud samples or true negative rate, which\nwas very high due to the high number of negative samples.\nThe European credit card dataset is highly imbalanced,\n96902 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nTABLE 2. Summary of the DL-based credit card fraud studies.\nTABLE 3. Parameters of the various deep learning models.\nTABLE 4. Performance evaluation of the various classifiers.\nwith most records being legitimate and only a few being\nlabelled as fraud. Therefore, training the models on such an\nimbalanced dataset ensured they were efficient at classifying\nthe majority class samples, contributing to their high accuracy\nand specificity values,\nHowever, the GRU model achieved superior performance\nacross the various metrics, including having the best sensitiv-\nity. The GRU is known for its effectiveness in capturing tem-\nporal dependencies with lesser parameters compared to the\nLSTM, a possible reason for the high scores. Meanwhile, the\nhigh sensitivity value demonstrates that the GRU is relatively\nbetter at identifying fraud instances, which is crucial in credit",
  "LSTM, a possible reason for the high scores. Meanwhile, the\nhigh sensitivity value demonstrates that the GRU is relatively\nbetter at identifying fraud instances, which is crucial in credit\ncard fraud detection engines. Also, its balance between preci-\nsion and sensitivity, as indicated in the F-measure of 0.8254,\nimplies that the GRU efficiently manages the trade-off\nbetween detecting fraud instances and minimizing false\npositives.\nFurthermore, the MLP model achieved good performance,\nespecially with regard to accuracy and specificity. However,\nits sensitivity indicates a limitation in predicting fraud cases\ncompared to the GRU. Meanwhile, Simple RNN and CNN\nseem to have the lowest performance compared to the\nother models. The RNN achieved the least F-measure of\n0.772. RNNs are inefficient when faced with long-term\ndependencies due to the vanishing gradient issue, which\ncould explain the poor performance. The CNN obtained an\nF-measure of 0.780, which is better than the RNNs but less",
  "dependencies due to the vanishing gradient issue, which\ncould explain the poor performance. The CNN obtained an\nF-measure of 0.780, which is better than the RNNs but less\nthan the remaining models. Lastly, the discrepancy between\nthe high performance in the majority class samples and\nVOLUME 12, 2024 96903",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nthe poor performance in the minority class (fraud) samples\ncan be mainly attributed to the imbalanced credit card\ndata. In order to enhance the performance of the minority\nclass, researchers can explore methods such as oversampling,\nensemble learning, and cost-sensitive learning that penalize\nwrong predictions in the minority class more than the other\nclass.\nVIII. CHALLENGES AND POTENTIAL SOLUTIONS\nResearchers and practitioners usually encounter challenges\nwhen developing deep learning-based credit card fraud\ndetection models. In this section, an attempt is made to\ndiscuss some of these challenges and potential solutions.\nA. CLASS IMBALANCE\nIn binary classification problems, such as credit card fraud\ndetection, a class imbalance occurs when one class (also\ncalled the majority class) significantly outnumbers the other\nclass, known as the minority class. This results in a skewed",
  "detection, a class imbalance occurs when one class (also\ncalled the majority class) significantly outnumbers the other\nclass, known as the minority class. This results in a skewed\ndataset, making the learning process challenging for machine\nlearning algorithms. Most credit card transaction data have\na class imbalance, making it challenging to identify fraud.\nMeanwhile, the minority class (fraud samples) is more\nimportant than the majority class, and wrongly predicting\na fraud case as legitimate has a higher cost than predicting\na legitimate transaction as fraud [99]. Also, most machine\nlearning algorithms used for classification tasks assume that\nthere are equal amounts of samples in each class.\nFurthermore, class imbalance can also lead to biased\nand poor results. Therefore, addressing this problem is\nessential when building ML and DL models. Some meth-\nods to address the imbalance class problem include data\nresampling techniques like oversampling and undersampling,",
  "essential when building ML and DL models. Some meth-\nods to address the imbalance class problem include data\nresampling techniques like oversampling and undersampling,\nensemble methods that can handle imbalanced datasets,\nand cost-sensitive learning algorithms that assign different\nweights to minority and majority classes [100]. However,\nwhen building deep learning models for credit card fraud\ndetection, the following approaches have attracted a lot of\nattention from the DL community:\n• Loss function adaptation: This can be used to make deep\nlearning methods learn effectively from imbalanced\ndata. It involves changing the loss function of the DL\nmodel to make it insensitive to the skewed distribution.\nThe loss function adaptation is an algorithm-level\nmodification and has been successfully applied in\nseveral DL models [101], [102]. This approach is similar\nto cost-sensitive learning as it is based on the premise\nthat instances should not be treated equally during",
  "several DL models [101], [102]. This approach is similar\nto cost-sensitive learning as it is based on the premise\nthat instances should not be treated equally during\ntraining and that errors in minority class instances are\ncostlier than those in the majority class and, hence,\nshould be penalised more severely [103]. Based on this\nidea, Wang et al. [104] and Lin et al. [105] proposed\nmean false error and focal loss, two robust adapted\nloss functions for deep learning modelling. Other loss\nfunctions include generalised cross-entropy loss [106]\nand class-balanced loss [107], which were introduced\nmore recently.\n• Hybrid Models: Hybrid models that combine deep\nlearning algorithms with traditional machine learning\nalgorithms that are better suited for imbalance clas-\nsification, such as decision trees and random forests,\nhave been studied recently and have achieved excellent\nresults. For example, Dar et al. [108] developed a\nhybrid model, combining DNN and XGBoost and",
  "have been studied recently and have achieved excellent\nresults. For example, Dar et al. [108] developed a\nhybrid model, combining DNN and XGBoost and\nSemwal et al. [109] combined CNN with LSTM and\nGRU. The performance of hybrid methods has been\nshown to be superior to standard DL classifiers [110].\n• Ensemble methods: Ensemble learning can be employed\nin combining deep learning models. Additionally, the\nmodels can be trained on different resampled data.\nEnsemble techniques such as EasyEnsemble [111] and\nBalanced Bagging [112] are effective in creating ensem-\nble models that are well-suited to handle imbalanced\ndata.\nB. LACK OF SUFFICIENT DATA\nAt the moment, there are not enough real-world credit card\ndatasets to create reliable models for a variety of reasons,\nmostly pertaining to privacy concerns [113]. Also, most\navailable data are unlabelled. Hence, it takes extra effort\nto label the data. Therefore, a frequently used technique",
  "mostly pertaining to privacy concerns [113]. Also, most\navailable data are unlabelled. Hence, it takes extra effort\nto label the data. Therefore, a frequently used technique\nto identify fraud is anomaly detection. However, anomaly\ndetection depends on user behaviour, and any deviation can\nbe interpreted as fraud. Systems that detect anomalies rely on\nusers’ past behaviour, which has limitations.\nA potential approach used to solve this problem includes\ninstance generation using DNNs: Generative models based\non deep neural networks can be adapted to function similarly\nto oversampling methods, where artificial instances can be\neffectively introduced into a particular embedding space by\nan encoder/decoder pair. To learn the latent distribution of\ndata, researchers have successfully used generative adversar-\nial networks (GANs), variational autoencoders (V AEs), and\nWasserstein autoencoders (WAEs) [114]. These methods can\nbe expanded to generate more data for CCFD modelling.",
  "ial networks (GANs), variational autoencoders (V AEs), and\nWasserstein autoencoders (WAEs) [114]. These methods can\nbe expanded to generate more data for CCFD modelling.\nC. INTERPRETABILITY\nDeep learning models are often considered black-box models,\nmaking it challenging to interpret their results. Understanding\nwhy a transaction was classified as fraud or legitimate can be\ndifficult. Meanwhile, achieving complete interpretability in\nDL models can be difficult. The following techniques can be\nemployed to enhance the transparency and understandability\nof the deep learning model’s decision-making process,\nmaking it more useful in practical applications, such as credit\ncard fraud detection.\n• Regularization Techniques: Applying regularization\ntechniques, such as L1 regularization that encourages\nsparsity in the model’s parameters, could result in\nsimpler and more interpretable models [115].\n96904 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n• SHapley Additive exPlanations (SHAP): This method\nuses Shapley values from game theory in explaining a\ngiven prediction. The SHAP values assign contributions\nto every feature used in making the prediction [116].\nIt provides a method to understand the significance of\neach feature in the decision-making process of the deep\nlearning model.\n• Model Distillation: This technique involves transferring\nknowledge from a large model to a smaller model [117].\nExamples of large models include deep learning and\nensemble learning-based models. Using this approach,\na simpler and interpretable model can be trained on\nthe predictions of the DL model. Smaller models, such\nas decision trees and logistic regression, are easy to\ninterpret.\nD. DATA DRIFT\nMany ML models are built on the assumption that the data\ndistribution used in training and testing remains stationary.",
  "as decision trees and logistic regression, are easy to\ninterpret.\nD. DATA DRIFT\nMany ML models are built on the assumption that the data\ndistribution used in training and testing remains stationary.\nData drift, also known as covariate shift, occurs when the\ndistribution of the data used in training the model differs\nfrom the distribution of the data on which the model is\nbeing applied [118]. It is a common problem in many real-\nworld systems, such as credit card fraud detection. Therefore,\ncredit card data needs to be monitored regularly for changes\nin the statistical properties, and this can be achieved via\nvisualisation, statistical tests, and observing key metrics.\nModels trained on older data may not effectively detect new\nfraud patterns. Some of the methods used in solving this\nproblem include:\n• Incremental Learning: Incremental learning is an\napproach used to update a model with the latest data\nwhile retaining the learned knowledge from the previous",
  "problem include:\n• Incremental Learning: Incremental learning is an\napproach used to update a model with the latest data\nwhile retaining the learned knowledge from the previous\ntraining. This ensures the entire model is not retrained,\nand methods such as transfer learning, online learning,\nand fine-tuning can be used to achieve such incremental\nlearning [119].\n• Adaptive Learning Rate: The learning rate of deep\nlearning models can be adjusted during training to adapt\nto changes in the data distribution [119]. Specifically,\nlower learning rates can be used to ensure the model\nconverges to a new distribution without forgetting the\nprevious data.\n• Model retraining: A well-known method for handling\ndata drift is retraining the DL model with new data. Such\nretraining can be automated and set at regular intervals\nor manually triggered when sufficient drift is observed.\n• Ensemble modelling: Ensemble models can be used\nto combine the predictions from multiple DL models,",
  "or manually triggered when sufficient drift is observed.\n• Ensemble modelling: Ensemble models can be used\nto combine the predictions from multiple DL models,\nwhere one or more models can be used to determine\ndata drift and modify the ensemble model’s composition\naccordingly [120].\nE. PRIVACY AND SECURITY CONCERNS\nUsing credit card transaction data containing personal and\nfinancial information raises concerns about data breaches and\nunauthorized access. To address these concerns, researchers\nand practitioners must implement robust security measures,\nsuch as data anonymization and encryption, to ensure\nthe confidentiality and integrity of the data [121]. Data\nanonymization involves removing or obfuscating personally\nidentifiable information from the data while maintaining the\ncore patterns and characteristics necessary for training the\ndeep learning models. It can be achieved using generalization,\nsuppression, or perturbation techniques.",
  "core patterns and characteristics necessary for training the\ndeep learning models. It can be achieved using generalization,\nsuppression, or perturbation techniques.\nGeneralization involves substituting specific values with\nmore general categories or ranges [122]. For example,\ninstead of using exact transaction amounts, the data can be\ngrouped into ranges, such as <USD20, USD20-USD50, and\nUSD50-USD100, etc., preserving the overall distribution of\ntransaction amounts while protecting individual transaction\ndetails. Suppression entails removing sensitive attributes,\nsuch as credit card numbers and the cardholder’s name,\nensuring that no personal information is accessible. Another\nmethod for anonymizing sensitive financial data is perturba-\ntion. It entails adjusting the values of particular attributes or\nintroducing random noise [123]. In perturbation, transaction\namounts can be perturbed by adding a small random value to\neach amount, making it difficult to determine the exact values",
  "introducing random noise [123]. In perturbation, transaction\namounts can be perturbed by adding a small random value to\neach amount, making it difficult to determine the exact values\nwhile maintaining the data’s statistical properties.\nIn addition to data anonymization, encryption is crucial\nin ensuring the security of sensitive financial data used for\ntraining deep learning models. Encryption entails converting\nthe data into a format only accessible with the correct\ndecryption key [124]. It ensures that the data will remain\nunreadable and unusable even if it is intercepted or viewed\nwithout authorization. Asymmetric and symmetric encryp-\ntion are two examples of the different encryption methods\nthat can be used. Asymmetric encryption employs two keys:\na public key for encryption and a private key for decryption.\nSymmetric encryption uses a single key for both encryption\nand decryption.\nF. ETHICS AND FAIRNESS\nWhen constructing credit card fraud detection models using",
  "Symmetric encryption uses a single key for both encryption\nand decryption.\nF. ETHICS AND FAIRNESS\nWhen constructing credit card fraud detection models using\ndeep learning techniques, it is crucial to ensure that these\nmodels do not exhibit bias towards particular individuals\nor groups based on factors such as ethnicity, gender,\nor socioeconomic status [125]. One challenge in achieving\nfairness is the potential for bias in the training data. If the\ntraining data is skewed towards specific groups, the resulting\nmodel may likewise demonstrate bias in its predictions.\nFor instance, if the majority of the training data consists\nof fraudulent transactions from a particular demography,\nthe model can unfairly identify transactions from that\ndemographic as fraudulent, resulting in biased treatment.\nTo address this challenge, researchers and practitioners\nmust carefully curate the training data to ensure the inclusion\nof a wide range of demographic groups. This can be",
  "To address this challenge, researchers and practitioners\nmust carefully curate the training data to ensure the inclusion\nof a wide range of demographic groups. This can be\naccomplished by gathering data from diverse sources and\nensuring that the data is evenly distributed among different\nVOLUME 12, 2024 96905",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\ngroups. In addition, methods such as data augmentation can\nbe employed to artificially enhance the presence of under-\nrepresented groups in the training data. Another approach\nto addressing bias in deep learning models is the utilization\nof fairness metrics and algorithms. Fairness metrics measure\nthe degree of fairness or bias in the predictions made by\nthe model, whereas fairness algorithms aim to reduce any\nidentified bias [126]. For example, one approach involves\nmodifying the decision threshold of the model according\nto various demographic groupings to provide equivalent\nsensitivity to fraudulent transactions across all groups.\nG. ADAPTABILITY AND SCALABILITY\nAnother challenge is the adaptability and scalability of deep\nlearning-based credit card fraud detection models. Given the\never-changing nature of fraud, it is imperative for the models",
  "Another challenge is the adaptability and scalability of deep\nlearning-based credit card fraud detection models. Given the\never-changing nature of fraud, it is imperative for the models\nto be adaptable and have the ability to identify new and\nemerging patterns of fraudulent activity [127], [128]. One\nchallenge in achieving adaptability is the need for continuous\nmodel updates and retraining. Conventional machine learning\nmodels sometimes necessitate manual feature engineering\nand the retraining of models, which can consume significant\ntime and resources. However, deep learning models have\nthe ability to automatically learn and adapt to new patterns\nwithout requiring user intervention. Meanwhile, this requires\naccess to up-to-date and relevant data for training.\nPossible solutions to this problem include the use of\ntechniques such as transfer learning and online learning.\nTransfer learning is utilising pre-trained deep learning models",
  "Possible solutions to this problem include the use of\ntechniques such as transfer learning and online learning.\nTransfer learning is utilising pre-trained deep learning models\ntrained with large-scale datasets and fine-tuning them for the\nspecific objective of credit card fraud detection. It enables\nthe model to leverage the information and patterns acquired\nfrom the large datasets while adjusting to the particular\nfraud detection objective. Online learning enables the model\nto consistently update and acquire knowledge from newly\naccessible data. Online learning allows for incremental\nmodifications based on new data rather than retraining\nthe entire model from scratch [129]. This makes it more\nadaptable and scalable in detecting new fraud patterns.\nFurthermore, a vital aspect of the adaptability and scalability\nchallenge pertains to the computational resources necessary\nfor the training and deployment of deep learning models.",
  "Furthermore, a vital aspect of the adaptability and scalability\nchallenge pertains to the computational resources necessary\nfor the training and deployment of deep learning models.\nOrganisations with limited resources or infrastructure often\nface challenges while training deep learning models due to\nthe substantial data and processing power requirements.\nLastly, to tackle this issue, academics can investigate\nmethodologies like distributed computing and cloud com-\nputing. Distributed computing entails the distribution of\ncomputational tasks among numerous machines or nodes,\nenabling accelerated and more efficient training of deep\nlearning models. Distributed computing can be achieved\nby utilising parallel processing and distributed training\nframeworks [130]. Cloud computing enables users to access\nflexible and readily available computing resources via the\ninternet. Organisations can optimise the training and deploy-\nment of DL models by utilising cloud computing systems,",
  "flexible and readily available computing resources via the\ninternet. Organisations can optimise the training and deploy-\nment of DL models by utilising cloud computing systems,\nwhich enable them to flexibly adjust their computational\nresources according to their requirements.\nIX. DISCUSSIONS AND FUTURE RESEARCH DIRECTIONS\nDeep learning models have significantly transformed numer-\nous domains, including fraud detection. This research con-\ncisely describes the main deep learning-based architectures\nused for credit card fraud detection, including simple RNN,\nLSTM, GRU, BiLSTM, BiGRU, and CNN. The effectiveness\nof these models in real-world situations, particularly in the\ndynamic credit card fraud detection field, varies. Firstly,\nMLP has gained extensive usage in diverse applications due\nto its ability to learn complex patterns and make accurate\npredictions. However, its effectiveness in credit card fraud\ndetection needs has been examined and found to be limited.",
  "to its ability to learn complex patterns and make accurate\npredictions. However, its effectiveness in credit card fraud\ndetection needs has been examined and found to be limited.\nThe fundamental reason for this is that MLP does not possess\nthe ability to capture temporal dependencies and sequential\npatterns, which are essential in detecting fraudulent activities.\nOn the other hand, initially designed for image analysis,\nCNN has demonstrated encouraging results in detecting\ncredit card fraud, as shown in Table 2. By considering\nthe transaction data as a two-dimensional image, CNN can\neffectively extract relevant features and identify fraudulent\npatterns. However, it is also limited with regard to credit\ncard fraud detection. Furthermore, simple RNN, LSTM,\nGRU, BiLSTM, and BiGRU have been explored for\ncredit card fraud detection. Simple RNN, although capable\nof capturing temporal dependencies, has struggled with\nlong-term dependencies, limiting its effectiveness in this",
  "credit card fraud detection. Simple RNN, although capable\nof capturing temporal dependencies, has struggled with\nlong-term dependencies, limiting its effectiveness in this\ndomain. Conversely, LSTM has demonstrated exceptional\nperformance due to its ability to retain information over\nlong sequences, making it well-suited for credit card fraud\ndetection [18]. GRU, a variant of LSTM, has also shown\npromising results, combining the ability to retain information\nwith a simplified architecture. BiLSTM and BiGRU, which\nincorporate bidirectional processing, have been found to\nfurther improve the accuracy of fraud detection models by\nconsidering both past and future contexts.\nSeveral key conclusions can be drawn from this research.\nFirstly, deep learning architectures play a crucial role in\nefficiently detecting credit card fraud. The performance of the\nmodels differs with changes in the distribution of the samples.\nFor example, models trained with balanced datasets achieve",
  "efficiently detecting credit card fraud. The performance of the\nmodels differs with changes in the distribution of the samples.\nFor example, models trained with balanced datasets achieve\nmore robust performance than those trained with imbalanced\ndata. Therefore, effective data resampling and engineering\nshould be considered before model training. Also, optimizing\ndeep learning models to consider the imbalanced nature of the\ndata is beneficial.\nSecondly, though several deep learning-based architectures\nhave been employed for detecting credit card fraud, the\nfollowing architectures have been widely utilized: CNN,\nLSTM, GRU, and other RNN variants. Even though they can\nbe computationally expensive compared to traditional ML\nalgorithms, they usually achieve higher performance. Thirdly,\ndifferent research works have used single DL classifiers,\n96906 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nachieving excellent classification performance. However,\nsome researchers have explored hybrid deep learning models,\nwhich perform significantly better than single deep learning\nmodels. Also, the ensemble of deep learning models has led\nto superior performance compared to single deep learning\nmodels. However, it increases the computational complexity\nof the model.\nFurthermore, future research in credit card fraud detection\nusing deep learning can explore several promising avenues\nto enhance the robustness, accuracy, and applicability of\ndetection systems. One critical area is the development\nand implementation of hybrid and ensemble deep learning\narchitectures. Combining different models, such as LSTM\nwith CNN or GRU with Transformer models, can leverage\nthe strengths of each architecture to improve overall perfor-\nmance. These hybrid models can potentially provide more",
  "with CNN or GRU with Transformer models, can leverage\nthe strengths of each architecture to improve overall perfor-\nmance. These hybrid models can potentially provide more\naccurate detection by capturing both temporal dependencies\nand spatial features of transaction data. Additionally, ensem-\nble methods, which integrate multiple models’ predictions,\ncan enhance the system’s robustness by reducing the variance\nand bias associated with individual models.\nMoreover, while hybrid and ensemble models hold great\npromise, their practical deployment often faces challenges\nrelated to computational complexity and resource require-\nments. Future research can focus on optimizing these models\nto make them more efficient and scalable for real-world\napplications. Techniques such as model pruning, quantiza-\ntion, and the use of efficient neural network architectures\ncan significantly reduce computational overhead without\nsacrificing accuracy. Investigating the trade-offs between",
  "tion, and the use of efficient neural network architectures\ncan significantly reduce computational overhead without\nsacrificing accuracy. Investigating the trade-offs between\nmodel complexity and performance and developing adaptive\nmodels that can dynamically adjust their complexity based\non the available computational resources will be crucial for\ndeploying these advanced systems in operational environ-\nments.\nAnother important direction for future research is enhanc-\ning the interpretability and explainability of deep learning\nmodels in fraud detection. As these models become more\ncomplex, understanding their decision-making processes\nbecomes more challenging, yet it is essential for gaining trust\nfrom users and meeting regulatory requirements. Research\nshould focus on developing methods that can provide clear\nand actionable insights into how models make predictions.\nTechniques like attention mechanisms, SHapley Additive\nexPlanations, and layer-wise relevance propagation can",
  "and actionable insights into how models make predictions.\nTechniques like attention mechanisms, SHapley Additive\nexPlanations, and layer-wise relevance propagation can\nhelp explain the inner workings of deep learning models.\nAdditionally, integrating these interpretability methods with\nreal-time fraud detection systems will ensure that finan-\ncial institutions can respond quickly and transparently to\nfraudulent activities, thereby improving the overall security\nand trustworthiness of credit card transaction systems.\nX. CONCLUSION\nDeep learning methods have been widely applied in different\nfields due to their robustness and performance. Recently,\ndeep learning architectures have produced exceptional\nperformance in credit card fraud detection. This paper\npresents a comprehensive review of the current state of\ndeep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.\nThe study provides valuable insights for researchers and",
  "deep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.\nThe study provides valuable insights for researchers and\npractitioners and can guide the development of more robust\nand efficient fraud detection models, ultimately contributing\nto more secure financial transactions and reducing the\neconomic impact of fraud.\nREFERENCES\n[1] B. Lebichot, G. M. Paldino, W. Siblini, L. He-Guelton, F. Oblé, and\nG. Bontempi, ‘‘Incremental learning strategies for credit cards fraud\ndetection,’’Int. J. Data Sci. Anal. , vol. 12, no. 2, pp. 165–174, Aug. 2021.\n[2] X. Zhang, Y . Han, W. Xu, and Q. Wang, ‘‘HOBA: A novel feature\nengineering methodology for credit card fraud detection with a deep\nlearning architecture,’’ Inf. Sci., vol. 557, pp. 302–316, May 2021.\n[3] S. Bakhtiari, Z. Nasiri, and J. Vahidi, ‘‘Credit card fraud detection using\nensemble data mining methods,’’ Multimedia Tools Appl., vol. 82, no. 19,\npp. 29057–29075, Aug. 2023.",
  "[3] S. Bakhtiari, Z. Nasiri, and J. Vahidi, ‘‘Credit card fraud detection using\nensemble data mining methods,’’ Multimedia Tools Appl., vol. 82, no. 19,\npp. 29057–29075, Aug. 2023.\n[4] M.-H. Yang, J.-N. Luo, M. Vijayalakshmi, and S. M. Shalinie, ‘‘Contact-\nless credit cards payment fraud protection by ambient authentication,’’\nSensors, vol. 22, no. 5, p. 1989, Mar. 2022.\n[5] J. Wang, W. Liu, Y . Kou, D. Xiao, X. Wang, and X. Tang, ‘‘Approx-\nSMOTE federated learning credit card fraud detection system,’’ in Proc.\nIEEE 47th Annu. Comput., Softw., Appl. Conf. (COMPSAC) , Jun. 2023,\npp. 1370–1375.\n[6] A. A. El-Naby, E. E.-D. Hemdan, and A. El-Sayed, ‘‘An efficient\nfraud detection framework with credit card imbalanced data in financial\nservices,’’ Multimedia Tools Appl. , vol. 82, no. 3, pp. 4139–4160,\nJan. 2023.\n[7] F. K. Alarfaj, I. Malik, H. U. Khan, N. Almusallam, M. Ramzan,\nand M. Ahmed, ‘‘Credit card fraud detection using state-of-the-art",
  "Jan. 2023.\n[7] F. K. Alarfaj, I. Malik, H. U. Khan, N. Almusallam, M. Ramzan,\nand M. Ahmed, ‘‘Credit card fraud detection using state-of-the-art\nmachine learning and deep learning algorithms,’’ IEEE Access , vol. 10,\npp. 39700–39715, 2022.\n[8] M. A. Islam, M. A. Uddin, S. Aryal, and G. Stea, ‘‘An ensemble learning\napproach for anomaly detection in credit card data with imbalanced\nand overlapped classes,’’ J. Inf. Secur. Appl. , vol. 78, Nov. 2023,\nArt. no. 103618.\n[9] T. K. Dang, T. C. Tran, L. M. Tuan, and M. V . Tiep, ‘‘Machine learning\nbased on resampling approaches and deep reinforcement learning for\ncredit card fraud detection systems,’’ Appl. Sci., vol. 11, no. 21, p. 10004,\nOct. 2021.\n[10] N. S. Alfaiz and S. M. Fati, ‘‘Enhanced credit card fraud detection model\nusing machine learning,’’ Electronics, vol. 11, no. 4, p. 662, Feb. 2022.\n[11] I. D. Mienye and N. Jere, ‘‘A survey of decision trees: Concepts, algo-",
  "using machine learning,’’ Electronics, vol. 11, no. 4, p. 662, Feb. 2022.\n[11] I. D. Mienye and N. Jere, ‘‘A survey of decision trees: Concepts, algo-\nrithms, and applications,’’ IEEE Access, vol. 12, pp. 86716–86727, 2024.\n[12] S. Dong, Y . Xia, and T. Peng, ‘‘Network abnormal traffic detection model\nbased on semi-supervised deep reinforcement learning,’’ IEEE Trans.\nNetw. Service Manag. , vol. 18, no. 4, pp. 4197–4212, Dec. 2021.\n[13] E. A. L. M. Btoush, X. Zhou, R. Gururajan, K. C. Chan, R. Genrich, and\nP. Sankaran, ‘‘A systematic review of literature on credit card cyber fraud\ndetection using machine and deep learning,’’ PeerJ Comput. Sci. , vol. 9,\np. e1278, Apr. 2023.\n[14] R. Bin Sulaiman, V . Schetinin, and P. Sant, ‘‘Review of machine learning\napproach on credit card fraud detection,’’ Hum.-Centric Intell. Syst. ,\nvol. 2, no. 1, pp. 55–68, 2022.\n[15] E. N. Osegi and E. F. Jumbo, ‘‘Comparative analysis of credit card fraud",
  "approach on credit card fraud detection,’’ Hum.-Centric Intell. Syst. ,\nvol. 2, no. 1, pp. 55–68, 2022.\n[15] E. N. Osegi and E. F. Jumbo, ‘‘Comparative analysis of credit card fraud\ndetection in simulated annealing trained artificial neural network and\nhierarchical temporal memory,’’ Mach. Learn. Appl. , vol. 6, Dec. 2021,\nArt. no. 100080.\n[16] P. Wang, E. Fan, and P. Wang, ‘‘Comparative analysis of image\nclassification algorithms based on traditional machine learning and deep\nlearning,’’Pattern Recognit. Lett. , vol. 141, pp. 61–67, Jan. 2021.\n[17] I. D. Mienye and Y . Sun, ‘‘A deep learning ensemble with data\nresampling for credit card fraud detection,’’ IEEE Access , vol. 11,\npp. 30628–30638, 2023.\n[18] E. Esenogho, I. D. Mienye, T. G. Swart, K. Aruleba, and G. Obaido,\n‘‘A neural network ensemble with feature engineering for improved credit\ncard fraud detection,’’ IEEE Access, vol. 10, pp. 16400–16407, 2022.\nVOLUME 12, 2024 96907",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n[19] M. L. Gambo, A. Zainal, and M. N. Kassim, ‘‘A convolutional neural\nnetwork model for credit card fraud detection,’’ in Proc. Int. Conf. Data\nSci. Appl. (ICoDSA) , Jul. 2022, pp. 198–202.\n[20] T. Berhane, T. Melese, A. Walelign, and A. Mohammed, ‘‘A hybrid\nconvolutional neural network and support vector machine-based credit\ncard fraud detection model,’’ Math. Problems Eng. , vol. 2023, pp. 1–10,\nJun. 2023.\n[21] D. Sehrawat and Y . Singh, ‘‘Auto-encoder and LSTM-based credit\ncard fraud detection,’’ Social Netw. Comput. Sci. , vol. 4, no. 5, p. 557,\nJul. 2023.\n[22] J. Raval, P. Bhattacharya, N. K. Jadav, S. Tanwar, G. Sharma,\nP. N. Bokoro, M. Elmorsy, A. Tolba, and M. S. Raboaca, ‘‘RaKShA: A\ntrusted explainable LSTM model to classify fraud patterns on credit card\ntransactions,’’Mathematics, vol. 11, no. 8, p. 1901, Apr. 2023.",
  "trusted explainable LSTM model to classify fraud patterns on credit card\ntransactions,’’Mathematics, vol. 11, no. 8, p. 1901, Apr. 2023.\n[23] I. Benchaji, S. Douzi, B. El Ouahidi, and J. Jaafari, ‘‘Enhanced credit card\nfraud detection based on attention mechanism and LSTM deep model,’’\nJ. Big Data , vol. 8, no. 1, pp. 1–21, Dec. 2021.\n[24] S. Gold, ‘‘The evolution of payment card fraud,’’ Comput. Fraud Secur.,\nvol. 2014, no. 3, pp. 12–17, Mar. 2014.\n[25] K. L. Ambashtha and P. Kumar, ‘‘Online fraud,’’ in Financial Crimes:\nA Guide to Financial Exploitation in a Digital Age . Berlin, Germany:\nSpringer, 2023, pp. 97–108.\n[26] K. Guers, M. M. Chowdhury, and N. Rifat, ‘‘Card skimming: A\ncybercrime by hackers,’’ in Proc. IEEE Int. Conf. Electro Inf. Technol.\n(eIT), May 2022, pp. 575–579.\n[27] R. Van Belle, B. Baesens, and J. De Weerdt, ‘‘CATCHM: A novel\nnetwork-based credit card fraud detection method using node representa-",
  "(eIT), May 2022, pp. 575–579.\n[27] R. Van Belle, B. Baesens, and J. De Weerdt, ‘‘CATCHM: A novel\nnetwork-based credit card fraud detection method using node representa-\ntion learning,’’ Decis. Support Syst. , vol. 164, Jan. 2023, Art. no. 113866.\n[28] I. D. Mienye and Y . Sun, ‘‘A machine learning method with hybrid feature\nselection for improved credit card fraud detection,’’ Appl. Sci. , vol. 13,\nno. 12, p. 7254, Jun. 2023.\n[29] V . S. S. Karthik, A. Mishra, and U. S. Reddy, ‘‘Credit card fraud detection\nby modelling behaviour pattern using hybrid ensemble model,’’ Arabian\nJ. Sci. Eng. , vol. 47, no. 2, pp. 1987–1997, Feb. 2022.\n[30] K. Randhawa, C. K. Loo, M. Seera, C. P. Lim, and A. K. Nandi, ‘‘Credit\ncard fraud detection using AdaBoost and majority voting,’’ IEEE Access,\nvol. 6, pp. 14277–14284, 2018.\n[31] A. M. Aburbeian and H. I. Ashqar, ‘‘Credit card fraud detection\nusing enhanced random forest classifier for imbalanced data,’’ in Proc.",
  "vol. 6, pp. 14277–14284, 2018.\n[31] A. M. Aburbeian and H. I. Ashqar, ‘‘Credit card fraud detection\nusing enhanced random forest classifier for imbalanced data,’’ in Proc.\nInt. Conf. Adv. Comput. Res. Cham, Switzerland: Springer, 2023,\npp. 605–616.\n[32] S. E. Kafhali and M. Tayebi, ‘‘XGBoost based solutions for detecting\nfraudulent credit card transactions,’’ in Proc. Int. Conf. Adv. Creative\nNetw. Intell. Syst. (ICACNIS) , Nov. 2022, pp. 1–6.\n[33] K. Illanko, R. Soleymanzadeh, and X. Fernando, ‘‘A big data deep\nlearning approach for credit card fraud detection,’’ in Computer Networks,\nBig Data and IoT . Cham, Switzerland: Springer, 2022, pp. 633–641.\n[34] J. Karthika and A. Senthilselvi, ‘‘Smart credit card fraud detection system\nbased on dilated convolutional neural network with sampling technique,’’\nMultimedia Tools Appl. , vol. 82, no. 20, pp. 31691–31708, Aug. 2023.\n[35] H. Fanai and H. Abbasimehr, ‘‘A novel combined approach based on deep",
  "Multimedia Tools Appl. , vol. 82, no. 20, pp. 31691–31708, Aug. 2023.\n[35] H. Fanai and H. Abbasimehr, ‘‘A novel combined approach based on deep\nautoencoder and deep classifiers for credit card fraud detection,’’ Exp.\nSyst. Appl., vol. 217, May 2023, Art. no. 119562.\n[36] Y . Xie, G. Liu, C. Yan, C. Jiang, M. Zhou, and M. Li, ‘‘Learning\ntransactional behavioral representations for credit card fraud detection,’’\nIEEE Trans. Neural Netw. Learn. Syst. , vol. 35, no. 4, pp. 5735–5748,\nApr. 2024.\n[37] Z. Wang, S. Kim, and I. Joe, ‘‘An improved LSTM-based failure\nclassification model for financial companies using natural language\nprocessing,’’Appl. Sci., vol. 13, no. 13, p. 7884, Jul. 2023.\n[38] J. Karthika and A. Senthilselvi, ‘‘An integration of deep learning model\nwith navo minority over-sampling technique to detect the frauds in\ncredit cards,’’ Multimedia Tools Appl. , vol. 82, no. 14, pp. 21757–21774,\nJun. 2023.\n[39] N. Prabhakaran and R. Nedunchelian, ‘‘Oppositional cat swarm",
  "credit cards,’’ Multimedia Tools Appl. , vol. 82, no. 14, pp. 21757–21774,\nJun. 2023.\n[39] N. Prabhakaran and R. Nedunchelian, ‘‘Oppositional cat swarm\noptimization-based feature selection approach for credit card fraud\ndetection,’’Comput. Intell. Neurosci. , vol. 2023, pp. 1–13, Jan. 2023.\n[40] V . Bach Nguyen, K. G. Dastidar, M. Granitzer, and W. Siblini,\n‘‘The importance of future information in credit card fraud detection,’’\nin Proc. 25th Int. Conf. Artif. Intell. Statist. , vol. 151, G. Camps-Valls,\nF. J. R. Ruiz, and I. Valera, Eds., Mar. 2022, pp. 10067–10077.\n[41] K. Modi and R. Dayma, ‘‘Review on fraud detection methods in credit\ncard transactions,’’ in Proc. Int. Conf. Intell. Comput. Control (IC) ,\nJun. 2017, pp. 1–5.\n[42] Y . Lucas and J. Jurgovsky, ‘‘Credit card fraud detection using machine\nlearning: A survey,’’ 2020, arXiv:2010.06479.\n[43] K. G. Al-Hashedi and P. Magalingam, ‘‘Financial fraud detection\napplying data mining techniques: A comprehensive review from 2009 to",
  "learning: A survey,’’ 2020, arXiv:2010.06479.\n[43] K. G. Al-Hashedi and P. Magalingam, ‘‘Financial fraud detection\napplying data mining techniques: A comprehensive review from 2009 to\n2019,’’Comput. Sci. Rev. , vol. 40, May 2021, Art. no. 100402.\n[44] R. R. Popat and J. Chaudhary, ‘‘A survey on credit card fraud detection\nusing machine learning,’’ in Proc. 2nd Int. Conf. Trends Electron.\nInformat. (ICOEI), May 2018, pp. 1120–1125.\n[45] N. F. Ryman-Tubb, P. Krause, and W. Garn, ‘‘How artificial intelligence\nand machine learning research impacts payment card fraud detection:\nA survey and industry benchmark,’’ Eng. Appl. Artif. Intell. , vol. 76,\npp. 130–157, Nov. 2018.\n[46] K. Pandey, P. Sachan, and N. G. Ganpatrao, ‘‘A review of credit card fraud\ndetection techniques,’’ in Proc. 5th Int. Conf. Comput. Methodologies\nCommun. (ICCMC), Apr. 2021, pp. 1645–1653.\n[47] M. Alamri and M. Ykhlef, ‘‘Survey of credit card anomaly and fraud",
  "detection techniques,’’ in Proc. 5th Int. Conf. Comput. Methodologies\nCommun. (ICCMC), Apr. 2021, pp. 1645–1653.\n[47] M. Alamri and M. Ykhlef, ‘‘Survey of credit card anomaly and fraud\ndetection using sampling techniques,’’ Electronics, vol. 11, no. 23,\np. 4003, Dec. 2022.\n[48] (2018). Credit Card Fraud Detection . Accessed: Oct. 17, 2023. [Online].\nAvailable: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n[49] (2019). IEEE-CIS Fraud Detection . Accessed: Oct. 17, 2023. [Online].\nAvailable: https://www.kaggle.com/c/ieee-fraud-detection\n[50] E. A. Lopez-Rojas, A. Elmir, and S. Axelsson, ‘‘PaySim: A financial\nmobile money simulator for fraud detection,’’ in Proc. 28th Eur. Modeling\nSimulation Symp. (EMSS) , Sep. 2016, pp. 249–255.\n[51] G. F. Montufar, R. Pascanu, K. Cho, and Y . Bengio, ‘‘On the number of\nlinear regions of deep neural networks,’’ in Proc. Adv. Neural Inf. Process.\nSyst., vol. 27, 2014, pp. 1–9.",
  "[51] G. F. Montufar, R. Pascanu, K. Cho, and Y . Bengio, ‘‘On the number of\nlinear regions of deep neural networks,’’ in Proc. Adv. Neural Inf. Process.\nSyst., vol. 27, 2014, pp. 1–9.\n[52] B. Yuen, M. T. Hoang, X. Dong, and T. Lu, ‘‘Universal activation function\nfor machine learning,’’ Sci. Rep., vol. 11, no. 1, p. 18757, Sep. 2021.\n[53] I. H. Sarker, ‘‘AI-based modeling: Techniques, applications and research\nissues towards automation, intelligent and smart systems,’’ Social Netw.\nComput. Sci., vol. 3, no. 2, p. 158, Mar. 2022.\n[54] Y . LeCun, Y . Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,\nno. 7553, pp. 436–444, 7553.\n[55] I. D. Mienye and Y . Sun, ‘‘Effective feature selection for improved\nprediction of heart disease,’’ in Proc. Pan-African Artif. Intell. Smart\nSystems Conf. Cham, Switzerland: Springer, 2021, pp. 94–107.\n[56] C. Guo, B. Zhao, and Y . Bai, ‘‘Deepcore: A comprehensive library for\ncoreset selection in deep learning,’’ in Proc. Int. Conf. Database Expert",
  "[56] C. Guo, B. Zhao, and Y . Bai, ‘‘Deepcore: A comprehensive library for\ncoreset selection in deep learning,’’ in Proc. Int. Conf. Database Expert\nSyst. Appl. Cham, Switzerland: Springer, 2022, pp. 181–195.\n[57] W.-F. Zeng, X.-X. Zhou, S. Willems, C. Ammar, M. Wahle, I. Bludau,\nE. V oytik, M. T. Strauss, and M. Mann, ‘‘AlphaPeptDeep: A modular deep\nlearning framework to predict peptide properties for proteomics,’’ Nature\nCommun., vol. 13, no. 1, p. 7238, Nov. 2022.\n[58] A. Mehrish, N. Majumder, R. Bharadwaj, R. Mihalcea, and S. Poria,\n‘‘A review of deep learning techniques for speech processing,’’ Inf.\nFusion, vol. 99, Nov. 2023, Art. no. 101869.\n[59] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n[60] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN encoder–decoder for statistical machine translation,’’ 2014,",
  "H. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN encoder–decoder for statistical machine translation,’’ 2014,\narXiv:1406.1078.\n[61] Y . Moodi, M. Ghasemi, and S. R. Mousavi, ‘‘Estimating the compressive\nstrength of rectangular fiber reinforced polymer–confined columns\nusing multilayer perceptron, radial basis function, and support vector\nregression methods,’’ J. Reinforced Plastics Compos. , vol. 41, nos. 3–4,\npp. 130–146, Feb. 2022.\n[62] S. R. Dubey, S. K. Singh, and B. B. Chaudhuri, ‘‘Activation functions in\ndeep learning: A comprehensive survey and benchmark,’’ Neurocomput-\ning, vol. 503, pp. 92–108, Sep. 2022.\n[63] S. S. Yadav and S. M. Jadhav, ‘‘Deep convolutional neural network based\nmedical image classification for disease diagnosis,’’ J. Big Data , vol. 6,\nno. 1, pp. 1–18, Dec. 2019.\n[64] J. Naranjo-Torres, M. Mora, R. Hernández-García, R. J. Barrientos,\nC. Fredes, and A. Valenzuela, ‘‘A review of convolutional neural network",
  "no. 1, pp. 1–18, Dec. 2019.\n[64] J. Naranjo-Torres, M. Mora, R. Hernández-García, R. J. Barrientos,\nC. Fredes, and A. Valenzuela, ‘‘A review of convolutional neural network\napplied to fruit image processing,’’ Appl. Sci. , vol. 10, no. 10, p. 3443,\nMay 2020.\n96908 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n[65] I. D. Mienye, P. Kenneth Ainah, I. D. Emmanuel, and E. Esenogho,\n‘‘Sparse noise minimization in image classification using genetic\nalgorithm and DenseNet,’’ in Proc. Conf. Inf. Commun. Technol. Soc.\n(ICTAS), Mar. 2021, pp. 103–108.\n[66] R. San Miguel Carrasco and M.-Á. Sicilia-Urbán, ‘‘Evaluation of deep\nneural networks for reduction of credit card fraud alerts,’’ IEEE Access ,\nvol. 8, pp. 186421–186432, 2020.\n[67] K. Fu, D. Cheng, Y . Tu, and L. Zhang, ‘‘Credit card fraud detection\nusing convolutional neural networks,’’ in Neural Information Processing,\nKyoto, Japan. Cham, Switzerland: Springer, 2016, pp. 483–490.\n[68] A. Dhillon and G. K. Verma, ‘‘Convolutional neural network: A review of\nmodels, methodologies and applications to object detection,’’ Prog. Artif.\nIntell., vol. 9, no. 2, pp. 85–112, Jun. 2020.\n[69] M. Krichen, ‘‘Convolutional neural networks: A survey,’’ Computers,",
  "models, methodologies and applications to object detection,’’ Prog. Artif.\nIntell., vol. 9, no. 2, pp. 85–112, Jun. 2020.\n[69] M. Krichen, ‘‘Convolutional neural networks: A survey,’’ Computers,\nvol. 12, no. 8, p. 151, Jul. 2023.\n[70] R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi,\n‘‘Convolutional neural networks: An overview and application\nin radiology,’’ Insights into Imag. , vol. 9, no. 4, pp. 611–629,\nAug. 2018.\n[71] L. González-Rodríguez and A. Plasencia-Salgueiro, ‘‘Uncertainty-aware\nautonomous mobile robot navigation with deep reinforcement learning,’’\nin Deep Learning for Unmanned Systems , Cham, Switzerland, 2021,\npp. 225–257.\n[72] A. Tsantekidis, N. Passalis, and A. Tefas, ‘‘Recurrent neural networks,’’\nin Deep Learning for Robot Perception and Cognition . Amsterdam,\nThe Netherlands: Elsevier, 2022, pp. 101–115.\n[73] P. Oliveira, B. Fernandes, C. Analide, and P. Novais, ‘‘Forecasting energy\nconsumption of wastewater treatment plants with a transfer learning",
  "The Netherlands: Elsevier, 2022, pp. 101–115.\n[73] P. Oliveira, B. Fernandes, C. Analide, and P. Novais, ‘‘Forecasting energy\nconsumption of wastewater treatment plants with a transfer learning\napproach for sustainable cities,’’ Electronics, vol. 10, no. 10, p. 1149,\nMay 2021.\n[74] J. Yang, J. Qu, Q. Mi, and Q. Li, ‘‘A CNN-LSTM model for tailings dam\nrisk prediction,’’ IEEE Access, vol. 8, pp. 206491–206502, 2020.\n[75] M. Ma, C. Liu, R. Wei, B. Liang, and J. Dai, ‘‘Predicting machine’s\nperformance record using the stacked long short-term memory (LSTM)\nneural networks,’’ J. Appl. Clin. Med. Phys. , vol. 23, no. 3, 2022,\nArt. no. e13558.\n[76] H. Xie, M. Randall, and K.-W. Chau, ‘‘Green roof hydrological modelling\nwith GRU and LSTM networks,’’ Water Resour. Manag., vol. 36, no. 3,\npp. 1107–1122, Feb. 2022.\n[77] S. Gao, Y . Huang, S. Zhang, J. Han, G. Wang, M. Zhang, and Q. Lin,\n‘‘Short-term runoff prediction with GRU and LSTM networks without",
  "pp. 1107–1122, Feb. 2022.\n[77] S. Gao, Y . Huang, S. Zhang, J. Han, G. Wang, M. Zhang, and Q. Lin,\n‘‘Short-term runoff prediction with GRU and LSTM networks without\nrequiring time step optimization during sample generation,’’ J. Hydrol.,\nvol. 589, Oct. 2020, Art. no. 125188.\n[78] C. Cui, P. Wang, Y . Li, and Y . Zhang, ‘‘McVCsB: A new hybrid deep\nlearning network for stock index prediction,’’ Exp. Syst. Appl. , vol. 232,\nDec. 2023, Art. no. 120902.\n[79] Y .-H. Li, L. N. Harfiya, K. Purwandari, and Y .-D. Lin, ‘‘Real-time cuffless\ncontinuous blood pressure estimation using deep learning model,’’\nSensors, vol. 20, no. 19, p. 5606, Sep. 2020.\n[80] Y . Hao, L. Dong, F. Wei, and K. Xu, ‘‘Self-attention attribution:\nInterpreting information interactions inside transformer,’’ in Proc. AAAI\nConf. Artif. Intell. , 2021, vol. 35, no. 14, pp. 12963–12971.\n[81] D. A. Tarzanagh, Y . Li, X. Zhang, and S. Oymak, ‘‘Max-margin token\nselection in attention mechanism,’’ in Proc. 37th Conf. Neural Inf.",
  "[81] D. A. Tarzanagh, Y . Li, X. Zhang, and S. Oymak, ‘‘Max-margin token\nselection in attention mechanism,’’ in Proc. 37th Conf. Neural Inf.\nProcess. Syst., 2023, pp. 48314–48362.\n[82] G. Obaido, B. Ogbuokiri, C. W. Chukwu, F. J. Osaye, O. F. Egbelowo,\nM. I. Uzochukwu, I. D. Mienye, K. Aruleba, M. Primus, and O. Achilonu,\n‘‘An improved ensemble method for predicting hyperchloremia in adults\nwith diabetic ketoacidosis,’’ IEEE Access , vol. 12, pp. 9536–9549,\n2024.\n[83] T. O’Halloran, G. Obaido, B. Otegbade, and I. D. Mienye, ‘‘A deep\nlearning approach for maize lethal necrosis and maize streak virus disease\ndetection,’’Mach. Learn. Appl. , vol. 16, Jun. 2024, Art. no. 100556.\n[84] R. Trevethan, ‘‘Sensitivity, specificity, and predictive values: Founda-\ntions, pliabilities, and pitfalls in research and practice,’’ Frontiers Public\nHealth, vol. 5, Nov. 2017, Art. no. 308890.\n[85] I. D. Mienye, G. Obaido, K. Aruleba, and O. A. Dada, ‘‘Enhanced",
  "tions, pliabilities, and pitfalls in research and practice,’’ Frontiers Public\nHealth, vol. 5, Nov. 2017, Art. no. 308890.\n[85] I. D. Mienye, G. Obaido, K. Aruleba, and O. A. Dada, ‘‘Enhanced\nprediction of chronic kidney disease using feature selection and boosted\nclassifiers,’’ in Proc. Int. Conf. Intell. Syst. Design Appl. Cham,\nSwitzerland: Springer, 2021, pp. 527–537.\n[86] M. Rizwan, A. Nadeem, and M. A. Sindhu, ‘‘Analyses of classifier’s\nperformance measures used in software fault prediction studies,’’ IEEE\nAccess, vol. 7, pp. 82764–82775, 2019.\n[87] G. Obaido, O. Achilonu, B. Ogbuokiri, C. S. Amadi, L. Habeebullahi,\nT. Ohalloran, C. W. Chukwu, E. D. Mienye, M. Aliyu, O. Fasawe,\nI. A. Modupe, E. J. Omietimi, and K. Aruleba, ‘‘An improved framework\nfor detecting thyroid disease using filter-based feature selection and\nstacking ensemble,’’ IEEE Access, vol. 12, pp. 89098–89112, 2024.\n[88] B. Ozenne, F. Subtil, and D. Maucort-Boulch, ‘‘The precision–recall",
  "stacking ensemble,’’ IEEE Access, vol. 12, pp. 89098–89112, 2024.\n[88] B. Ozenne, F. Subtil, and D. Maucort-Boulch, ‘‘The precision–recall\ncurve overcame the optimism of the receiver operating characteristic\ncurve in rare diseases,’’ J. Clin. Epidemiol. , vol. 68, no. 8, pp. 855–859,\nAug. 2015.\n[89] J. F. Roseline, G. Naidu, V . S. Pandi, S. A. A. Rajasree, and N. Mageswari,\n‘‘Autonomous credit card fraud detection using machine learning\napproach?’’ Comput. Electr. Eng., vol. 102, Sep. 2022, Art. no. 108132.\n[90] H. Najadat, O. Altiti, A. A. Aqouleh, and M. Younes, ‘‘Credit card fraud\ndetection based on machine and deep learning,’’ in Proc. 11th Int. Conf.\nInf. Commun. Syst. (ICICS) , Apr. 2020, pp. 204–208.\n[91] J. Forough and S. Momtazi, ‘‘Ensemble of deep sequential models for\ncredit card fraud detection,’’ Appl. Soft Comput. , vol. 99, Feb. 2021,\nArt. no. 106883.\n[92] N. F. Aurna, M. D. Hossain, Y . Taenaka, and Y . Kadobayashi, ‘‘Federated",
  "credit card fraud detection,’’ Appl. Soft Comput. , vol. 99, Feb. 2021,\nArt. no. 106883.\n[92] N. F. Aurna, M. D. Hossain, Y . Taenaka, and Y . Kadobayashi, ‘‘Federated\nlearning-based credit card fraud detection: Performance analysis with\nsampling methods and deep learning algorithms,’’ in Proc. IEEE Int.\nConf. Cyber Secur. Resilience (CSR) , Jul. 2023, pp. 180–186.\n[93] Y . Xie, G. Liu, C. Yan, C. Jiang, and M. Zhou, ‘‘Time-aware attention-\nbased gated network for credit card fraud detection by extracting\ntransactional behaviors,’’ IEEE Trans. Computat. Social Syst. , vol. 10,\nno. 3, pp. 1004–1016, Jun. 2023.\n[94] E. Ajitha, S. Sneha, S. Makesh, and K. Jaspin, ‘‘A comparative\nanalysis of credit card fraud detection with machine learning algorithms\nand convolutional neural network,’’ in Proc. Int. Conf. Adv. Comput.,\nCommun. Appl. Informat. (ACCAI) , May 2023, pp. 1–8.\n[95] M. N. Y . Ali, T. Kabir, N. L. Raka, S. S. Toma, M. L. Rahman,",
  "and convolutional neural network,’’ in Proc. Int. Conf. Adv. Comput.,\nCommun. Appl. Informat. (ACCAI) , May 2023, pp. 1–8.\n[95] M. N. Y . Ali, T. Kabir, N. L. Raka, S. S. Toma, M. L. Rahman,\nand J. Ferdaus, ‘‘SMOTE based credit card fraud detection using\nconvolutional neural network,’’ in Proc. 25th Int. Conf. Comput. Inf.\nTechnol. (ICCIT), Dec. 2022, pp. 55–60.\n[96] M. Z. Mizher and A. B. Nassif, ‘‘Deep CNN approach for unbalanced\ncredit card fraud detection data,’’ in Proc. Adv. Sci. Eng. Technol. Int.\nConf. (ASET), Feb. 2023, pp. 1–7.\n[97] A. Iqbal and R. Amin, ‘‘Time series forecasting and anomaly detection\nusing deep learning,’’ Comput. Chem. Eng. , vol. 182, Mar. 2024,\nArt. no. 108560.\n[98] Y . Tang and Z. Liu, ‘‘A distributed knowledge distillation framework for\nfinancial fraud detection based on transformer,’’ IEEE Access , vol. 12,\npp. 62899–62911, 2024.\n[99] I. D. Mienye and Y . Sun, ‘‘Performance analysis of cost-sensitive learning",
  "financial fraud detection based on transformer,’’ IEEE Access , vol. 12,\npp. 62899–62911, 2024.\n[99] I. D. Mienye and Y . Sun, ‘‘Performance analysis of cost-sensitive learning\nmethods with application to imbalanced medical data,’’ Informat. Med.\nUnlocked, vol. 25, Jan. 2021, Art. no. 100690.\n[100] D. Dablain, B. Krawczyk, and N. V . Chawla, ‘‘DeepSMOTE: Fusing deep\nlearning and SMOTE for imbalanced data,’’ IEEE Trans. Neural Netw.\nLearn. Syst., vol. 34, no. 9, pp. 6390–6404, Sep. 2023.\n[101] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, ‘‘Learning\nimbalanced datasets with label-distribution-aware margin loss,’’ in Proc.\nAdv. Neural Inf. Process. Syst. , vol. 32, 2019, pp. 1–12.\n[102] Q. Dong, S. Gong, and X. Zhu, ‘‘Imbalanced deep learning by minority\nclass incremental rectification,’’ IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 41, no. 6, pp. 1367–1381, Jun. 2019.\n[103] C. Zhang, K. C. Tan, H. Li, and G. S. Hong, ‘‘A cost-sensitive deep belief",
  "class incremental rectification,’’ IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 41, no. 6, pp. 1367–1381, Jun. 2019.\n[103] C. Zhang, K. C. Tan, H. Li, and G. S. Hong, ‘‘A cost-sensitive deep belief\nnetwork for imbalanced classification,’’ IEEE Trans. Neural Netw. Learn.\nSyst., vol. 30, no. 1, pp. 109–122, Jan. 2019.\n[104] S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy, ‘‘Training\ndeep neural networks on imbalanced data sets,’’ in Proc. Int. Joint Conf.\nNeural Netw. (IJCNN) , Jul. 2016, pp. 4368–4374.\n[105] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, ‘‘Focal loss for\ndense object detection,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,\nOct. 2017, pp. 2999–3007.\n[106] Z. Zhang and M. Sabuncu, ‘‘Generalized cross entropy loss for training\ndeep neural networks with noisy labels,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 31, 2018, pp. 1–11.\n[107] Y . Cui, M. Jia, T.-Y . Lin, Y . Song, and S. Belongie, ‘‘Class-balanced",
  "deep neural networks with noisy labels,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 31, 2018, pp. 1–11.\n[107] Y . Cui, M. Jia, T.-Y . Lin, Y . Song, and S. Belongie, ‘‘Class-balanced\nloss based on effective number of samples,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 9260–9269.\n[108] J. A. Dar, K. K. Srivastava, and S. Ahmed Lone, ‘‘Design and\ndevelopment of hybrid optimization enabled deep learning model for\nCOVID-19 detection with comparative analysis with DCNN, BIAT-GRU,\nXGBoost,’’Comput. Biol. Med. , vol. 150, Nov. 2022, Art. no. 106123.\nVOLUME 12, 2024 96909",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n[109] V . B. Semwal, A. Gupta, and P. Lalwani, ‘‘An optimized hybrid\ndeep learning model using ensemble learning approach for human\nwalking activities recognition,’’ J. Supercomput. , vol. 77, no. 11,\npp. 12256–12279, Nov. 2021.\n[110] M. U. Salur and I. Aydin, ‘‘A novel hybrid deep learning model for\nsentiment classification,’’ IEEE Access, vol. 8, pp. 58080–58093, 2020.\n[111] T.-Y . Liu, ‘‘EasyEnsemble and feature selection for imbalance data\nsets,’’ in Proc. Int. Joint Conf. Bioinf., Syst. Biol. Intell. Comput. , 2009,\npp. 517–520.\n[112] J. Blaszczyński and J. Stefanowski, ‘‘Actively balanced bagging for\nimbalanced data,’’ in Foundations of Intelligent Systems, Warsaw, Poland.\nCham, Switzerland: Springer, 2017, pp. 271–281.\n[113] E. Altman, ‘‘Synthesizing credit card transactions,’’ in Proc. 2nd ACM\nInt. Conf. AI Finance , Nov. 2021, pp. 1–9.",
  "Cham, Switzerland: Springer, 2017, pp. 271–281.\n[113] E. Altman, ‘‘Synthesizing credit card transactions,’’ in Proc. 2nd ACM\nInt. Conf. AI Finance , Nov. 2021, pp. 1–9.\n[114] A. Chakrabarty and S. Das, ‘‘Statistical regeneration guarantees of the\nWasserstein autoencoder with latent space consistency,’’ in Proc. Adv.\nNeural Inf. Process. Syst. , vol. 34, 2021, pp. 17098–17110.\n[115] D. Urda, J. Montes-Torres, F. Moreno, L. Franco, and J. M. Jerez, ‘‘Deep\nlearning to analyze RNA-seq gene expression data,’’ in Advances in\nComputational Intelligence, Cadiz, Spain. Cham, Switzerland: Springer,\n2017, pp. 50–59.\n[116] L. Antwarg, R. M. Miller, B. Shapira, and L. Rokach, ‘‘Explaining\nanomalies detected by autoencoders using Shapley additive explana-\ntions,’’Exp. Syst. Appl. , vol. 186, Dec. 2021, Art. no. 115736.\n[117] Y . Yang, J. Qiu, M. Song, D. Tao, and X. Wang, ‘‘Distilling knowledge\nfrom graph convolutional networks,’’ in Proc. IEEE/CVF Conf. Comput.",
  "[117] Y . Yang, J. Qiu, M. Song, D. Tao, and X. Wang, ‘‘Distilling knowledge\nfrom graph convolutional networks,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 7072–7081.\n[118] H. Raza, G. Prasad, and Y . Li, ‘‘EWMA model based shift-detection\nmethods for detecting covariate shifts in non-stationary environments,’’\nPattern Recognit., vol. 48, no. 3, pp. 659–669, Mar. 2015.\n[119] S.-S. Zhang, J.-W. Liu, and X. Zuo, ‘‘Adaptive online incremental\nlearning for evolving data streams,’’ Appl. Soft Comput. , vol. 105,\nJul. 2021, Art. no. 107255.\n[120] K. Rahmani, R. Thapa, P. Tsou, S. Casie Chetty, G. Barnes, C. Lam, and\nC. Foon Tso, ‘‘Assessing the effects of data drift on the performance of\nmachine learning models used in clinical sepsis prediction,’’ Int. J. Med.\nInformat., vol. 173, May 2023, Art. no. 104930.\n[121] S. Savvides, D. Khandelwal, and P. Eugster, ‘‘Efficient confidentiality-",
  "Informat., vol. 173, May 2023, Art. no. 104930.\n[121] S. Savvides, D. Khandelwal, and P. Eugster, ‘‘Efficient confidentiality-\npreserving data analytics over symmetrically encrypted datasets,’’ Proc.\nVLDB Endowment, vol. 13, no. 8, pp. 1290–1303, 2020.\n[122] S. Murthy, A. Abu Bakar, F. Abdul Rahim, and R. Ramli, ‘‘A comparative\nstudy of data anonymization techniques,’’ in Proc. IEEE 5th Int. Conf.\nBig Data Secur. Cloud (BigDataSecurity), Int. Conf. High Perform. Smart\nComput., (HPSC), IEEE Int. Conf. Intell. Data Secur. (IDS) , May 2019,\npp. 306–309.\n[123] H. Lee and Y . D. Chung, ‘‘Differentially private release of medical\nmicrodata: An efficient and practical approach for preserving informative\nattribute values,’’ BMC Med. Informat. Decis. Making , vol. 20, no. 1,\npp. 1–15, Dec. 2020.\n[124] A. Ali, M. F. Pasha, J. Ali, O. H. Fang, M. Masud, A. D. Jurcut, and\nM. A. Alzain, ‘‘Deep learning based homomorphic secure search-able",
  "pp. 1–15, Dec. 2020.\n[124] A. Ali, M. F. Pasha, J. Ali, O. H. Fang, M. Masud, A. D. Jurcut, and\nM. A. Alzain, ‘‘Deep learning based homomorphic secure search-able\nencryption for keyword search in blockchain healthcare system: A novel\napproach to cryptography,’’ Sensors, vol. 22, no. 2, p. 528, Jan. 2022.\n[125] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\n‘‘A survey on bias and fairness in machine learning,’’ ACM Comput. Surv.,\nvol. 54, no. 6, pp. 1–35, Jul. 2022.\n[126] J. Mary, C. Calauzenes, and N. El Karoui, ‘‘Fairness-aware learning for\ncontinuous attributes and treatments,’’ in Proc. Int. Conf. Mach. Learn. ,\n2019, pp. 4382–4391.\n[127] A. Alharbi, M. Alshammari, O. D. Okon, A. Alabrah, H. T. Rauf,\nH. Alyami, and T. Meraj, ‘‘A novel text2IMG mechanism of credit card\nfraud detection: A deep learning approach,’’ Electronics, vol. 11, no. 5,\np. 756, Mar. 2022.\n[128] I. D. Mienye and N. Jere, ‘‘Optimized ensemble learning approach",
  "fraud detection: A deep learning approach,’’ Electronics, vol. 11, no. 5,\np. 756, Mar. 2022.\n[128] I. D. Mienye and N. Jere, ‘‘Optimized ensemble learning approach\nwith explainable AI for improved heart disease prediction,’’ Information,\nvol. 15, no. 7, p. 394, Jul. 2024.\n[129] M. N. Fekri, H. Patel, K. Grolinger, and V . Sharma, ‘‘Deep learning for\nload forecasting with smart meter data: Online adaptive recurrent neural\nnetwork,’’Appl. Energy, vol. 282, Jan. 2021, Art. no. 116177.\n[130] A. Arunarani, D. Manjula, and V . Sugumaran, ‘‘Task scheduling\ntechniques in cloud computing: A literature survey,’’ Future Gener.\nComput. Syst., vol. 91, pp. 407–415, Feb. 2019.\nIBOMOIYE DOMOR MIENYE(Member, IEEE)\nreceived the B.Eng. degree in electrical and\nelectronic engineering and the M.Sc. degree (cum\nlaude) in computer systems engineering from the\nUniversity of East London, in 2012 and 2014,\nrespectively, and the Ph.D. degree in electrical\nand electronic engineering from the University of",
  "laude) in computer systems engineering from the\nUniversity of East London, in 2012 and 2014,\nrespectively, and the Ph.D. degree in electrical\nand electronic engineering from the University of\nJohannesburg, South Africa. His research interests\ninclude machine learning and deep learning for\nfinance and healthcare applications.\nNOBERT JERE received the M.Sc. and Ph.D.\ndegrees in computer science from the University of\nFort Hare, South Africa, in 2009 and 2013, respec-\ntively. He is currently an Associate Professor with\nthe Department of Information Technology, Walter\nSisulu University, South Africa. He has authored\nor co-authored numerous peer-reviewed journal\narticles and conference proceedings, chaired/co-\nchaired international conferences, and serves as\na reviewer for numerous reputable journals. His\nmain research interest includes ICT for sustainable development.\n96910 VOLUME 12, 2024",
  "Received 18 June 2024, accepted 9 July 2024, date of publication 11 July 2024, date of current version 23 July 2024.\nDigital Object Identifier 10.1 109/ACCESS.2024.3426955\nDeep Learning for Credit Card Fraud Detection:\nA Review of Algorithms, Challenges,\nand Solutions\nIBOMOIYE DOMOR MIENYE\n , (Member, IEEE), AND NOBERT JERE\nDepartment of Information Technology, Walter Sisulu University, Buffalo City Campus, East London 5200, South Africa\nCorresponding author: Ibomoiye Domor Mienye (imienye@wsu.ac.za)\nABSTRACT Deep learning (DL), a branch of machine learning (ML), is the core technology in today’s\ntechnological advancements and innovations. Deep learning-based approaches are the state-of-the-art\nmethods used to analyse and detect complex patterns in large datasets, such as credit card transactions.\nHowever, most credit card fraud models in the literature are based on traditional ML algorithms, and recently,",
  "However, most credit card fraud models in the literature are based on traditional ML algorithms, and recently,\nthere has been a rise in applications based on deep learning techniques. This study reviews the recent\nDL-based literature and presents a concise description and performance comparison of the widely used DL\ntechniques, including convolutional neural network (CNN), simple recurrent neural network (RNN), long\nshort-term memory (LSTM), and gated recurrent unit (GRU). Additionally, an attempt is made to discuss\nsuitable performance metrics, common challenges encountered when training credit card fraud models using\nDL architectures and potential solutions, which are lacking in previous studies and would benefit deep\nlearning researchers and practitioners. Meanwhile, the experimental results and analysis using a real-world\ndataset indicate the robustness of the deep learning architectures in credit card fraud detection.",
  "dataset indicate the robustness of the deep learning architectures in credit card fraud detection.\nINDEX TERMS Credit card, CNN, deep learning, fraud detection, GRU, LSTM, machine learning.\nI. INTRODUCTION\nCredit card transactions have grown due to the rapid\ntechnological advancements and convenience of electronic\nservices [1], [2]. Consequently, there has been an increase in\nsecurity issues, such as credit card fraud, which has become\na significant concern for both financial institutions and cus-\ntomers [3], [4]. According to the Nielsen report, losses from\ncredit card fraud in 2019, 2020, and 2021 were approximately\n28.65, 28.50, and 32.34 billion dollars, respectively [5], [6],\n[7]. Additionally, losses due to credit card fraud globally have\ntripled in the last decade, from 9.84 billion dollars in 2011 to\n32.34 billion dollars in 2021 [8].\nMachine learning (ML) methods have been widely used\nfor credit card fraud detection (CCFD), achieving state-of-",
  "32.34 billion dollars in 2021 [8].\nMachine learning (ML) methods have been widely used\nfor credit card fraud detection (CCFD), achieving state-of-\nthe-art performances [9], [10], [11]. ML algorithms can be\nclassified into supervised, unsupervised, semi-supervised,\nor reinforcement learning [12]. The most widely used ML\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Tony Thomas.\nmethod for identifying credit card fraud is the supervised\nlearning (SL) method [13]. Supervised learning entails\ntraining an ML algorithm using a dataset where each data\npoint has a label. The label indicates the specific class the data\npoint belongs to, such as fraud or not fraud. SL techniques\ntend to learn the relationship between the input features\n(or independent variables) and the output labels (dependent\nvariables).\nSeveral studies have demonstrated the ability of neural\nnetworks to identify fraudulent transactions in complex",
  "(or independent variables) and the output labels (dependent\nvariables).\nSeveral studies have demonstrated the ability of neural\nnetworks to identify fraudulent transactions in complex\ncredit card data [14], [15]. A neural network is a type of\nmachine learning with a learning process that mimics the\nhuman brain and can be supervised or unsupervised [16].\nNeural networks with multiple layers in the network\nalso called deep learning (DL), can progressively extract\nhigher-level features and analyse complex patterns with\nenhanced predictions. DL approaches have been used to\nidentify fraudulent transactions in credit card data. For\nexample, Mienye and Sun [17] developed an approach for\ncredit card fraud detection using a stacked ensemble of\nVOLUME 12, 2024",
  "2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by/4.0/ 96893",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nlong short-term memory (LSTM) and gated recurrent unit\n(GRU) networks, with a multilayer perceptron (MLP) as the\nbase learner. The DL-based ensemble performed excellently\ncompared to other ML algorithms and the individual DL\narchitectures. Similarly, Esenogho et al. [18] proposed a\nDL-based approach for credit card fraud detection using the\nLSTM neural network as the base learner in the adaptive\nboosting (AdaBoost) implementation, achieving excellent\nclassification performance. Additionally, different studies\nhave used convolutional neural networks [19], [20].\nMeanwhile, recurrent neural networks (RNN) and their\nvariants, such as LSTM and GRU, are the most widely used\nDL-based networks for modelling and analysing credit card\ntransactions due to their ability to learn sequential data and\ndetect temporal relationships [21], [22], [23]. The LSTM\nnetwork is useful for learning long-term dependencies in a",
  "transactions due to their ability to learn sequential data and\ndetect temporal relationships [21], [22], [23]. The LSTM\nnetwork is useful for learning long-term dependencies in a\nsequence. It is powerful because it can remember information\nfrom previous time steps and selectively forget or update that\ninformation as new inputs are processed. Like LSTM, GRU\ncan selectively update or forget data from earlier time steps\ndue to its gating mechanism, making it suitable for time series\nmodelling.\nHowever, despite the robustness of deep learning tech-\nniques, there are certain benefits and limitations in using\nthem for credit card fraud detection. Therefore, this study\naims to review the application and role of deep learning in\ncredit card fraud detection. The significance of this study\nlies in its comprehensive review of the current state of\ndeep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.",
  "lies in its comprehensive review of the current state of\ndeep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.\nBy systematically analyzing various deep learning techniques\nand their performance, this study provides valuable insights\nfor researchers and practitioners. The main objectives and\ncontributions of this review are as follows:\n• A review of the most current research on credit card\nfraud detection, focusing on deep learning techniques.\n• A concise but comprehensive overview of the main\ndeep learning techniques used in CCFD and their\nperformance comparison.\n• A detailed evaluation of the widely used performance\nevaluation metrics, focusing on their suitability for\nCCFD.\n• An in-depth analysis of existing challenges in using\nDL-based techniques for credit card fraud detection,\npotential solutions, and research directions.\nFurthermore, to ensure a comprehensive and unbiased",
  "DL-based techniques for credit card fraud detection,\npotential solutions, and research directions.\nFurthermore, to ensure a comprehensive and unbiased\nreview, a systematic approach was employed for literature\ngathering. The literature selection began with a compre-\nhensive search of multiple academic databases, including\nIEEE Xplore, SpringerLink, ScienceDirect, and Google\nScholar. Keywords such as ‘‘credit card fraud detection,’’\n‘‘deep learning,’’ ‘‘CNN,’’ ‘‘RNN,’’ ‘‘LSTM,’’ and ‘‘GRU’’\nwere used to identify relevant studies. We included articles\npublished between 2015 and 2024 to capture the most recent\nadvancements and trends in the field. Studies were further\nfiltered based on relevance, impact, and their contribution to\nthe understanding of deep learning applications in credit card\nfraud detection.\nThe rest of the paper is structured as follows: section II\npresents a discussion of credit card fraud detection and related",
  "fraud detection.\nThe rest of the paper is structured as follows: section II\npresents a discussion of credit card fraud detection and related\nreviews. Section III discusses publicly available credit card\ndatasets, and Section IV presents a comprehensive overview\nof deep learning and relevant architectures. Section V\ndiscusses widely used performance evaluation metrics and\ntheir suitability for credit card fraud detection. Section VI\npresents a review of recent studies that applied deep learning\nfor credit card fraud detection. Section VII presents the\nexperimental results and analysis of the various DL methods,\nand Section VIII discusses the challenges of using DL to\ndetect credit card fraud and possible solutions. Section IX\npresents a general discussion and future research directions,\nand Section X concludes the study and highlights future\nresearch directions.\nII. RELATED WORKS\nA. CREDIT CARD FRAUD DETECTION\nCredit card fraud occurs when an unauthorised user obtains",
  "and Section X concludes the study and highlights future\nresearch directions.\nII. RELATED WORKS\nA. CREDIT CARD FRAUD DETECTION\nCredit card fraud occurs when an unauthorised user obtains\naccess to someone’s credit card details and performs trans-\nactions. It is an inclusive term for fraud committed via a\nbank card, including credit and debit cards [24]. Although\nthe transactions are frequently carried out online, they can be\ncarried out using the actual card when misplaced or stolen.\nFraudsters use different methods to obtain the cardholder’s\ninformation, including phishing, where a fraudster poses as\na financial official to coerce a user into disclosing personal\ninformation, and skimmers use an interface to an automated\nteller machine (ATM) or point-of-sale device that can read a\ncard directly [25], [26].\nDetecting credit card fraud is essential in ensuring the\nsecurity of consumers’ finances and financial information.\nThe two main approaches to detecting fraudulent activity",
  "card directly [25], [26].\nDetecting credit card fraud is essential in ensuring the\nsecurity of consumers’ finances and financial information.\nThe two main approaches to detecting fraudulent activity\nare automated systems and manual investigation. While\nautomated systems rely on algorithms and machine learning\ntechniques to identify patterns of fraudulent behaviour,\nmanual investigation involves human intervention to analyse\nsuspicious activities and gather evidence. Automated systems\nare more popular due to their ability to process large volumes\nof data quickly and efficiently, and they utilize advanced\nstatistical and machine learning models [27].\nFurthermore, machine learning algorithms, including neu-\nral networks, are widely employed for detecting credit\ncard fraud. For example, Mienye and Sun [28] proposed\na CCFD method using hybrid feature selection based on\ngenetic algorithm and information gain, and the learning\nalgorithm was the extreme learning machine (ELM). The",
  "a CCFD method using hybrid feature selection based on\ngenetic algorithm and information gain, and the learning\nalgorithm was the extreme learning machine (ELM). The\ngenetic algorithm’s fitness function employed in the study\nwas the geometric mean, which was used to tackle the\nclass imbalance problem, leading to improved classification\nperformance. Similarly, Karthik et al. [29] proposed a hybrid\nensemble approach for credit card fraud detection to solve\nthe imbalance class problem. The study combined boosting\n96894 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nand bagging methods, i.e., adaptive boosting (AdaBoost) and\nrandom forest, respectively, achieving superior performance\ncompared to the individual classifiers.\nRandhawa et al. [30] developed a hybrid ensemble based\non majority voting and adaptive boosting. They compared\nthe performance with some single classifiers, including\ndecision tree, support vector machines (SVM), and naïve\nBayes. The proposed hybrid method achieved the best\nMatthews Correlation Coefficient (MCC) score of 1. Other\nexamples of ML algorithms in credit card fraud detection\ninclude random forest [31], XGBoost [32], convolutional\nneural network (CNN) [33], [34], RNN [35], LSTM [22],\n[36], [37], GRU [38], and bidirectional gated recurrent unit\n(BiGRU) [39].\nMeanwhile, apprehending credit card scammers often\nrelies on the availability and quality of data. Law enforce-\nment agencies and financial institutions utilize transactional",
  "(BiGRU) [39].\nMeanwhile, apprehending credit card scammers often\nrelies on the availability and quality of data. Law enforce-\nment agencies and financial institutions utilize transactional\ndata, along with advanced machine learning algorithms,\nto identify suspicious patterns indicative of fraud [24].\nCollaboration between banks and cybersecurity firms enables\nreal-time monitoring and alerts, which are crucial in catching\nfraudsters. For instance, data-sharing agreements allow for\nthe aggregation of data across different banks, providing\na broader view of fraudulent activities. This collaborative\neffort enhances the detection and prevention of credit\ncard fraud, thereby increasing the chances of apprehending\nscammers [40]. Furthermore, anonymized and synthetic data\ngeneration techniques can be used to augment training\ndatasets, allowing models to better generalize and detect\nnew types of fraud, ultimately aiding in the apprehension of\nscammers.\nB. RELATED REVIEWS",
  "datasets, allowing models to better generalize and detect\nnew types of fraud, ultimately aiding in the apprehension of\nscammers.\nB. RELATED REVIEWS\nSeveral research works have examined fraud detection in\nmany reviews and surveys that have appeared in peer-\nreviewed articles. For instance, Modi and Dayma [41]\npresented reviews regarding the application of ML in\ndetecting credit card fraud. Lucas and Jurgovsky [42]\nexamined the difficulties in detecting credit card fraud. They\nconcentrated on methods proposed to handle the concept drift\nand imbalance problems, which are two major challenges\nfaced when analysing credit card transaction data. Concept\ndrift occurs when the statistical properties of the data used\nto train an ML model change over time. As a result, the\nmodel may function differently than intended or produce\nless accurate predictions. The review provided a detailed\ndiscussion of concept drift, imbalance classification and\napproaches to handling them.",
  "less accurate predictions. The review provided a detailed\ndiscussion of concept drift, imbalance classification and\napproaches to handling them.\nAl-Hashedi and Magalingam [43] provided a broad review\nof fraud detection, including insurance, credit card, and\nother financial fraud. The review described the ML methods\nused for the different fraud detection problems. Additionally,\ndatasets and performance evaluation metrics were discussed.\nAlso, the article lists the benefits and drawbacks of each ML\nmethod. Nevertheless, the review is limited to the following\nML techniques: SVM, logistic regression, artificial neural\nnetwork, k-nearest neighbor (KNN), GA, Bayesian network,\ndecision tree, fuzzy logic, and hidden Markov model.\nPopat and Chaudhary [44] examined several ML-based\nCCFD studies, focusing on the difficulties encountered by\nthe ML models when detecting fraud. The methods studied\ninclude logistic regression, SVM, decision tree, ANN, and",
  "CCFD studies, focusing on the difficulties encountered by\nthe ML models when detecting fraud. The methods studied\ninclude logistic regression, SVM, decision tree, ANN, and\nBayesian Belief Network.. Ryman-Tubb et al. [45] conducted\na review and analysed current techniques for detecting card\nfraud via transactional volumes. The methods reviewed\ninclude SVM, KNN, CNN, MLP, decision tree, and random\nforest. Pandey et al. [46] reviewed CCFD, focusing on the\ndifferent types and statistics of credit card fraud in India.\nAlamri and Ykhlef [47] presented a survey of credit\ncard fraud detection studies that employed sampling tech-\nniques after identifying the imbalance class problem as\nthe main challenge researchers face when building CCFD\nmodels. The study considered oversampling techniques, such\nas synthetic minority oversampling technique (SMOTE)\nand Borderline-SMOTE, undersampling methods, such as\nrandom undersampling (RUS) technique and Tomek links,",
  "as synthetic minority oversampling technique (SMOTE)\nand Borderline-SMOTE, undersampling methods, such as\nrandom undersampling (RUS) technique and Tomek links,\nand hybrid sampling methods, such as SMOTE-ENN and\nSMOTE-Tomek links. The study identified hybrid sampling\nmethods as more efficient in handling the imbalance class\nproblem in CCFD, while noting that oversampling techniques\ncan lead to overfitting and undersampling can discard\nessential samples.\nWhile several reviews examine credit card fraud detection\nsystems, most of them have a very narrow scope, such\nas those focusing on sampling techniques [47], where the\nauthors specifically reviewed studies that aimed to solve\nthe imbalance problem in credit card fraud detection using\nresampling methods, showing the importance of effective\ndata resampling. Meanwhile, some of the reviews have a\nbroad scope, including [41], [43], and [44]. While they\ntouched on vital areas of fraud detection, they have some",
  "data resampling. Meanwhile, some of the reviews have a\nbroad scope, including [41], [43], and [44]. While they\ntouched on vital areas of fraud detection, they have some\nlimitations. For instance, Modi and Dayma [41] focused on\nperformance evaluation of the machine learning algorithms\nwithout delving deep into the inner workings of the algo-\nrithms, Al-Hashedi and Magalingam [43] only surveyed the\nperiod 2009 to 2019, and Popat and Chaudhary [44] reviewed\nselected ML algorithms. Meanwhile, credit card fraud has\nincreased considerably in recent years, and considering the\nrobustness of deep learning methods in different areas,\nit has become imperative to explore their applicability and\nperformance in credit card fraud detection. Therefore, this\nstudy aims to review deep learning methods and their\nperformance in detecting credit card fraud. In addition, this\nreview also covers specific gaps in related reviews, such\nas identifying and reviewing suitable evaluation metrics,",
  "performance in detecting credit card fraud. In addition, this\nreview also covers specific gaps in related reviews, such\nas identifying and reviewing suitable evaluation metrics,\nchallenges in building CCFD models, and potential solutions.\nIII. CREDIT CARD DATASETS\nDue to privacy and security concerns, credit card datasets\nare not easily accessible. However, there are a few publicly\navailable credit card datasets that are used for fraud detection,\nVOLUME 12, 2024 96895",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nand they are described in this section. Meanwhile, other\npublicly available credit card datasets are not considered in\nthis section since they were not curated for fraud detection.\nFor example, the Taiwan and Australian credit card datasets\nwere designed for credit card default and risk prediction. The\nfraud detection datasets include the following:\n• European credit card dataset: The European credit card\ndataset [48] has been widely used by researchers in\nbuilding robust CCFD models. This dataset contains\n284,807 transactions from European countries, which\nhave been labeled as either normal or fraudulent. Each\ntransaction includes 28 features, such as time of the\ntransaction, amount, and various anonymized variables.\nThe dataset has become a benchmark for evaluating\nthe performance of fraud detection algorithms. Of the\n284,807 transactions, only a tiny fraction (0.17%)",
  "The dataset has become a benchmark for evaluating\nthe performance of fraud detection algorithms. Of the\n284,807 transactions, only a tiny fraction (0.17%)\nbelong to the positive class (i.e., fraud transactions),\nwhile the majority class (99.83%) represents the neg-\native class or legitimate transactions. This imbalanced\ndistribution poses a significant challenge for many\nmachine learning algorithms and requires careful con-\nsideration during model development. The dataset was\nreleased in 2013, and while it is older, it remains relevant\nfor current research due to its comprehensive nature.\n• Brazilian credit card dataset: This dataset was obtained\nfrom a large Brazilian bank, and it contains 374,823\ntransactions [29]. The fraud samples comprise 3.74% of\nthe records. Each record in the dataset has 17 numerical\nfeatures, including merchant category code, post/zip\ncode of current and previous transactions, current\ntransaction amount, previous transaction amount, trans-",
  "features, including merchant category code, post/zip\ncode of current and previous transactions, current\ntransaction amount, previous transaction amount, trans-\naction type (card present), credit limit, card type (e.g.,\nMastercard, Visa, Diners), local/international transac-\ntion, previous transaction fraud score, and time since\nlast transaction. Despite its age, this dataset provides\nvaluable insights into transaction patterns and fraud\ndetection.\n• IEEE-CIS Fraud Detection Dataset: Released in 2019,\nthe IEEE-CIS dataset is one of the more recent publicly\navailable datasets [49]. It contains transaction data\nprovided by Vesta Corporation and includes a mix of\nfraud and non-fraud transactions over a period. The\ndataset consists of two files: one with identity infor-\nmation and another with transaction details, comprising\napproximately 590,000 transactions. Features include\ndevice type, device information, card information,\ntransaction amount, and timestamp. The dataset is",
  "approximately 590,000 transactions. Features include\ndevice type, device information, card information,\ntransaction amount, and timestamp. The dataset is\nhighly imbalanced, with a small fraction representing\nfraudulent transactions.\n• PaySim Synthetic Dataset: PaySim is a synthetic dataset\ngenerated using a simulation based on real transaction\ndata [50]. Although it is not real-world data, it was\ncreated to mimic the transaction behaviors found in a\nreal financial institution. Released in 2017, the dataset\nincludes features such as transaction type, amount,\nbalance, and origin and destination accounts. PaySim is\nvaluable for its realistic simulation of fraud scenarios,\nand it contains over 6 million transactions.\nIV. OVERVIEW OF DEEP LEARNING\nIn this Section, an overview of DL is presented, including\na detailed description of the widely used DL architectures.\nDeep learning, a branch of ML, maps input data to\nnew representations or generates predictions using neural",
  "a detailed description of the widely used DL architectures.\nDeep learning, a branch of ML, maps input data to\nnew representations or generates predictions using neural\nnetworks [51]. Meanwhile, neural networks consist of\ninterconnected neurons with weighted connections. The\nneuron converts its input into a single output by summing its\nweighted inputs using a non-linear activation function [52].\nThe network’s weight parameters are modified using gradient\ndescent optimisation, reducing the loss function, i.e., the\ndiscrepancy between the expected and actual outputs. A neu-\nral network can have one or more hidden layers. A neural\nnetwork with one hidden layer is often referred to as a shallow\nnetwork, while a network with many hidden layers is called\na deep neural network (DNN). Figure 9 shows a general\nshallow neural network (or simple ANN) and deep neural\nnetwork architectures, where the latter has multiple hidden\nlayers.\nFurthermore, deep learning is a broader term used to",
  "shallow neural network (or simple ANN) and deep neural\nnetwork architectures, where the latter has multiple hidden\nlayers.\nFurthermore, deep learning is a broader term used to\ndescribe ML techniques that are based on neural networks\nwith many layers (deep architectures). Deep learning can\nbe unsupervised, semi-supervised, or supervised [54]. Deep\nlearning methods perform better than shallow machine\nlearning methods in most applications with big and high-\ndimensional data [55], [56]. Additionally, the ability of deep\nlearning to achieve excellent performance when the data\nincreases sets it apart from conventional machine learning.\nBecause DL architectures can handle massive datasets to\ncreate an efficient data-driven model, it is beneficial when\nworking with large volumes of data, such as credit card\ntransaction data [56], [57].\nDeep-learning architectures include DNN, RNN, CNN,\ntransformers, and deep reinforcement learning. These DL",
  "working with large volumes of data, such as credit card\ntransaction data [56], [57].\nDeep-learning architectures include DNN, RNN, CNN,\ntransformers, and deep reinforcement learning. These DL\narchitectures have produced results that are as good as\nhuman expert performance and sometimes outperforming\nthe human experts in domains such as image recognition,\nnatural language processing, computer vision, and speech\nrecognition. Meanwhile, RNNs are well-suited for sequential\ndata modelling, such as credit card transactions, and are a\nsignificant focus of this study. Though RNNs can model\nsequence data effectively, they are challenging to train due\nto issues with vanishing and exploding gradients [58], which\nled Hochreiter and Schmidhuber [59] to develop the LSTM\nto tackle the vanishing gradients problem effectively. The\nGRU, first presented in [60], manages the data and performs\nLSTM-like tasks without requiring an additional memory",
  "to tackle the vanishing gradients problem effectively. The\nGRU, first presented in [60], manages the data and performs\nLSTM-like tasks without requiring an additional memory\nunit. The bidirectional variants of these networks are unique\nvariations that enable the system to forecast the current state\nmore accurately by utilising data from subsequent time steps\nin addition to earlier time steps. The following subsections\npresent brief but concise overview of these deep learning\nmethods.\n96896 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nFIGURE 1. Simple ANN vs deep learning [53].\nA. MULTILAYER PERCEPTRON\nMultilayer Perceptron is a feedforward artificial neural\nnetwork for supervised learning problems. MLP is considered\nthe foundation network of deep learning or deep neural\nnetworks. It is a fully connected network comprising an input\nlayer where input data is received, one or more hidden layers\nthat serve as the neural network’s computational engine, and\nan output layer that makes the prediction based on the given\ninputs [61]. Furthermore, backpropagation, a supervised\nlearning algorithm, is widely utilised for training the MLP.\nThe backpropagation is considered as the primary building\nblock of network networks. Meanwhile, the widely used error\nfunction in the MLP is the mean squared error, represented as\nfollows:\nE = 1\n2\nn∑\ni=1\n|| pi − ti ||2 (1)\nwhere n represents the sample size, while pi and ti represent",
  "function in the MLP is the mean squared error, represented as\nfollows:\nE = 1\n2\nn∑\ni=1\n|| pi − ti ||2 (1)\nwhere n represents the sample size, while pi and ti represent\nthe predicted and actual outputs for the i − th sample.\nMeanwhile, the MLP network uses activation functions to\ndetermine its output, and examples of the activation functions\ninclude Softmax, rectified linear unit (ReLu), Sigmoid, and\nhyperbolic tangent (Tanh) [62]. In training the MLP, different\noptimisation techniques can be used, including stochastic\ngradient descent (SGD) and adaptive moment estimation\n(Adam). Lastly, the MLP hyperparameters mainly need to\nbe tuned for optimal performance, and these hyperparameters\ninclude the number of neurons, hidden layers, and iterations.\nB. CONVOLUTIONAL NEURAL NETWORK\nThe convolutional neural network is a well-known deep\nlearning architecture with wide applications in image\nrecognition [63], [64], [65], achieving state-of-the-art per-",
  "The convolutional neural network is a well-known deep\nlearning architecture with wide applications in image\nrecognition [63], [64], [65], achieving state-of-the-art per-\nformances, and has recently been applied in several cCCFD\nmodels [66], [67]. It consists of neurons that have learnable\nweights and biases. Meanwhile, the CNN’s hidden layers\nare made up of convolutional, pooling, and fully connected\nlayers [68]. A CNN showing this multi-layer architecture\nis shown in Figure 2. The convolutional layer, CNN’s core\ncomponent, uses learnable filters to compute a convolution\noperation on the input. A set of feature maps is produced\nafter the convolution operation. The pooling layer is utilized\nto reduce the feature maps’ spatial dimensions [69]. After\nthe feature extraction and downsampling by the convolutional\nand pooling layers, respectively, their output is mapped by the\nfully connected layers to the final output of the CNN [70]. For",
  "the feature extraction and downsampling by the convolutional\nand pooling layers, respectively, their output is mapped by the\nfully connected layers to the final output of the CNN [70]. For\na classification problem like CCFD, this mapping returns the\nprobability for each class (fraud or not fraud).\nC. SIMPLE RNN\nConventional neural networks assume that each unit in the\ninput vectors is independent. As a result, sequential data\ncannot be predicted by the typical neural network. However,\nrecurrent neural networks are built to have time series\nmemory, making them suitable for processing sequential\ndata [72]. They can effectively model temporal dependencies\nin the data. Figure 3 shows a simple RNN architecture.\nThe autoregressive architecture of RNNs allows them to\nmaintain a hidden state that can capture information from\nprior time steps. This feature is significant when working\nwith sequential data like credit card transaction records. In the",
  "maintain a hidden state that can capture information from\nprior time steps. This feature is significant when working\nwith sequential data like credit card transaction records. In the\nsimple RNN implementation, the current hidden state ht is\ncomputed according to :\nht = tanh(Uxt + Wht−1) (2)\nwhere U is the matrix of trainable weights for the input xt ,\nW is the matrix of trainable weights for the previous hidden\nstate ht−1, and tanh is the activation function applied element-\nwise.\nD. LONG SHORT-TERM MEMORY\nThe LSTM network is a well-known RNN variant that\nwas developed primarily to solve the vanishing gradient\nissue associated with the simple RNN, which made it\nchallenging to identify long-term dependencies in the data.\nLSTMs have unique gating mechanisms that enable them to\nstore information better over longer sequences, as shown in\nFigure 4.\nVOLUME 12, 2024 96897",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nFIGURE 2. CNN Architecture [71].\nFIGURE 3. The architecture of Simple RNN.\nFIGURE 4. Architecture of the LSTM network [73].\nLSTM models have performed excellently in many time\nseries prediction applications, including credit card fraud\ndetection. It comprises three types of gates: forget, output,\nand input [18]. An LSTM’s forget gate decides what data\nfrom the previous hidden state should be kept or discarded.\nDuring training, the LSTM can focus on relevant inputs\nbetter and maintain a steady gradient by selectively ignoring\ncertain information. Only necessary information is added\nto the cell state because the input gate regulates new data\nflow into the cell state. The output gate then uses the\nupdated cell state and the input data to determine what\ninformation should be transmitted to the next block. The\ngating mechanism enables LSTM to extract the sequence’s\nFIGURE 5. A stacked LSTM.",
  "updated cell state and the input data to determine what\ninformation should be transmitted to the next block. The\ngating mechanism enables LSTM to extract the sequence’s\nFIGURE 5. A stacked LSTM.\nlong-term properties. Assuming xt is the input, the following\nfunctions are computed by the LSTM cell :\nit = σ(Vixt + Wiht−1 + bi) (3)\nft = σ(Vf xt + Wf h(t−1) + bf ) (4)\n˜ct = tanh(Vcxt + Wch(t−1) + bc) (5)\nct = ft ⊗ c(t−1) + it ⊗ ˜ct (6)\not = σ(Voxt + Woh(t−1) + bo) (7)\nht = ot ⊗ tanh(ct ) (8)\nwhere it , ft , ct , and ot denote the input, forget, cell, and output\ngates, respectively. Meanwhile, V∗, W∗, and b∗ represent the\nlearnable parameters, while h∗ represents the hidden state.\nFurthermore, σ is the sigmoid activation function and ⊗\ndenotes the element-wise product [74].\nThe standard LSTM network is made up of one hidden\nLSTM layer and a feedforward output later, but it can be\nextended to have many hidden layers and every layer to have",
  "The standard LSTM network is made up of one hidden\nLSTM layer and a feedforward output later, but it can be\nextended to have many hidden layers and every layer to have\nseveral memory cells, and this is called a stacked LSTM\nnetwork. By stacking the LSTM hidden layers, the network\nbecomes deeper, which is important because the success\nof deep learning models has been linked to how deep the\nnetwork is [75]. A general block diagram of a stacked LSTM\nis shown in Figure 5.\nE. GATED RECURRENT UNIT\nThe GRU was introduced by Cho et al. [60]. They also have\ngating mechanisms that aid in managing the information\nflow inside the network but without an output gate. A model\ncan be trained using the GRU to keep previous information\nor remove irrelevant information. The GRU architecture is\nshown in Figure 6. In contrast to LSTMs, GRUs have a\nsimpler architecture with just two gates: an update gate zt and\na reset gate rt . The reset gate regulates how much the previous",
  "shown in Figure 6. In contrast to LSTMs, GRUs have a\nsimpler architecture with just two gates: an update gate zt and\na reset gate rt . The reset gate regulates how much the previous\nhidden state influences the current hidden state, whereas the\nupdate gate controls how much the prior hidden state is\nkept [76]. With a less complex and more computationally\n96898 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nefficient network than LSTMs, GRUs are able to capture\nlong-term dependencies in sequential data more effectively\nbecause of this gating mechanism. The functions that a GRU\ncell computes are as follows :\nrt = σ(Vr xt + Wr h(t−1) + br ) (9)\nzt = σ(Vzxt + Wzh(t−1) + bz) (10)\n˜ct = tanh(Vcxt + Wc(rt ⊗ ht−1) + bc) (11)\nct = (1 − zt ) ⊗ c(t−1) + zt ⊗ ˜ct (12)\nht = ct (13)\nwhere Wr , Wz, Vr , and Vz are weight matrices while br and\nbz represent the bias vectors [77].\nFIGURE 6. Architecture of the GRU network.\nF. BIDIRECTIONAL LONG SHORT-TERM MEMORY\nUnidirectional Long Short-Term Memory, or LSTM, only\nstores past data since its inputs are limited to the past and must\nbe fed in the correct order. On the other hand, a bidirectional\nlong short-term memory network (BiLSTM) combines two\nhidden states to process the inputs in both forward and\nbackward directions, as shown in Figure 7. This feature",
  "long short-term memory network (BiLSTM) combines two\nhidden states to process the inputs in both forward and\nbackward directions, as shown in Figure 7. This feature\nallows the network to store information from incoming\ninputs, guaranteeing that information about previous and\nupcoming states is always accessible. In other words,\na BiLSTM is precisely like an LSTM, except that it employs\nhistorical and future data to compute the weights [17]. The\nBiLSTM’s network structure comprises two LSTMs with\nopposite information propagation directions. At each time\nunit, the current pre-hidden state output and post-hidden\nstate output are derived and recorded. The BiLSTM’s output\nvalue is then determined by connecting the two hidden states.\nThe mathematical formulation of the BiLSTM network is as\nfollows:\nhf\nt = LSTM(xt , hf\nt−1) (14)\nhb\nt = LSTM(xt , hb\nt−1) (15)\not = Wf .hf\nt + Wb.hb\nt + b (16)\nwhere LSTM(·) represent the mapping of the LSTM layers,",
  "follows:\nhf\nt = LSTM(xt , hf\nt−1) (14)\nhb\nt = LSTM(xt , hb\nt−1) (15)\not = Wf .hf\nt + Wb.hb\nt + b (16)\nwhere LSTM(·) represent the mapping of the LSTM layers,\nwhile Wf and Wb are the weight matrix of the forward and\nbackward LSTM layers, and b is the output layer’s deviation\nvector. Furthermore, BiLSTM models are substantially more\nefficient in natural language processing and can outperform\nconventional unidirectional LSTMs in time series predic-\ntion [78]. Because of its dual model architecture, BiLSTMs\nhave the drawback of requiring longer training times.\nFIGURE 7. The architecture of the BiLSTM network [78].\nG. BIDIRECTIONAL GATED RECURRENT UNIT\nBidirectional gated recurrent unit (Bi-GRU) is a variant\nof the popular GRU recurrent neural network designed to\nimprove temporal modelling accuracy. It is an example\nof a bi-directional RNN, meaning it can process input\nsequences in both forward and backward directions. In recent\nyears, Bi-GRU has become a popular choice for temporal",
  "of a bi-directional RNN, meaning it can process input\nsequences in both forward and backward directions. In recent\nyears, Bi-GRU has become a popular choice for temporal\nmodelling in deep learning applications. It is an efficient\nmodel that combines forward and backwards information\npropagation to improve accuracy when predicting future\nevents or sequences. Its main strengths over GRU are its\nability to capture bidirectional dependencies and its improved\nperformance in tasks involving long-term dependencies.\nIn the Bi-GRU implementation, the input sequences are\ncomputed in both directions using two sublayers, modelling\nforward and backward hidden sequences, which are com-\nbined to obtain the current hidden state ht and output ot of the\nnetwork [79]. The mathematical formulation is represented\nby the following :\nhf\nt = GRU(xt , hf\nt−1) (17)\nhb\nt = GRU(xt , hb\nt−1) (18)\nht = Wf .hf\nt + Wb.hb\nt ) (19)\not = φ(W oht ) (20)\nwhere hf\nt and hb\nt represent the forward and backward",
  "by the following :\nhf\nt = GRU(xt , hf\nt−1) (17)\nhb\nt = GRU(xt , hb\nt−1) (18)\nht = Wf .hf\nt + Wb.hb\nt ) (19)\not = φ(W oht ) (20)\nwhere hf\nt and hb\nt represent the forward and backward\nhidden sequences, the GRU function denotes the nonlinear\ntransformation of the input, W o represents the weight\ncoefficient in the network’s hidden and output layers, and φ\ndenotes the activation function applied to the output layer.\nH. TRANSFORMER MODELS\nWhile traditional deep learning architectures, such as CNNs,\nLSTM, and GRU, have been widely used in fraud detection\nand have achieved excellent performance, they have limita-\ntions, particularly in capturing long-range dependencies and\nprocessing large-scale datasets efficiently. Transformer mod-\nels have recently gained attention in the field of credit card\nVOLUME 12, 2024 96899",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nfraud detection due to their robust performance in sequence\nmodelling and anomaly detection tasks. Unlike traditional\nRNNs and CNNs, Transformers employs a self-attention\nmechanism that allows them to weigh the importance of\ndifferent parts of an input sequence dynamically. The core\ncomponent of a Transformer model is the self-attention\nmechanism, which computes attention scores for each pair of\ntokens in the input sequence [80]. The attention mechanism\nis defined as follows :\nAttention(Q, K, V ) = softmax\n(QKT\n√dk\n)\nV (21)\nwhere Q (queries), K (keys), and V (values) are the input\nmatrices, and dk is the dimension of the keys. The softmax\nfunction ensures that the attention scores sum to one,\nhighlighting the most relevant tokens [81]. Meanwhile, the\nTransformer model uses multiple self-attention heads to\ncapture different aspects of the relationships within the input\nsequence:",
  "highlighting the most relevant tokens [81]. Meanwhile, the\nTransformer model uses multiple self-attention heads to\ncapture different aspects of the relationships within the input\nsequence:\nMultiHead(Q, K, V )=Concat(head1, head2, . . . ,headh)W O\n(22)\nwhere each attention head head i is computed as :\nheadi = Attention(QW Q\ni , KW K\ni , VW V\ni ) (23)\nwhere W Q\ni , W K\ni , and W V\ni are learned projection matrices,\nand W O is the output projection matrix. Furthermore, Trans-\nformers can be pre-trained on large datasets and fine-tuned\non specific fraud detection tasks, leveraging transfer learning\nto improve performance. The pre-training phase typically\ninvolves learning general representations from a large corpus\nof transaction data, while the fine-tuning phase adapts these\nrepresentations to the specific characteristics of fraudulent\ntransactions.\nLtotal = Lpre-train + λLfine-tune (24)\nwhere Lpre-train is the loss during the pre-training phase,",
  "representations to the specific characteristics of fraudulent\ntransactions.\nLtotal = Lpre-train + λLfine-tune (24)\nwhere Lpre-train is the loss during the pre-training phase,\nLfine-tune is the loss during the fine-tuning phase, and λ is a\nweighting factor.\nV. PERFORMANCE EVALUATION METRICS\nAn essential step in ensuring effective credit card fraud\ndetection is the performance metrics used to assess the\nmodel’s prediction performance. This section discusses\nmetrics that are widely applied and their suitability in\ncredit card fraud detection. The confusion matrix provides a\nsummary of the binary classification results, and it is shown\nin Table 1, indicating true Positive (TP), true negative (TN),\nfalse position (FP), and false negative (FN).\nAccuracy = TP + TN\nTP + TN + FP + FN (25)\nErrorRate = 1 − Accuracy (26)\nWhen assessing the performance of ML models, the\nmost commonly utilised measures are accuracy and error\nTABLE 1. Confusion matrix.",
  "TP + TN + FP + FN (25)\nErrorRate = 1 − Accuracy (26)\nWhen assessing the performance of ML models, the\nmost commonly utilised measures are accuracy and error\nTABLE 1. Confusion matrix.\nrate [82], [83], i.e., Equation 25 and Equation 26, respectively.\nHowever, when dealing with CCFD, which is mostly an\nimbalance classification task, neither is sufficient because the\nmajority class, or the non-fraud class, dominates the final\nvalue. Hence, a naïve classifier can achieve 99% accuracy\nby labelling all samples as not fraud when given input data\nwhere the positive class distribution makes up only 1% of the\ndata set. There would be no actual benefit to such a model.\nOther commonly used metrics are sensitivity, specificity, and\nprecision, and their mathematical formulations are shown\nbelow:\nPrecision = TP\nTP + FP (27)\nSensitivity = TP\nTP + FN (28)\nSpecificity = TN\nTN + FP (29)\nPrecision represents the fraction of positively predicted\nsamples that are classified correctly. Because precision",
  "TP + FP (27)\nSensitivity = TP\nTP + FN (28)\nSpecificity = TN\nTN + FP (29)\nPrecision represents the fraction of positively predicted\nsamples that are classified correctly. Because precision\ntakes into account the number of negative instances that\nare wrongly predicted as positive, it is sensitive to class\nimbalance [84], [85]. However, precision on its own is\ninadequate as it does not reveal how many positive instances\nwere mistakenly classified as negative. Sensitivity, also\nknown as the true positive rate (TPR), quantifies the\nproportion of the positive instances that the classifier\naccurately predicted to be positive. The class imbalance has\nno effect on sensitivity since it solely depends on the positive\nclass. Meanwhile, the number of negative instances that are\nincorrectly classified as positive is not taken into account by\nsensitivity. Specificity, also called true negative rate (TNR),\nquantifies the proportion of the negative instances that were",
  "incorrectly classified as positive is not taken into account by\nsensitivity. Specificity, also called true negative rate (TNR),\nquantifies the proportion of the negative instances that were\nclassified correctly. Furthermore, there are other metrics\nthat tend to combine earlier discussed metrics to obtain a\nmore comprehensive evaluation of the model performance,\nincluding F-measure, G-mean, and balance accuracy. Their\nmathematical formulations are shown as follows :\nF − Measure = 2XPrecisionXSensitivity\nPrecision + Sensitivity (30)\nG − Mean =\n√\nSensitivityXSpecificity (31)\nBalancedAccuracy = Sensitivity + Specificity\n2 (32)\nF-measure is the harmonic mean of sensitivity and preci-\nsion. It presents a way to combine sensitivity and precision\ninto one metric that captures the properties of both metrics.\nThe geometric mean (G-mean) combines specificity and\nsensitivity into one metric that considers a balance between\nboth minority and majority class performances. Like G-mean,",
  "The geometric mean (G-mean) combines specificity and\nsensitivity into one metric that considers a balance between\nboth minority and majority class performances. Like G-mean,\nthe balanced accuracy metric computes a metric sensitive to\n96900 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nthe minority class instances by combining TPR and TNR.\nWhen evaluating the performance of classifiers trained with\nimbalanced data, F-measure, G-Mean, and balanced accuracy\nare better metrics compared to accuracy and error rate [86].\nFurthermore, the receiver operating characteristic (ROC)\ncurve, the area under the ROC curve (AUC), and the\nprecision-recall curve are other important metrics. The ROC\ncurve is a plot of the true positive rate versus the false positive\nrate at various classification thresholds, and it demonstrates\nthe ability of a classifier to distinguish between the positive\nand negative classes [65]. The AUC is a summary of the ROC\ncurve. It has a value range of 0 to 1, with 1 indicating that all of\nthe classifier’s predictions are accurate and 0 indicating that\nall of the predictions are incorrect. The precision-recall (PR)\ncurve demonstrates the tradeoff between precision and recall",
  "the classifier’s predictions are accurate and 0 indicating that\nall of the predictions are incorrect. The precision-recall (PR)\ncurve demonstrates the tradeoff between precision and recall\nat various thresholds [87]. Since high precision indicates a\nlow false positive rate, while high recall indicates a low false\nnegative rate, an area under the precision-recall curve with a\nhigh value indicates high recall and precision values.\nIn imbalance classification tasks, such as credit card\nfraud detection, the ROC curve can be misleading because\na small number of correct or wrong classifications can\nlead to a significant change in the ROC curve or AUC\nvalue. Meanwhile, the PR curve focuses on the minority\nclass, making it a more suitable metric for imbalance\nclassification [88]. Hence, the PR curve, together with\nother metrics previously discussed, is recommended for\nimbalanced credit card fraud detection.\nVI. DEEP LEARNING APPLICATIONS IN CREDIT CARD\nFRAUD DETECTION",
  "other metrics previously discussed, is recommended for\nimbalanced credit card fraud detection.\nVI. DEEP LEARNING APPLICATIONS IN CREDIT CARD\nFRAUD DETECTION\nBenchaji et al. [23] developed a CCFD model via sequential\nmodelling of the credit card data using deep LSTM neural\nnetworks and attention mechanisms. The proposed approach\ntakes into account the sequential nature of the credit card data\nand enables the classifier to determine which transactions\nin the input sequence are the most significant. Specifically,\nin the proposed approach, the LSTM was used to ensure\nsequential modelling of the data, the attention mechanism\nwas employed to improve the performance of the LSTM, and\nthe uniform manifold approximation and projection (UMAP)\nwas introduced to select the most significant attributes. The\nmodels yielded good performance with an accuracy of 96.7%.\nSimilarly, Femila et al. [89] developed a credit card fraud\ndetection model with the aim of lowering losses caused by",
  "models yielded good performance with an accuracy of 96.7%.\nSimilarly, Femila et al. [89] developed a credit card fraud\ndetection model with the aim of lowering losses caused by\ncredit card fraud. This study aimed to identify credit card\nfraud using an LSTM model. An attention mechanism was\nalso incorporated to boost the LSTM’s performance since\nmodels with such a structure have shown to be effective\nin sequence modelling. Other classifiers like SVM, naive\nBayes, and ANN were contrasted with the LSTM, and the\nexperimental results showed that the LSTM yielded robust\noutcomes, including an accuracy of 100%.\nNajadat et al. [90] developed a model based on BiLSTM\nand BiGRU with MaxPooling layers. Meanwhile, the dataset\nwas preprocessed using three resampling techniques: random\noversampling, random undersampling, and SMOTE. The\nstudy compared the performance of the deep learning-based\nclassifier and other ML classifiers, including logistic regres-",
  "oversampling, random undersampling, and SMOTE. The\nstudy compared the performance of the deep learning-based\nclassifier and other ML classifiers, including logistic regres-\nsion, random forest, voting, naïve base, AdaBoost, and\ndecision tree. When random oversampling was applied, the\nproposed BiLSTM-BiGRU obtained excellent performance,\nwith an AUC of 91.4%.\nForough and Momtazi [91] developed a credit card fraud\ndetection model that considers the sequential structure of\ncredit card transactions. The method used LSTM models\nas base classifiers in an ensemble implementation, where a\nfeed-forward neural network (FFNN) was used as the voting\nmechanism. The proposed LSTM ensemble outperformed\nother ML and DL techniques when experimented on two\ncredit card datasets. Specifically, the proposed ensemble\nachieved an AUC of 0.879 and 0.88 on the European and\nBrazilian datasets.\nAurna et al. [92] proposed a federated learning (FL)\nbased CCFD approach in order to protect the privacy of",
  "achieved an AUC of 0.879 and 0.88 on the European and\nBrazilian datasets.\nAurna et al. [92] proposed a federated learning (FL)\nbased CCFD approach in order to protect the privacy of\nsensitive credit card data. This allows the model to be trained\nwithout exposing credit card information to third parties on\nthe cloud. The study considered three deep learning models\nbased on LSTM, MLP, and CNN. The influence on the\nconventional centralised and FL systems is then examined\nusing four different sampling procedures to address the data\nimbalance problem. The proposed approach was compared\nwith other well-performing methods in the literature, and\nthe experimental results show that the proposed method\nobtains excellent performance with accuracies for CNN,\nMLP, and LSTM models being 99.51%, 98.77%, and 98.20%,\nrespectively.\nXie et al. [93] developed a time-aware attention-based\ninteractive LSTM (TAI-LSTM) method for credit card\nfraud detection. The method contains two time-aware gates,",
  "respectively.\nXie et al. [93] developed a time-aware attention-based\ninteractive LSTM (TAI-LSTM) method for credit card\nfraud detection. The method contains two time-aware gates,\na time-aware attention module and an interaction module.\nThe approach was built to capture the customer’s long\nand short-term spending behaviour and detect behavioural\nchanges over time. The time-aware attention model aims to\nextract behavioural information from the sequential credit\ncard data, while the interactive module aims to acquire\nmore thorough and logical representations. The findings\ndemonstrate that the learned representation can accurately\ndifferentiate between fraudulent and genuine behaviours and\nthat the suggested approach outperforms similar methods\nwith a sensitivity of 99.6%.\nSehrawat and Singh [21] used an auto-encoder with\nLSTM and GRU neural networks to detect credit card\nfraud. In the proposed approach, the autoencoder performed\nrepresentation learning from the data, which was achieved",
  "LSTM and GRU neural networks to detect credit card\nfraud. In the proposed approach, the autoencoder performed\nrepresentation learning from the data, which was achieved\nby excluding the class labels. The auto-encoder’s output\ncombined with the class labels were supplied as input to the\nLSTM and GRU models to detect fraud. The LSTM obtained\na classification accuracy of 99.1%.\nAjitha et al. [94] compared the performance of a CNN\nmodel with other ML algorithms, including XGBoost, SVM,\nrandom forest, KNN, logistic regression, and decision. The\nVOLUME 12, 2024 96901",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nCNN model consists of one flattened layer, one fully\nconnected layer, and two convolutional layers with the\nReLu activation function. The experimental results indicated\nthat the CNN obtained a classification accuracy of 97.2%,\noutperforming the other classifiers.\nYousuf Ali et al. [95] developed DL models combined with\nthe SMOTE oversampling approach to forecast credit card\nfraud. The paper employed three widely used deep learning\narchitectures: LSTM, CNN, and a DNN. The experimental\nresults showed that the CNN model achieved a significant\nincrease in accuracy after the SMOTE-based resampling,\nespecially in detecting fraud instances. The CNN obtained\nan accuracy of 99.9%. The study concluded that the CNN\narchitecture can aid in reducing financial losses due to credit\ncard fraud. Gambo et al. [19] also employed CNN for credit\ncard fraud detection, combining it with the adaptive synthetic",
  "architecture can aid in reducing financial losses due to credit\ncard fraud. Gambo et al. [19] also employed CNN for credit\ncard fraud detection, combining it with the adaptive synthetic\n(ADASYN) sampling method. After the resampling of the\ncredit card dataset, the CNN model achieved an accuracy\nof 99.8%.\nMizher and Nassif [96] presented credit card fraud\ndetection models based on the convolutional neural network\ntechnique and two machine learning algorithms: SVM\nand random forest. Using highly skewed real-world credit\ncard data, the models were assessed and contrasted, and\nthe random forest achieved the best performance with\nan accuracy of 99.7%. Meanwhile, the CNN obtained an\naccuracy of 93.5%.\nFurthermore, recent advancements in deep learning have\nseen the rise of Transformer-based models, which have\nrevolutionized various fields, including natural language\nprocessing and, more recently, fraud detection. Transformers,\nsuch as the Bidirectional Encoder Representations from",
  "revolutionized various fields, including natural language\nprocessing and, more recently, fraud detection. Transformers,\nsuch as the Bidirectional Encoder Representations from\nTransformers (BERT) model and its variants, have demon-\nstrated exceptional performance in sequence modelling and\nanomaly detection tasks due to their ability to capture\nlong-range dependencies and contextual information effec-\ntively. The application of Transformer models in credit card\nfraud detection offers several advantages over traditional\nDL architectures like LSTM and CNN. For example, their\nself-attention mechanism allows them to focus on the\nmost relevant parts of a transaction sequence, improving\nthe accuracy of fraud detection. Studies such as those\nof Igbal and Amin [97] and Tang and Liu [98] have\nexplored the application of Transformers in credit card fraud\ndetection, showing promising results, with accuracy of 100%\nand 98.98%, respectively. Table 2 summarises the deep",
  "explored the application of Transformers in credit card fraud\ndetection, showing promising results, with accuracy of 100%\nand 98.98%, respectively. Table 2 summarises the deep\nlearning methods reviewed in this study, comparing their\nperformances based on the accuracy metric.\nVII. EXPERIMENTAL RESULTS AND ANALYSIS\nIn this section, we analyze the performance of the MLP\nand deep learning architectures trained with the European\ncredit card dataset. The DL techniques include CNN, simple\nRNN, LSTM, GRU, BiLSTM, and BiGRU. To ensure a fair\ncomparison, the models were trained using the parameters\nlisted in Table 4. These parameters were chosen based on\ntheir widespread use in the literature and their effectiveness\nin similar tasks. To ensure robust evaluation, the models\nwere trained and validated using k-fold cross-validation.\nSpecifically, we used 5-fold cross-validation, where the\ndataset was randomly partitioned into five equal-sized\nsubsets. Each subset was used as a validation set once,",
  "Specifically, we used 5-fold cross-validation, where the\ndataset was randomly partitioned into five equal-sized\nsubsets. Each subset was used as a validation set once,\nwhile the remaining four subsets were used for training.\nThis process was repeated five times, and the performance\nmetrics were averaged over the five folds to provide a reliable\nestimate of model performance.\nTable 4 and Figure 8 show the performance of the various\nmodels in terms of accuracy, sensitivity, specificity, precision,\nand F-measure. Additionally, Figure 9 shows the ROC curves\nof the models. The results reveal interesting insights into the\nbehaviour of the models, especially regarding the near-perfect\naccuracy and specificity values compared to the relatively\nlower sensitivity and F-measure.\nFIGURE 8. Performance comparison.\nFIGURE 9. ROC curves of the various models.\nFirstly, all the models performed well in classifying the\nmajority class (non-fraud samples), indicated by the high",
  "FIGURE 8. Performance comparison.\nFIGURE 9. ROC curves of the various models.\nFirstly, all the models performed well in classifying the\nmajority class (non-fraud samples), indicated by the high\naccuracy and specificity scores. Specificity and accuracy\nmetrics reflect the model’s performance on the majority\nclass samples. Precisely, specificity measures the correctly\nclassified non-fraud samples or true negative rate, which\nwas very high due to the high number of negative samples.\nThe European credit card dataset is highly imbalanced,\n96902 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nTABLE 2. Summary of the DL-based credit card fraud studies.\nTABLE 3. Parameters of the various deep learning models.\nTABLE 4. Performance evaluation of the various classifiers.\nwith most records being legitimate and only a few being\nlabelled as fraud. Therefore, training the models on such an\nimbalanced dataset ensured they were efficient at classifying\nthe majority class samples, contributing to their high accuracy\nand specificity values,\nHowever, the GRU model achieved superior performance\nacross the various metrics, including having the best sensitiv-\nity. The GRU is known for its effectiveness in capturing tem-\nporal dependencies with lesser parameters compared to the\nLSTM, a possible reason for the high scores. Meanwhile, the\nhigh sensitivity value demonstrates that the GRU is relatively\nbetter at identifying fraud instances, which is crucial in credit",
  "LSTM, a possible reason for the high scores. Meanwhile, the\nhigh sensitivity value demonstrates that the GRU is relatively\nbetter at identifying fraud instances, which is crucial in credit\ncard fraud detection engines. Also, its balance between preci-\nsion and sensitivity, as indicated in the F-measure of 0.8254,\nimplies that the GRU efficiently manages the trade-off\nbetween detecting fraud instances and minimizing false\npositives.\nFurthermore, the MLP model achieved good performance,\nespecially with regard to accuracy and specificity. However,\nits sensitivity indicates a limitation in predicting fraud cases\ncompared to the GRU. Meanwhile, Simple RNN and CNN\nseem to have the lowest performance compared to the\nother models. The RNN achieved the least F-measure of\n0.772. RNNs are inefficient when faced with long-term\ndependencies due to the vanishing gradient issue, which\ncould explain the poor performance. The CNN obtained an\nF-measure of 0.780, which is better than the RNNs but less",
  "dependencies due to the vanishing gradient issue, which\ncould explain the poor performance. The CNN obtained an\nF-measure of 0.780, which is better than the RNNs but less\nthan the remaining models. Lastly, the discrepancy between\nthe high performance in the majority class samples and\nVOLUME 12, 2024 96903",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nthe poor performance in the minority class (fraud) samples\ncan be mainly attributed to the imbalanced credit card\ndata. In order to enhance the performance of the minority\nclass, researchers can explore methods such as oversampling,\nensemble learning, and cost-sensitive learning that penalize\nwrong predictions in the minority class more than the other\nclass.\nVIII. CHALLENGES AND POTENTIAL SOLUTIONS\nResearchers and practitioners usually encounter challenges\nwhen developing deep learning-based credit card fraud\ndetection models. In this section, an attempt is made to\ndiscuss some of these challenges and potential solutions.\nA. CLASS IMBALANCE\nIn binary classification problems, such as credit card fraud\ndetection, a class imbalance occurs when one class (also\ncalled the majority class) significantly outnumbers the other\nclass, known as the minority class. This results in a skewed",
  "detection, a class imbalance occurs when one class (also\ncalled the majority class) significantly outnumbers the other\nclass, known as the minority class. This results in a skewed\ndataset, making the learning process challenging for machine\nlearning algorithms. Most credit card transaction data have\na class imbalance, making it challenging to identify fraud.\nMeanwhile, the minority class (fraud samples) is more\nimportant than the majority class, and wrongly predicting\na fraud case as legitimate has a higher cost than predicting\na legitimate transaction as fraud [99]. Also, most machine\nlearning algorithms used for classification tasks assume that\nthere are equal amounts of samples in each class.\nFurthermore, class imbalance can also lead to biased\nand poor results. Therefore, addressing this problem is\nessential when building ML and DL models. Some meth-\nods to address the imbalance class problem include data\nresampling techniques like oversampling and undersampling,",
  "essential when building ML and DL models. Some meth-\nods to address the imbalance class problem include data\nresampling techniques like oversampling and undersampling,\nensemble methods that can handle imbalanced datasets,\nand cost-sensitive learning algorithms that assign different\nweights to minority and majority classes [100]. However,\nwhen building deep learning models for credit card fraud\ndetection, the following approaches have attracted a lot of\nattention from the DL community:\n• Loss function adaptation: This can be used to make deep\nlearning methods learn effectively from imbalanced\ndata. It involves changing the loss function of the DL\nmodel to make it insensitive to the skewed distribution.\nThe loss function adaptation is an algorithm-level\nmodification and has been successfully applied in\nseveral DL models [101], [102]. This approach is similar\nto cost-sensitive learning as it is based on the premise\nthat instances should not be treated equally during",
  "several DL models [101], [102]. This approach is similar\nto cost-sensitive learning as it is based on the premise\nthat instances should not be treated equally during\ntraining and that errors in minority class instances are\ncostlier than those in the majority class and, hence,\nshould be penalised more severely [103]. Based on this\nidea, Wang et al. [104] and Lin et al. [105] proposed\nmean false error and focal loss, two robust adapted\nloss functions for deep learning modelling. Other loss\nfunctions include generalised cross-entropy loss [106]\nand class-balanced loss [107], which were introduced\nmore recently.\n• Hybrid Models: Hybrid models that combine deep\nlearning algorithms with traditional machine learning\nalgorithms that are better suited for imbalance clas-\nsification, such as decision trees and random forests,\nhave been studied recently and have achieved excellent\nresults. For example, Dar et al. [108] developed a\nhybrid model, combining DNN and XGBoost and",
  "have been studied recently and have achieved excellent\nresults. For example, Dar et al. [108] developed a\nhybrid model, combining DNN and XGBoost and\nSemwal et al. [109] combined CNN with LSTM and\nGRU. The performance of hybrid methods has been\nshown to be superior to standard DL classifiers [110].\n• Ensemble methods: Ensemble learning can be employed\nin combining deep learning models. Additionally, the\nmodels can be trained on different resampled data.\nEnsemble techniques such as EasyEnsemble [111] and\nBalanced Bagging [112] are effective in creating ensem-\nble models that are well-suited to handle imbalanced\ndata.\nB. LACK OF SUFFICIENT DATA\nAt the moment, there are not enough real-world credit card\ndatasets to create reliable models for a variety of reasons,\nmostly pertaining to privacy concerns [113]. Also, most\navailable data are unlabelled. Hence, it takes extra effort\nto label the data. Therefore, a frequently used technique",
  "mostly pertaining to privacy concerns [113]. Also, most\navailable data are unlabelled. Hence, it takes extra effort\nto label the data. Therefore, a frequently used technique\nto identify fraud is anomaly detection. However, anomaly\ndetection depends on user behaviour, and any deviation can\nbe interpreted as fraud. Systems that detect anomalies rely on\nusers’ past behaviour, which has limitations.\nA potential approach used to solve this problem includes\ninstance generation using DNNs: Generative models based\non deep neural networks can be adapted to function similarly\nto oversampling methods, where artificial instances can be\neffectively introduced into a particular embedding space by\nan encoder/decoder pair. To learn the latent distribution of\ndata, researchers have successfully used generative adversar-\nial networks (GANs), variational autoencoders (V AEs), and\nWasserstein autoencoders (WAEs) [114]. These methods can\nbe expanded to generate more data for CCFD modelling.",
  "ial networks (GANs), variational autoencoders (V AEs), and\nWasserstein autoencoders (WAEs) [114]. These methods can\nbe expanded to generate more data for CCFD modelling.\nC. INTERPRETABILITY\nDeep learning models are often considered black-box models,\nmaking it challenging to interpret their results. Understanding\nwhy a transaction was classified as fraud or legitimate can be\ndifficult. Meanwhile, achieving complete interpretability in\nDL models can be difficult. The following techniques can be\nemployed to enhance the transparency and understandability\nof the deep learning model’s decision-making process,\nmaking it more useful in practical applications, such as credit\ncard fraud detection.\n• Regularization Techniques: Applying regularization\ntechniques, such as L1 regularization that encourages\nsparsity in the model’s parameters, could result in\nsimpler and more interpretable models [115].\n96904 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n• SHapley Additive exPlanations (SHAP): This method\nuses Shapley values from game theory in explaining a\ngiven prediction. The SHAP values assign contributions\nto every feature used in making the prediction [116].\nIt provides a method to understand the significance of\neach feature in the decision-making process of the deep\nlearning model.\n• Model Distillation: This technique involves transferring\nknowledge from a large model to a smaller model [117].\nExamples of large models include deep learning and\nensemble learning-based models. Using this approach,\na simpler and interpretable model can be trained on\nthe predictions of the DL model. Smaller models, such\nas decision trees and logistic regression, are easy to\ninterpret.\nD. DATA DRIFT\nMany ML models are built on the assumption that the data\ndistribution used in training and testing remains stationary.",
  "as decision trees and logistic regression, are easy to\ninterpret.\nD. DATA DRIFT\nMany ML models are built on the assumption that the data\ndistribution used in training and testing remains stationary.\nData drift, also known as covariate shift, occurs when the\ndistribution of the data used in training the model differs\nfrom the distribution of the data on which the model is\nbeing applied [118]. It is a common problem in many real-\nworld systems, such as credit card fraud detection. Therefore,\ncredit card data needs to be monitored regularly for changes\nin the statistical properties, and this can be achieved via\nvisualisation, statistical tests, and observing key metrics.\nModels trained on older data may not effectively detect new\nfraud patterns. Some of the methods used in solving this\nproblem include:\n• Incremental Learning: Incremental learning is an\napproach used to update a model with the latest data\nwhile retaining the learned knowledge from the previous",
  "problem include:\n• Incremental Learning: Incremental learning is an\napproach used to update a model with the latest data\nwhile retaining the learned knowledge from the previous\ntraining. This ensures the entire model is not retrained,\nand methods such as transfer learning, online learning,\nand fine-tuning can be used to achieve such incremental\nlearning [119].\n• Adaptive Learning Rate: The learning rate of deep\nlearning models can be adjusted during training to adapt\nto changes in the data distribution [119]. Specifically,\nlower learning rates can be used to ensure the model\nconverges to a new distribution without forgetting the\nprevious data.\n• Model retraining: A well-known method for handling\ndata drift is retraining the DL model with new data. Such\nretraining can be automated and set at regular intervals\nor manually triggered when sufficient drift is observed.\n• Ensemble modelling: Ensemble models can be used\nto combine the predictions from multiple DL models,",
  "or manually triggered when sufficient drift is observed.\n• Ensemble modelling: Ensemble models can be used\nto combine the predictions from multiple DL models,\nwhere one or more models can be used to determine\ndata drift and modify the ensemble model’s composition\naccordingly [120].\nE. PRIVACY AND SECURITY CONCERNS\nUsing credit card transaction data containing personal and\nfinancial information raises concerns about data breaches and\nunauthorized access. To address these concerns, researchers\nand practitioners must implement robust security measures,\nsuch as data anonymization and encryption, to ensure\nthe confidentiality and integrity of the data [121]. Data\nanonymization involves removing or obfuscating personally\nidentifiable information from the data while maintaining the\ncore patterns and characteristics necessary for training the\ndeep learning models. It can be achieved using generalization,\nsuppression, or perturbation techniques.",
  "core patterns and characteristics necessary for training the\ndeep learning models. It can be achieved using generalization,\nsuppression, or perturbation techniques.\nGeneralization involves substituting specific values with\nmore general categories or ranges [122]. For example,\ninstead of using exact transaction amounts, the data can be\ngrouped into ranges, such as <USD20, USD20-USD50, and\nUSD50-USD100, etc., preserving the overall distribution of\ntransaction amounts while protecting individual transaction\ndetails. Suppression entails removing sensitive attributes,\nsuch as credit card numbers and the cardholder’s name,\nensuring that no personal information is accessible. Another\nmethod for anonymizing sensitive financial data is perturba-\ntion. It entails adjusting the values of particular attributes or\nintroducing random noise [123]. In perturbation, transaction\namounts can be perturbed by adding a small random value to\neach amount, making it difficult to determine the exact values",
  "introducing random noise [123]. In perturbation, transaction\namounts can be perturbed by adding a small random value to\neach amount, making it difficult to determine the exact values\nwhile maintaining the data’s statistical properties.\nIn addition to data anonymization, encryption is crucial\nin ensuring the security of sensitive financial data used for\ntraining deep learning models. Encryption entails converting\nthe data into a format only accessible with the correct\ndecryption key [124]. It ensures that the data will remain\nunreadable and unusable even if it is intercepted or viewed\nwithout authorization. Asymmetric and symmetric encryp-\ntion are two examples of the different encryption methods\nthat can be used. Asymmetric encryption employs two keys:\na public key for encryption and a private key for decryption.\nSymmetric encryption uses a single key for both encryption\nand decryption.\nF. ETHICS AND FAIRNESS\nWhen constructing credit card fraud detection models using",
  "Symmetric encryption uses a single key for both encryption\nand decryption.\nF. ETHICS AND FAIRNESS\nWhen constructing credit card fraud detection models using\ndeep learning techniques, it is crucial to ensure that these\nmodels do not exhibit bias towards particular individuals\nor groups based on factors such as ethnicity, gender,\nor socioeconomic status [125]. One challenge in achieving\nfairness is the potential for bias in the training data. If the\ntraining data is skewed towards specific groups, the resulting\nmodel may likewise demonstrate bias in its predictions.\nFor instance, if the majority of the training data consists\nof fraudulent transactions from a particular demography,\nthe model can unfairly identify transactions from that\ndemographic as fraudulent, resulting in biased treatment.\nTo address this challenge, researchers and practitioners\nmust carefully curate the training data to ensure the inclusion\nof a wide range of demographic groups. This can be",
  "To address this challenge, researchers and practitioners\nmust carefully curate the training data to ensure the inclusion\nof a wide range of demographic groups. This can be\naccomplished by gathering data from diverse sources and\nensuring that the data is evenly distributed among different\nVOLUME 12, 2024 96905",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\ngroups. In addition, methods such as data augmentation can\nbe employed to artificially enhance the presence of under-\nrepresented groups in the training data. Another approach\nto addressing bias in deep learning models is the utilization\nof fairness metrics and algorithms. Fairness metrics measure\nthe degree of fairness or bias in the predictions made by\nthe model, whereas fairness algorithms aim to reduce any\nidentified bias [126]. For example, one approach involves\nmodifying the decision threshold of the model according\nto various demographic groupings to provide equivalent\nsensitivity to fraudulent transactions across all groups.\nG. ADAPTABILITY AND SCALABILITY\nAnother challenge is the adaptability and scalability of deep\nlearning-based credit card fraud detection models. Given the\never-changing nature of fraud, it is imperative for the models",
  "Another challenge is the adaptability and scalability of deep\nlearning-based credit card fraud detection models. Given the\never-changing nature of fraud, it is imperative for the models\nto be adaptable and have the ability to identify new and\nemerging patterns of fraudulent activity [127], [128]. One\nchallenge in achieving adaptability is the need for continuous\nmodel updates and retraining. Conventional machine learning\nmodels sometimes necessitate manual feature engineering\nand the retraining of models, which can consume significant\ntime and resources. However, deep learning models have\nthe ability to automatically learn and adapt to new patterns\nwithout requiring user intervention. Meanwhile, this requires\naccess to up-to-date and relevant data for training.\nPossible solutions to this problem include the use of\ntechniques such as transfer learning and online learning.\nTransfer learning is utilising pre-trained deep learning models",
  "Possible solutions to this problem include the use of\ntechniques such as transfer learning and online learning.\nTransfer learning is utilising pre-trained deep learning models\ntrained with large-scale datasets and fine-tuning them for the\nspecific objective of credit card fraud detection. It enables\nthe model to leverage the information and patterns acquired\nfrom the large datasets while adjusting to the particular\nfraud detection objective. Online learning enables the model\nto consistently update and acquire knowledge from newly\naccessible data. Online learning allows for incremental\nmodifications based on new data rather than retraining\nthe entire model from scratch [129]. This makes it more\nadaptable and scalable in detecting new fraud patterns.\nFurthermore, a vital aspect of the adaptability and scalability\nchallenge pertains to the computational resources necessary\nfor the training and deployment of deep learning models.",
  "Furthermore, a vital aspect of the adaptability and scalability\nchallenge pertains to the computational resources necessary\nfor the training and deployment of deep learning models.\nOrganisations with limited resources or infrastructure often\nface challenges while training deep learning models due to\nthe substantial data and processing power requirements.\nLastly, to tackle this issue, academics can investigate\nmethodologies like distributed computing and cloud com-\nputing. Distributed computing entails the distribution of\ncomputational tasks among numerous machines or nodes,\nenabling accelerated and more efficient training of deep\nlearning models. Distributed computing can be achieved\nby utilising parallel processing and distributed training\nframeworks [130]. Cloud computing enables users to access\nflexible and readily available computing resources via the\ninternet. Organisations can optimise the training and deploy-\nment of DL models by utilising cloud computing systems,",
  "flexible and readily available computing resources via the\ninternet. Organisations can optimise the training and deploy-\nment of DL models by utilising cloud computing systems,\nwhich enable them to flexibly adjust their computational\nresources according to their requirements.\nIX. DISCUSSIONS AND FUTURE RESEARCH DIRECTIONS\nDeep learning models have significantly transformed numer-\nous domains, including fraud detection. This research con-\ncisely describes the main deep learning-based architectures\nused for credit card fraud detection, including simple RNN,\nLSTM, GRU, BiLSTM, BiGRU, and CNN. The effectiveness\nof these models in real-world situations, particularly in the\ndynamic credit card fraud detection field, varies. Firstly,\nMLP has gained extensive usage in diverse applications due\nto its ability to learn complex patterns and make accurate\npredictions. However, its effectiveness in credit card fraud\ndetection needs has been examined and found to be limited.",
  "to its ability to learn complex patterns and make accurate\npredictions. However, its effectiveness in credit card fraud\ndetection needs has been examined and found to be limited.\nThe fundamental reason for this is that MLP does not possess\nthe ability to capture temporal dependencies and sequential\npatterns, which are essential in detecting fraudulent activities.\nOn the other hand, initially designed for image analysis,\nCNN has demonstrated encouraging results in detecting\ncredit card fraud, as shown in Table 2. By considering\nthe transaction data as a two-dimensional image, CNN can\neffectively extract relevant features and identify fraudulent\npatterns. However, it is also limited with regard to credit\ncard fraud detection. Furthermore, simple RNN, LSTM,\nGRU, BiLSTM, and BiGRU have been explored for\ncredit card fraud detection. Simple RNN, although capable\nof capturing temporal dependencies, has struggled with\nlong-term dependencies, limiting its effectiveness in this",
  "credit card fraud detection. Simple RNN, although capable\nof capturing temporal dependencies, has struggled with\nlong-term dependencies, limiting its effectiveness in this\ndomain. Conversely, LSTM has demonstrated exceptional\nperformance due to its ability to retain information over\nlong sequences, making it well-suited for credit card fraud\ndetection [18]. GRU, a variant of LSTM, has also shown\npromising results, combining the ability to retain information\nwith a simplified architecture. BiLSTM and BiGRU, which\nincorporate bidirectional processing, have been found to\nfurther improve the accuracy of fraud detection models by\nconsidering both past and future contexts.\nSeveral key conclusions can be drawn from this research.\nFirstly, deep learning architectures play a crucial role in\nefficiently detecting credit card fraud. The performance of the\nmodels differs with changes in the distribution of the samples.\nFor example, models trained with balanced datasets achieve",
  "efficiently detecting credit card fraud. The performance of the\nmodels differs with changes in the distribution of the samples.\nFor example, models trained with balanced datasets achieve\nmore robust performance than those trained with imbalanced\ndata. Therefore, effective data resampling and engineering\nshould be considered before model training. Also, optimizing\ndeep learning models to consider the imbalanced nature of the\ndata is beneficial.\nSecondly, though several deep learning-based architectures\nhave been employed for detecting credit card fraud, the\nfollowing architectures have been widely utilized: CNN,\nLSTM, GRU, and other RNN variants. Even though they can\nbe computationally expensive compared to traditional ML\nalgorithms, they usually achieve higher performance. Thirdly,\ndifferent research works have used single DL classifiers,\n96906 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\nachieving excellent classification performance. However,\nsome researchers have explored hybrid deep learning models,\nwhich perform significantly better than single deep learning\nmodels. Also, the ensemble of deep learning models has led\nto superior performance compared to single deep learning\nmodels. However, it increases the computational complexity\nof the model.\nFurthermore, future research in credit card fraud detection\nusing deep learning can explore several promising avenues\nto enhance the robustness, accuracy, and applicability of\ndetection systems. One critical area is the development\nand implementation of hybrid and ensemble deep learning\narchitectures. Combining different models, such as LSTM\nwith CNN or GRU with Transformer models, can leverage\nthe strengths of each architecture to improve overall perfor-\nmance. These hybrid models can potentially provide more",
  "with CNN or GRU with Transformer models, can leverage\nthe strengths of each architecture to improve overall perfor-\nmance. These hybrid models can potentially provide more\naccurate detection by capturing both temporal dependencies\nand spatial features of transaction data. Additionally, ensem-\nble methods, which integrate multiple models’ predictions,\ncan enhance the system’s robustness by reducing the variance\nand bias associated with individual models.\nMoreover, while hybrid and ensemble models hold great\npromise, their practical deployment often faces challenges\nrelated to computational complexity and resource require-\nments. Future research can focus on optimizing these models\nto make them more efficient and scalable for real-world\napplications. Techniques such as model pruning, quantiza-\ntion, and the use of efficient neural network architectures\ncan significantly reduce computational overhead without\nsacrificing accuracy. Investigating the trade-offs between",
  "tion, and the use of efficient neural network architectures\ncan significantly reduce computational overhead without\nsacrificing accuracy. Investigating the trade-offs between\nmodel complexity and performance and developing adaptive\nmodels that can dynamically adjust their complexity based\non the available computational resources will be crucial for\ndeploying these advanced systems in operational environ-\nments.\nAnother important direction for future research is enhanc-\ning the interpretability and explainability of deep learning\nmodels in fraud detection. As these models become more\ncomplex, understanding their decision-making processes\nbecomes more challenging, yet it is essential for gaining trust\nfrom users and meeting regulatory requirements. Research\nshould focus on developing methods that can provide clear\nand actionable insights into how models make predictions.\nTechniques like attention mechanisms, SHapley Additive\nexPlanations, and layer-wise relevance propagation can",
  "and actionable insights into how models make predictions.\nTechniques like attention mechanisms, SHapley Additive\nexPlanations, and layer-wise relevance propagation can\nhelp explain the inner workings of deep learning models.\nAdditionally, integrating these interpretability methods with\nreal-time fraud detection systems will ensure that finan-\ncial institutions can respond quickly and transparently to\nfraudulent activities, thereby improving the overall security\nand trustworthiness of credit card transaction systems.\nX. CONCLUSION\nDeep learning methods have been widely applied in different\nfields due to their robustness and performance. Recently,\ndeep learning architectures have produced exceptional\nperformance in credit card fraud detection. This paper\npresents a comprehensive review of the current state of\ndeep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.\nThe study provides valuable insights for researchers and",
  "deep learning applications in credit card fraud detection,\nhighlighting the primary challenges and potential solutions.\nThe study provides valuable insights for researchers and\npractitioners and can guide the development of more robust\nand efficient fraud detection models, ultimately contributing\nto more secure financial transactions and reducing the\neconomic impact of fraud.\nREFERENCES\n[1] B. Lebichot, G. M. Paldino, W. Siblini, L. He-Guelton, F. Oblé, and\nG. Bontempi, ‘‘Incremental learning strategies for credit cards fraud\ndetection,’’Int. J. Data Sci. Anal. , vol. 12, no. 2, pp. 165–174, Aug. 2021.\n[2] X. Zhang, Y . Han, W. Xu, and Q. Wang, ‘‘HOBA: A novel feature\nengineering methodology for credit card fraud detection with a deep\nlearning architecture,’’ Inf. Sci., vol. 557, pp. 302–316, May 2021.\n[3] S. Bakhtiari, Z. Nasiri, and J. Vahidi, ‘‘Credit card fraud detection using\nensemble data mining methods,’’ Multimedia Tools Appl., vol. 82, no. 19,\npp. 29057–29075, Aug. 2023.",
  "[3] S. Bakhtiari, Z. Nasiri, and J. Vahidi, ‘‘Credit card fraud detection using\nensemble data mining methods,’’ Multimedia Tools Appl., vol. 82, no. 19,\npp. 29057–29075, Aug. 2023.\n[4] M.-H. Yang, J.-N. Luo, M. Vijayalakshmi, and S. M. Shalinie, ‘‘Contact-\nless credit cards payment fraud protection by ambient authentication,’’\nSensors, vol. 22, no. 5, p. 1989, Mar. 2022.\n[5] J. Wang, W. Liu, Y . Kou, D. Xiao, X. Wang, and X. Tang, ‘‘Approx-\nSMOTE federated learning credit card fraud detection system,’’ in Proc.\nIEEE 47th Annu. Comput., Softw., Appl. Conf. (COMPSAC) , Jun. 2023,\npp. 1370–1375.\n[6] A. A. El-Naby, E. E.-D. Hemdan, and A. El-Sayed, ‘‘An efficient\nfraud detection framework with credit card imbalanced data in financial\nservices,’’ Multimedia Tools Appl. , vol. 82, no. 3, pp. 4139–4160,\nJan. 2023.\n[7] F. K. Alarfaj, I. Malik, H. U. Khan, N. Almusallam, M. Ramzan,\nand M. Ahmed, ‘‘Credit card fraud detection using state-of-the-art",
  "Jan. 2023.\n[7] F. K. Alarfaj, I. Malik, H. U. Khan, N. Almusallam, M. Ramzan,\nand M. Ahmed, ‘‘Credit card fraud detection using state-of-the-art\nmachine learning and deep learning algorithms,’’ IEEE Access , vol. 10,\npp. 39700–39715, 2022.\n[8] M. A. Islam, M. A. Uddin, S. Aryal, and G. Stea, ‘‘An ensemble learning\napproach for anomaly detection in credit card data with imbalanced\nand overlapped classes,’’ J. Inf. Secur. Appl. , vol. 78, Nov. 2023,\nArt. no. 103618.\n[9] T. K. Dang, T. C. Tran, L. M. Tuan, and M. V . Tiep, ‘‘Machine learning\nbased on resampling approaches and deep reinforcement learning for\ncredit card fraud detection systems,’’ Appl. Sci., vol. 11, no. 21, p. 10004,\nOct. 2021.\n[10] N. S. Alfaiz and S. M. Fati, ‘‘Enhanced credit card fraud detection model\nusing machine learning,’’ Electronics, vol. 11, no. 4, p. 662, Feb. 2022.\n[11] I. D. Mienye and N. Jere, ‘‘A survey of decision trees: Concepts, algo-",
  "using machine learning,’’ Electronics, vol. 11, no. 4, p. 662, Feb. 2022.\n[11] I. D. Mienye and N. Jere, ‘‘A survey of decision trees: Concepts, algo-\nrithms, and applications,’’ IEEE Access, vol. 12, pp. 86716–86727, 2024.\n[12] S. Dong, Y . Xia, and T. Peng, ‘‘Network abnormal traffic detection model\nbased on semi-supervised deep reinforcement learning,’’ IEEE Trans.\nNetw. Service Manag. , vol. 18, no. 4, pp. 4197–4212, Dec. 2021.\n[13] E. A. L. M. Btoush, X. Zhou, R. Gururajan, K. C. Chan, R. Genrich, and\nP. Sankaran, ‘‘A systematic review of literature on credit card cyber fraud\ndetection using machine and deep learning,’’ PeerJ Comput. Sci. , vol. 9,\np. e1278, Apr. 2023.\n[14] R. Bin Sulaiman, V . Schetinin, and P. Sant, ‘‘Review of machine learning\napproach on credit card fraud detection,’’ Hum.-Centric Intell. Syst. ,\nvol. 2, no. 1, pp. 55–68, 2022.\n[15] E. N. Osegi and E. F. Jumbo, ‘‘Comparative analysis of credit card fraud",
  "approach on credit card fraud detection,’’ Hum.-Centric Intell. Syst. ,\nvol. 2, no. 1, pp. 55–68, 2022.\n[15] E. N. Osegi and E. F. Jumbo, ‘‘Comparative analysis of credit card fraud\ndetection in simulated annealing trained artificial neural network and\nhierarchical temporal memory,’’ Mach. Learn. Appl. , vol. 6, Dec. 2021,\nArt. no. 100080.\n[16] P. Wang, E. Fan, and P. Wang, ‘‘Comparative analysis of image\nclassification algorithms based on traditional machine learning and deep\nlearning,’’Pattern Recognit. Lett. , vol. 141, pp. 61–67, Jan. 2021.\n[17] I. D. Mienye and Y . Sun, ‘‘A deep learning ensemble with data\nresampling for credit card fraud detection,’’ IEEE Access , vol. 11,\npp. 30628–30638, 2023.\n[18] E. Esenogho, I. D. Mienye, T. G. Swart, K. Aruleba, and G. Obaido,\n‘‘A neural network ensemble with feature engineering for improved credit\ncard fraud detection,’’ IEEE Access, vol. 10, pp. 16400–16407, 2022.\nVOLUME 12, 2024 96907",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n[19] M. L. Gambo, A. Zainal, and M. N. Kassim, ‘‘A convolutional neural\nnetwork model for credit card fraud detection,’’ in Proc. Int. Conf. Data\nSci. Appl. (ICoDSA) , Jul. 2022, pp. 198–202.\n[20] T. Berhane, T. Melese, A. Walelign, and A. Mohammed, ‘‘A hybrid\nconvolutional neural network and support vector machine-based credit\ncard fraud detection model,’’ Math. Problems Eng. , vol. 2023, pp. 1–10,\nJun. 2023.\n[21] D. Sehrawat and Y . Singh, ‘‘Auto-encoder and LSTM-based credit\ncard fraud detection,’’ Social Netw. Comput. Sci. , vol. 4, no. 5, p. 557,\nJul. 2023.\n[22] J. Raval, P. Bhattacharya, N. K. Jadav, S. Tanwar, G. Sharma,\nP. N. Bokoro, M. Elmorsy, A. Tolba, and M. S. Raboaca, ‘‘RaKShA: A\ntrusted explainable LSTM model to classify fraud patterns on credit card\ntransactions,’’Mathematics, vol. 11, no. 8, p. 1901, Apr. 2023.",
  "trusted explainable LSTM model to classify fraud patterns on credit card\ntransactions,’’Mathematics, vol. 11, no. 8, p. 1901, Apr. 2023.\n[23] I. Benchaji, S. Douzi, B. El Ouahidi, and J. Jaafari, ‘‘Enhanced credit card\nfraud detection based on attention mechanism and LSTM deep model,’’\nJ. Big Data , vol. 8, no. 1, pp. 1–21, Dec. 2021.\n[24] S. Gold, ‘‘The evolution of payment card fraud,’’ Comput. Fraud Secur.,\nvol. 2014, no. 3, pp. 12–17, Mar. 2014.\n[25] K. L. Ambashtha and P. Kumar, ‘‘Online fraud,’’ in Financial Crimes:\nA Guide to Financial Exploitation in a Digital Age . Berlin, Germany:\nSpringer, 2023, pp. 97–108.\n[26] K. Guers, M. M. Chowdhury, and N. Rifat, ‘‘Card skimming: A\ncybercrime by hackers,’’ in Proc. IEEE Int. Conf. Electro Inf. Technol.\n(eIT), May 2022, pp. 575–579.\n[27] R. Van Belle, B. Baesens, and J. De Weerdt, ‘‘CATCHM: A novel\nnetwork-based credit card fraud detection method using node representa-",
  "(eIT), May 2022, pp. 575–579.\n[27] R. Van Belle, B. Baesens, and J. De Weerdt, ‘‘CATCHM: A novel\nnetwork-based credit card fraud detection method using node representa-\ntion learning,’’ Decis. Support Syst. , vol. 164, Jan. 2023, Art. no. 113866.\n[28] I. D. Mienye and Y . Sun, ‘‘A machine learning method with hybrid feature\nselection for improved credit card fraud detection,’’ Appl. Sci. , vol. 13,\nno. 12, p. 7254, Jun. 2023.\n[29] V . S. S. Karthik, A. Mishra, and U. S. Reddy, ‘‘Credit card fraud detection\nby modelling behaviour pattern using hybrid ensemble model,’’ Arabian\nJ. Sci. Eng. , vol. 47, no. 2, pp. 1987–1997, Feb. 2022.\n[30] K. Randhawa, C. K. Loo, M. Seera, C. P. Lim, and A. K. Nandi, ‘‘Credit\ncard fraud detection using AdaBoost and majority voting,’’ IEEE Access,\nvol. 6, pp. 14277–14284, 2018.\n[31] A. M. Aburbeian and H. I. Ashqar, ‘‘Credit card fraud detection\nusing enhanced random forest classifier for imbalanced data,’’ in Proc.",
  "vol. 6, pp. 14277–14284, 2018.\n[31] A. M. Aburbeian and H. I. Ashqar, ‘‘Credit card fraud detection\nusing enhanced random forest classifier for imbalanced data,’’ in Proc.\nInt. Conf. Adv. Comput. Res. Cham, Switzerland: Springer, 2023,\npp. 605–616.\n[32] S. E. Kafhali and M. Tayebi, ‘‘XGBoost based solutions for detecting\nfraudulent credit card transactions,’’ in Proc. Int. Conf. Adv. Creative\nNetw. Intell. Syst. (ICACNIS) , Nov. 2022, pp. 1–6.\n[33] K. Illanko, R. Soleymanzadeh, and X. Fernando, ‘‘A big data deep\nlearning approach for credit card fraud detection,’’ in Computer Networks,\nBig Data and IoT . Cham, Switzerland: Springer, 2022, pp. 633–641.\n[34] J. Karthika and A. Senthilselvi, ‘‘Smart credit card fraud detection system\nbased on dilated convolutional neural network with sampling technique,’’\nMultimedia Tools Appl. , vol. 82, no. 20, pp. 31691–31708, Aug. 2023.\n[35] H. Fanai and H. Abbasimehr, ‘‘A novel combined approach based on deep",
  "Multimedia Tools Appl. , vol. 82, no. 20, pp. 31691–31708, Aug. 2023.\n[35] H. Fanai and H. Abbasimehr, ‘‘A novel combined approach based on deep\nautoencoder and deep classifiers for credit card fraud detection,’’ Exp.\nSyst. Appl., vol. 217, May 2023, Art. no. 119562.\n[36] Y . Xie, G. Liu, C. Yan, C. Jiang, M. Zhou, and M. Li, ‘‘Learning\ntransactional behavioral representations for credit card fraud detection,’’\nIEEE Trans. Neural Netw. Learn. Syst. , vol. 35, no. 4, pp. 5735–5748,\nApr. 2024.\n[37] Z. Wang, S. Kim, and I. Joe, ‘‘An improved LSTM-based failure\nclassification model for financial companies using natural language\nprocessing,’’Appl. Sci., vol. 13, no. 13, p. 7884, Jul. 2023.\n[38] J. Karthika and A. Senthilselvi, ‘‘An integration of deep learning model\nwith navo minority over-sampling technique to detect the frauds in\ncredit cards,’’ Multimedia Tools Appl. , vol. 82, no. 14, pp. 21757–21774,\nJun. 2023.\n[39] N. Prabhakaran and R. Nedunchelian, ‘‘Oppositional cat swarm",
  "credit cards,’’ Multimedia Tools Appl. , vol. 82, no. 14, pp. 21757–21774,\nJun. 2023.\n[39] N. Prabhakaran and R. Nedunchelian, ‘‘Oppositional cat swarm\noptimization-based feature selection approach for credit card fraud\ndetection,’’Comput. Intell. Neurosci. , vol. 2023, pp. 1–13, Jan. 2023.\n[40] V . Bach Nguyen, K. G. Dastidar, M. Granitzer, and W. Siblini,\n‘‘The importance of future information in credit card fraud detection,’’\nin Proc. 25th Int. Conf. Artif. Intell. Statist. , vol. 151, G. Camps-Valls,\nF. J. R. Ruiz, and I. Valera, Eds., Mar. 2022, pp. 10067–10077.\n[41] K. Modi and R. Dayma, ‘‘Review on fraud detection methods in credit\ncard transactions,’’ in Proc. Int. Conf. Intell. Comput. Control (IC) ,\nJun. 2017, pp. 1–5.\n[42] Y . Lucas and J. Jurgovsky, ‘‘Credit card fraud detection using machine\nlearning: A survey,’’ 2020, arXiv:2010.06479.\n[43] K. G. Al-Hashedi and P. Magalingam, ‘‘Financial fraud detection\napplying data mining techniques: A comprehensive review from 2009 to",
  "learning: A survey,’’ 2020, arXiv:2010.06479.\n[43] K. G. Al-Hashedi and P. Magalingam, ‘‘Financial fraud detection\napplying data mining techniques: A comprehensive review from 2009 to\n2019,’’Comput. Sci. Rev. , vol. 40, May 2021, Art. no. 100402.\n[44] R. R. Popat and J. Chaudhary, ‘‘A survey on credit card fraud detection\nusing machine learning,’’ in Proc. 2nd Int. Conf. Trends Electron.\nInformat. (ICOEI), May 2018, pp. 1120–1125.\n[45] N. F. Ryman-Tubb, P. Krause, and W. Garn, ‘‘How artificial intelligence\nand machine learning research impacts payment card fraud detection:\nA survey and industry benchmark,’’ Eng. Appl. Artif. Intell. , vol. 76,\npp. 130–157, Nov. 2018.\n[46] K. Pandey, P. Sachan, and N. G. Ganpatrao, ‘‘A review of credit card fraud\ndetection techniques,’’ in Proc. 5th Int. Conf. Comput. Methodologies\nCommun. (ICCMC), Apr. 2021, pp. 1645–1653.\n[47] M. Alamri and M. Ykhlef, ‘‘Survey of credit card anomaly and fraud",
  "detection techniques,’’ in Proc. 5th Int. Conf. Comput. Methodologies\nCommun. (ICCMC), Apr. 2021, pp. 1645–1653.\n[47] M. Alamri and M. Ykhlef, ‘‘Survey of credit card anomaly and fraud\ndetection using sampling techniques,’’ Electronics, vol. 11, no. 23,\np. 4003, Dec. 2022.\n[48] (2018). Credit Card Fraud Detection . Accessed: Oct. 17, 2023. [Online].\nAvailable: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n[49] (2019). IEEE-CIS Fraud Detection . Accessed: Oct. 17, 2023. [Online].\nAvailable: https://www.kaggle.com/c/ieee-fraud-detection\n[50] E. A. Lopez-Rojas, A. Elmir, and S. Axelsson, ‘‘PaySim: A financial\nmobile money simulator for fraud detection,’’ in Proc. 28th Eur. Modeling\nSimulation Symp. (EMSS) , Sep. 2016, pp. 249–255.\n[51] G. F. Montufar, R. Pascanu, K. Cho, and Y . Bengio, ‘‘On the number of\nlinear regions of deep neural networks,’’ in Proc. Adv. Neural Inf. Process.\nSyst., vol. 27, 2014, pp. 1–9.",
  "[51] G. F. Montufar, R. Pascanu, K. Cho, and Y . Bengio, ‘‘On the number of\nlinear regions of deep neural networks,’’ in Proc. Adv. Neural Inf. Process.\nSyst., vol. 27, 2014, pp. 1–9.\n[52] B. Yuen, M. T. Hoang, X. Dong, and T. Lu, ‘‘Universal activation function\nfor machine learning,’’ Sci. Rep., vol. 11, no. 1, p. 18757, Sep. 2021.\n[53] I. H. Sarker, ‘‘AI-based modeling: Techniques, applications and research\nissues towards automation, intelligent and smart systems,’’ Social Netw.\nComput. Sci., vol. 3, no. 2, p. 158, Mar. 2022.\n[54] Y . LeCun, Y . Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,\nno. 7553, pp. 436–444, 7553.\n[55] I. D. Mienye and Y . Sun, ‘‘Effective feature selection for improved\nprediction of heart disease,’’ in Proc. Pan-African Artif. Intell. Smart\nSystems Conf. Cham, Switzerland: Springer, 2021, pp. 94–107.\n[56] C. Guo, B. Zhao, and Y . Bai, ‘‘Deepcore: A comprehensive library for\ncoreset selection in deep learning,’’ in Proc. Int. Conf. Database Expert",
  "[56] C. Guo, B. Zhao, and Y . Bai, ‘‘Deepcore: A comprehensive library for\ncoreset selection in deep learning,’’ in Proc. Int. Conf. Database Expert\nSyst. Appl. Cham, Switzerland: Springer, 2022, pp. 181–195.\n[57] W.-F. Zeng, X.-X. Zhou, S. Willems, C. Ammar, M. Wahle, I. Bludau,\nE. V oytik, M. T. Strauss, and M. Mann, ‘‘AlphaPeptDeep: A modular deep\nlearning framework to predict peptide properties for proteomics,’’ Nature\nCommun., vol. 13, no. 1, p. 7238, Nov. 2022.\n[58] A. Mehrish, N. Majumder, R. Bharadwaj, R. Mihalcea, and S. Poria,\n‘‘A review of deep learning techniques for speech processing,’’ Inf.\nFusion, vol. 99, Nov. 2023, Art. no. 101869.\n[59] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n[60] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN encoder–decoder for statistical machine translation,’’ 2014,",
  "H. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN encoder–decoder for statistical machine translation,’’ 2014,\narXiv:1406.1078.\n[61] Y . Moodi, M. Ghasemi, and S. R. Mousavi, ‘‘Estimating the compressive\nstrength of rectangular fiber reinforced polymer–confined columns\nusing multilayer perceptron, radial basis function, and support vector\nregression methods,’’ J. Reinforced Plastics Compos. , vol. 41, nos. 3–4,\npp. 130–146, Feb. 2022.\n[62] S. R. Dubey, S. K. Singh, and B. B. Chaudhuri, ‘‘Activation functions in\ndeep learning: A comprehensive survey and benchmark,’’ Neurocomput-\ning, vol. 503, pp. 92–108, Sep. 2022.\n[63] S. S. Yadav and S. M. Jadhav, ‘‘Deep convolutional neural network based\nmedical image classification for disease diagnosis,’’ J. Big Data , vol. 6,\nno. 1, pp. 1–18, Dec. 2019.\n[64] J. Naranjo-Torres, M. Mora, R. Hernández-García, R. J. Barrientos,\nC. Fredes, and A. Valenzuela, ‘‘A review of convolutional neural network",
  "no. 1, pp. 1–18, Dec. 2019.\n[64] J. Naranjo-Torres, M. Mora, R. Hernández-García, R. J. Barrientos,\nC. Fredes, and A. Valenzuela, ‘‘A review of convolutional neural network\napplied to fruit image processing,’’ Appl. Sci. , vol. 10, no. 10, p. 3443,\nMay 2020.\n96908 VOLUME 12, 2024",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n[65] I. D. Mienye, P. Kenneth Ainah, I. D. Emmanuel, and E. Esenogho,\n‘‘Sparse noise minimization in image classification using genetic\nalgorithm and DenseNet,’’ in Proc. Conf. Inf. Commun. Technol. Soc.\n(ICTAS), Mar. 2021, pp. 103–108.\n[66] R. San Miguel Carrasco and M.-Á. Sicilia-Urbán, ‘‘Evaluation of deep\nneural networks for reduction of credit card fraud alerts,’’ IEEE Access ,\nvol. 8, pp. 186421–186432, 2020.\n[67] K. Fu, D. Cheng, Y . Tu, and L. Zhang, ‘‘Credit card fraud detection\nusing convolutional neural networks,’’ in Neural Information Processing,\nKyoto, Japan. Cham, Switzerland: Springer, 2016, pp. 483–490.\n[68] A. Dhillon and G. K. Verma, ‘‘Convolutional neural network: A review of\nmodels, methodologies and applications to object detection,’’ Prog. Artif.\nIntell., vol. 9, no. 2, pp. 85–112, Jun. 2020.\n[69] M. Krichen, ‘‘Convolutional neural networks: A survey,’’ Computers,",
  "models, methodologies and applications to object detection,’’ Prog. Artif.\nIntell., vol. 9, no. 2, pp. 85–112, Jun. 2020.\n[69] M. Krichen, ‘‘Convolutional neural networks: A survey,’’ Computers,\nvol. 12, no. 8, p. 151, Jul. 2023.\n[70] R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi,\n‘‘Convolutional neural networks: An overview and application\nin radiology,’’ Insights into Imag. , vol. 9, no. 4, pp. 611–629,\nAug. 2018.\n[71] L. González-Rodríguez and A. Plasencia-Salgueiro, ‘‘Uncertainty-aware\nautonomous mobile robot navigation with deep reinforcement learning,’’\nin Deep Learning for Unmanned Systems , Cham, Switzerland, 2021,\npp. 225–257.\n[72] A. Tsantekidis, N. Passalis, and A. Tefas, ‘‘Recurrent neural networks,’’\nin Deep Learning for Robot Perception and Cognition . Amsterdam,\nThe Netherlands: Elsevier, 2022, pp. 101–115.\n[73] P. Oliveira, B. Fernandes, C. Analide, and P. Novais, ‘‘Forecasting energy\nconsumption of wastewater treatment plants with a transfer learning",
  "The Netherlands: Elsevier, 2022, pp. 101–115.\n[73] P. Oliveira, B. Fernandes, C. Analide, and P. Novais, ‘‘Forecasting energy\nconsumption of wastewater treatment plants with a transfer learning\napproach for sustainable cities,’’ Electronics, vol. 10, no. 10, p. 1149,\nMay 2021.\n[74] J. Yang, J. Qu, Q. Mi, and Q. Li, ‘‘A CNN-LSTM model for tailings dam\nrisk prediction,’’ IEEE Access, vol. 8, pp. 206491–206502, 2020.\n[75] M. Ma, C. Liu, R. Wei, B. Liang, and J. Dai, ‘‘Predicting machine’s\nperformance record using the stacked long short-term memory (LSTM)\nneural networks,’’ J. Appl. Clin. Med. Phys. , vol. 23, no. 3, 2022,\nArt. no. e13558.\n[76] H. Xie, M. Randall, and K.-W. Chau, ‘‘Green roof hydrological modelling\nwith GRU and LSTM networks,’’ Water Resour. Manag., vol. 36, no. 3,\npp. 1107–1122, Feb. 2022.\n[77] S. Gao, Y . Huang, S. Zhang, J. Han, G. Wang, M. Zhang, and Q. Lin,\n‘‘Short-term runoff prediction with GRU and LSTM networks without",
  "pp. 1107–1122, Feb. 2022.\n[77] S. Gao, Y . Huang, S. Zhang, J. Han, G. Wang, M. Zhang, and Q. Lin,\n‘‘Short-term runoff prediction with GRU and LSTM networks without\nrequiring time step optimization during sample generation,’’ J. Hydrol.,\nvol. 589, Oct. 2020, Art. no. 125188.\n[78] C. Cui, P. Wang, Y . Li, and Y . Zhang, ‘‘McVCsB: A new hybrid deep\nlearning network for stock index prediction,’’ Exp. Syst. Appl. , vol. 232,\nDec. 2023, Art. no. 120902.\n[79] Y .-H. Li, L. N. Harfiya, K. Purwandari, and Y .-D. Lin, ‘‘Real-time cuffless\ncontinuous blood pressure estimation using deep learning model,’’\nSensors, vol. 20, no. 19, p. 5606, Sep. 2020.\n[80] Y . Hao, L. Dong, F. Wei, and K. Xu, ‘‘Self-attention attribution:\nInterpreting information interactions inside transformer,’’ in Proc. AAAI\nConf. Artif. Intell. , 2021, vol. 35, no. 14, pp. 12963–12971.\n[81] D. A. Tarzanagh, Y . Li, X. Zhang, and S. Oymak, ‘‘Max-margin token\nselection in attention mechanism,’’ in Proc. 37th Conf. Neural Inf.",
  "[81] D. A. Tarzanagh, Y . Li, X. Zhang, and S. Oymak, ‘‘Max-margin token\nselection in attention mechanism,’’ in Proc. 37th Conf. Neural Inf.\nProcess. Syst., 2023, pp. 48314–48362.\n[82] G. Obaido, B. Ogbuokiri, C. W. Chukwu, F. J. Osaye, O. F. Egbelowo,\nM. I. Uzochukwu, I. D. Mienye, K. Aruleba, M. Primus, and O. Achilonu,\n‘‘An improved ensemble method for predicting hyperchloremia in adults\nwith diabetic ketoacidosis,’’ IEEE Access , vol. 12, pp. 9536–9549,\n2024.\n[83] T. O’Halloran, G. Obaido, B. Otegbade, and I. D. Mienye, ‘‘A deep\nlearning approach for maize lethal necrosis and maize streak virus disease\ndetection,’’Mach. Learn. Appl. , vol. 16, Jun. 2024, Art. no. 100556.\n[84] R. Trevethan, ‘‘Sensitivity, specificity, and predictive values: Founda-\ntions, pliabilities, and pitfalls in research and practice,’’ Frontiers Public\nHealth, vol. 5, Nov. 2017, Art. no. 308890.\n[85] I. D. Mienye, G. Obaido, K. Aruleba, and O. A. Dada, ‘‘Enhanced",
  "tions, pliabilities, and pitfalls in research and practice,’’ Frontiers Public\nHealth, vol. 5, Nov. 2017, Art. no. 308890.\n[85] I. D. Mienye, G. Obaido, K. Aruleba, and O. A. Dada, ‘‘Enhanced\nprediction of chronic kidney disease using feature selection and boosted\nclassifiers,’’ in Proc. Int. Conf. Intell. Syst. Design Appl. Cham,\nSwitzerland: Springer, 2021, pp. 527–537.\n[86] M. Rizwan, A. Nadeem, and M. A. Sindhu, ‘‘Analyses of classifier’s\nperformance measures used in software fault prediction studies,’’ IEEE\nAccess, vol. 7, pp. 82764–82775, 2019.\n[87] G. Obaido, O. Achilonu, B. Ogbuokiri, C. S. Amadi, L. Habeebullahi,\nT. Ohalloran, C. W. Chukwu, E. D. Mienye, M. Aliyu, O. Fasawe,\nI. A. Modupe, E. J. Omietimi, and K. Aruleba, ‘‘An improved framework\nfor detecting thyroid disease using filter-based feature selection and\nstacking ensemble,’’ IEEE Access, vol. 12, pp. 89098–89112, 2024.\n[88] B. Ozenne, F. Subtil, and D. Maucort-Boulch, ‘‘The precision–recall",
  "stacking ensemble,’’ IEEE Access, vol. 12, pp. 89098–89112, 2024.\n[88] B. Ozenne, F. Subtil, and D. Maucort-Boulch, ‘‘The precision–recall\ncurve overcame the optimism of the receiver operating characteristic\ncurve in rare diseases,’’ J. Clin. Epidemiol. , vol. 68, no. 8, pp. 855–859,\nAug. 2015.\n[89] J. F. Roseline, G. Naidu, V . S. Pandi, S. A. A. Rajasree, and N. Mageswari,\n‘‘Autonomous credit card fraud detection using machine learning\napproach?’’ Comput. Electr. Eng., vol. 102, Sep. 2022, Art. no. 108132.\n[90] H. Najadat, O. Altiti, A. A. Aqouleh, and M. Younes, ‘‘Credit card fraud\ndetection based on machine and deep learning,’’ in Proc. 11th Int. Conf.\nInf. Commun. Syst. (ICICS) , Apr. 2020, pp. 204–208.\n[91] J. Forough and S. Momtazi, ‘‘Ensemble of deep sequential models for\ncredit card fraud detection,’’ Appl. Soft Comput. , vol. 99, Feb. 2021,\nArt. no. 106883.\n[92] N. F. Aurna, M. D. Hossain, Y . Taenaka, and Y . Kadobayashi, ‘‘Federated",
  "credit card fraud detection,’’ Appl. Soft Comput. , vol. 99, Feb. 2021,\nArt. no. 106883.\n[92] N. F. Aurna, M. D. Hossain, Y . Taenaka, and Y . Kadobayashi, ‘‘Federated\nlearning-based credit card fraud detection: Performance analysis with\nsampling methods and deep learning algorithms,’’ in Proc. IEEE Int.\nConf. Cyber Secur. Resilience (CSR) , Jul. 2023, pp. 180–186.\n[93] Y . Xie, G. Liu, C. Yan, C. Jiang, and M. Zhou, ‘‘Time-aware attention-\nbased gated network for credit card fraud detection by extracting\ntransactional behaviors,’’ IEEE Trans. Computat. Social Syst. , vol. 10,\nno. 3, pp. 1004–1016, Jun. 2023.\n[94] E. Ajitha, S. Sneha, S. Makesh, and K. Jaspin, ‘‘A comparative\nanalysis of credit card fraud detection with machine learning algorithms\nand convolutional neural network,’’ in Proc. Int. Conf. Adv. Comput.,\nCommun. Appl. Informat. (ACCAI) , May 2023, pp. 1–8.\n[95] M. N. Y . Ali, T. Kabir, N. L. Raka, S. S. Toma, M. L. Rahman,",
  "and convolutional neural network,’’ in Proc. Int. Conf. Adv. Comput.,\nCommun. Appl. Informat. (ACCAI) , May 2023, pp. 1–8.\n[95] M. N. Y . Ali, T. Kabir, N. L. Raka, S. S. Toma, M. L. Rahman,\nand J. Ferdaus, ‘‘SMOTE based credit card fraud detection using\nconvolutional neural network,’’ in Proc. 25th Int. Conf. Comput. Inf.\nTechnol. (ICCIT), Dec. 2022, pp. 55–60.\n[96] M. Z. Mizher and A. B. Nassif, ‘‘Deep CNN approach for unbalanced\ncredit card fraud detection data,’’ in Proc. Adv. Sci. Eng. Technol. Int.\nConf. (ASET), Feb. 2023, pp. 1–7.\n[97] A. Iqbal and R. Amin, ‘‘Time series forecasting and anomaly detection\nusing deep learning,’’ Comput. Chem. Eng. , vol. 182, Mar. 2024,\nArt. no. 108560.\n[98] Y . Tang and Z. Liu, ‘‘A distributed knowledge distillation framework for\nfinancial fraud detection based on transformer,’’ IEEE Access , vol. 12,\npp. 62899–62911, 2024.\n[99] I. D. Mienye and Y . Sun, ‘‘Performance analysis of cost-sensitive learning",
  "financial fraud detection based on transformer,’’ IEEE Access , vol. 12,\npp. 62899–62911, 2024.\n[99] I. D. Mienye and Y . Sun, ‘‘Performance analysis of cost-sensitive learning\nmethods with application to imbalanced medical data,’’ Informat. Med.\nUnlocked, vol. 25, Jan. 2021, Art. no. 100690.\n[100] D. Dablain, B. Krawczyk, and N. V . Chawla, ‘‘DeepSMOTE: Fusing deep\nlearning and SMOTE for imbalanced data,’’ IEEE Trans. Neural Netw.\nLearn. Syst., vol. 34, no. 9, pp. 6390–6404, Sep. 2023.\n[101] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, ‘‘Learning\nimbalanced datasets with label-distribution-aware margin loss,’’ in Proc.\nAdv. Neural Inf. Process. Syst. , vol. 32, 2019, pp. 1–12.\n[102] Q. Dong, S. Gong, and X. Zhu, ‘‘Imbalanced deep learning by minority\nclass incremental rectification,’’ IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 41, no. 6, pp. 1367–1381, Jun. 2019.\n[103] C. Zhang, K. C. Tan, H. Li, and G. S. Hong, ‘‘A cost-sensitive deep belief",
  "class incremental rectification,’’ IEEE Trans. Pattern Anal. Mach. Intell. ,\nvol. 41, no. 6, pp. 1367–1381, Jun. 2019.\n[103] C. Zhang, K. C. Tan, H. Li, and G. S. Hong, ‘‘A cost-sensitive deep belief\nnetwork for imbalanced classification,’’ IEEE Trans. Neural Netw. Learn.\nSyst., vol. 30, no. 1, pp. 109–122, Jan. 2019.\n[104] S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy, ‘‘Training\ndeep neural networks on imbalanced data sets,’’ in Proc. Int. Joint Conf.\nNeural Netw. (IJCNN) , Jul. 2016, pp. 4368–4374.\n[105] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, ‘‘Focal loss for\ndense object detection,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) ,\nOct. 2017, pp. 2999–3007.\n[106] Z. Zhang and M. Sabuncu, ‘‘Generalized cross entropy loss for training\ndeep neural networks with noisy labels,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 31, 2018, pp. 1–11.\n[107] Y . Cui, M. Jia, T.-Y . Lin, Y . Song, and S. Belongie, ‘‘Class-balanced",
  "deep neural networks with noisy labels,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 31, 2018, pp. 1–11.\n[107] Y . Cui, M. Jia, T.-Y . Lin, Y . Song, and S. Belongie, ‘‘Class-balanced\nloss based on effective number of samples,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 9260–9269.\n[108] J. A. Dar, K. K. Srivastava, and S. Ahmed Lone, ‘‘Design and\ndevelopment of hybrid optimization enabled deep learning model for\nCOVID-19 detection with comparative analysis with DCNN, BIAT-GRU,\nXGBoost,’’Comput. Biol. Med. , vol. 150, Nov. 2022, Art. no. 106123.\nVOLUME 12, 2024 96909",
  "I. D. Mienye, N. Jere: DL for CCFD: A Review of Algorithms, Challenges, and Solutions\n[109] V . B. Semwal, A. Gupta, and P. Lalwani, ‘‘An optimized hybrid\ndeep learning model using ensemble learning approach for human\nwalking activities recognition,’’ J. Supercomput. , vol. 77, no. 11,\npp. 12256–12279, Nov. 2021.\n[110] M. U. Salur and I. Aydin, ‘‘A novel hybrid deep learning model for\nsentiment classification,’’ IEEE Access, vol. 8, pp. 58080–58093, 2020.\n[111] T.-Y . Liu, ‘‘EasyEnsemble and feature selection for imbalance data\nsets,’’ in Proc. Int. Joint Conf. Bioinf., Syst. Biol. Intell. Comput. , 2009,\npp. 517–520.\n[112] J. Blaszczyński and J. Stefanowski, ‘‘Actively balanced bagging for\nimbalanced data,’’ in Foundations of Intelligent Systems, Warsaw, Poland.\nCham, Switzerland: Springer, 2017, pp. 271–281.\n[113] E. Altman, ‘‘Synthesizing credit card transactions,’’ in Proc. 2nd ACM\nInt. Conf. AI Finance , Nov. 2021, pp. 1–9.",
  "Cham, Switzerland: Springer, 2017, pp. 271–281.\n[113] E. Altman, ‘‘Synthesizing credit card transactions,’’ in Proc. 2nd ACM\nInt. Conf. AI Finance , Nov. 2021, pp. 1–9.\n[114] A. Chakrabarty and S. Das, ‘‘Statistical regeneration guarantees of the\nWasserstein autoencoder with latent space consistency,’’ in Proc. Adv.\nNeural Inf. Process. Syst. , vol. 34, 2021, pp. 17098–17110.\n[115] D. Urda, J. Montes-Torres, F. Moreno, L. Franco, and J. M. Jerez, ‘‘Deep\nlearning to analyze RNA-seq gene expression data,’’ in Advances in\nComputational Intelligence, Cadiz, Spain. Cham, Switzerland: Springer,\n2017, pp. 50–59.\n[116] L. Antwarg, R. M. Miller, B. Shapira, and L. Rokach, ‘‘Explaining\nanomalies detected by autoencoders using Shapley additive explana-\ntions,’’Exp. Syst. Appl. , vol. 186, Dec. 2021, Art. no. 115736.\n[117] Y . Yang, J. Qiu, M. Song, D. Tao, and X. Wang, ‘‘Distilling knowledge\nfrom graph convolutional networks,’’ in Proc. IEEE/CVF Conf. Comput.",
  "[117] Y . Yang, J. Qiu, M. Song, D. Tao, and X. Wang, ‘‘Distilling knowledge\nfrom graph convolutional networks,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 7072–7081.\n[118] H. Raza, G. Prasad, and Y . Li, ‘‘EWMA model based shift-detection\nmethods for detecting covariate shifts in non-stationary environments,’’\nPattern Recognit., vol. 48, no. 3, pp. 659–669, Mar. 2015.\n[119] S.-S. Zhang, J.-W. Liu, and X. Zuo, ‘‘Adaptive online incremental\nlearning for evolving data streams,’’ Appl. Soft Comput. , vol. 105,\nJul. 2021, Art. no. 107255.\n[120] K. Rahmani, R. Thapa, P. Tsou, S. Casie Chetty, G. Barnes, C. Lam, and\nC. Foon Tso, ‘‘Assessing the effects of data drift on the performance of\nmachine learning models used in clinical sepsis prediction,’’ Int. J. Med.\nInformat., vol. 173, May 2023, Art. no. 104930.\n[121] S. Savvides, D. Khandelwal, and P. Eugster, ‘‘Efficient confidentiality-",
  "Informat., vol. 173, May 2023, Art. no. 104930.\n[121] S. Savvides, D. Khandelwal, and P. Eugster, ‘‘Efficient confidentiality-\npreserving data analytics over symmetrically encrypted datasets,’’ Proc.\nVLDB Endowment, vol. 13, no. 8, pp. 1290–1303, 2020.\n[122] S. Murthy, A. Abu Bakar, F. Abdul Rahim, and R. Ramli, ‘‘A comparative\nstudy of data anonymization techniques,’’ in Proc. IEEE 5th Int. Conf.\nBig Data Secur. Cloud (BigDataSecurity), Int. Conf. High Perform. Smart\nComput., (HPSC), IEEE Int. Conf. Intell. Data Secur. (IDS) , May 2019,\npp. 306–309.\n[123] H. Lee and Y . D. Chung, ‘‘Differentially private release of medical\nmicrodata: An efficient and practical approach for preserving informative\nattribute values,’’ BMC Med. Informat. Decis. Making , vol. 20, no. 1,\npp. 1–15, Dec. 2020.\n[124] A. Ali, M. F. Pasha, J. Ali, O. H. Fang, M. Masud, A. D. Jurcut, and\nM. A. Alzain, ‘‘Deep learning based homomorphic secure search-able",
  "pp. 1–15, Dec. 2020.\n[124] A. Ali, M. F. Pasha, J. Ali, O. H. Fang, M. Masud, A. D. Jurcut, and\nM. A. Alzain, ‘‘Deep learning based homomorphic secure search-able\nencryption for keyword search in blockchain healthcare system: A novel\napproach to cryptography,’’ Sensors, vol. 22, no. 2, p. 528, Jan. 2022.\n[125] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\n‘‘A survey on bias and fairness in machine learning,’’ ACM Comput. Surv.,\nvol. 54, no. 6, pp. 1–35, Jul. 2022.\n[126] J. Mary, C. Calauzenes, and N. El Karoui, ‘‘Fairness-aware learning for\ncontinuous attributes and treatments,’’ in Proc. Int. Conf. Mach. Learn. ,\n2019, pp. 4382–4391.\n[127] A. Alharbi, M. Alshammari, O. D. Okon, A. Alabrah, H. T. Rauf,\nH. Alyami, and T. Meraj, ‘‘A novel text2IMG mechanism of credit card\nfraud detection: A deep learning approach,’’ Electronics, vol. 11, no. 5,\np. 756, Mar. 2022.\n[128] I. D. Mienye and N. Jere, ‘‘Optimized ensemble learning approach",
  "fraud detection: A deep learning approach,’’ Electronics, vol. 11, no. 5,\np. 756, Mar. 2022.\n[128] I. D. Mienye and N. Jere, ‘‘Optimized ensemble learning approach\nwith explainable AI for improved heart disease prediction,’’ Information,\nvol. 15, no. 7, p. 394, Jul. 2024.\n[129] M. N. Fekri, H. Patel, K. Grolinger, and V . Sharma, ‘‘Deep learning for\nload forecasting with smart meter data: Online adaptive recurrent neural\nnetwork,’’Appl. Energy, vol. 282, Jan. 2021, Art. no. 116177.\n[130] A. Arunarani, D. Manjula, and V . Sugumaran, ‘‘Task scheduling\ntechniques in cloud computing: A literature survey,’’ Future Gener.\nComput. Syst., vol. 91, pp. 407–415, Feb. 2019.\nIBOMOIYE DOMOR MIENYE(Member, IEEE)\nreceived the B.Eng. degree in electrical and\nelectronic engineering and the M.Sc. degree (cum\nlaude) in computer systems engineering from the\nUniversity of East London, in 2012 and 2014,\nrespectively, and the Ph.D. degree in electrical\nand electronic engineering from the University of",
  "laude) in computer systems engineering from the\nUniversity of East London, in 2012 and 2014,\nrespectively, and the Ph.D. degree in electrical\nand electronic engineering from the University of\nJohannesburg, South Africa. His research interests\ninclude machine learning and deep learning for\nfinance and healthcare applications.\nNOBERT JERE received the M.Sc. and Ph.D.\ndegrees in computer science from the University of\nFort Hare, South Africa, in 2009 and 2013, respec-\ntively. He is currently an Associate Professor with\nthe Department of Information Technology, Walter\nSisulu University, South Africa. He has authored\nor co-authored numerous peer-reviewed journal\narticles and conference proceedings, chaired/co-\nchaired international conferences, and serves as\na reviewer for numerous reputable journals. His\nmain research interest includes ICT for sustainable development.\n96910 VOLUME 12, 2024",
  "Indonesian Journal of Electrical Engineering and Computer Science \nVol. 26, No. 1, April 2022, pp. 362~373 \nISSN: 2502-4752, DOI: 10.11591/ijeecs.v26.i1.pp362-373      362  \n \nJournal homepage: http://ijeecs.iaescore.com \nMultilayer perceptron artificial neural networks-based model \nfor credit card fraud detection \n \n \nBassam Kasasbeh, Balqees Aldabaybah, Hadeel Ahmad \nDepartment of Computer Science, Faculty of Information Technology, Applied Science Private University, Amman, Jordan \n \n \nArticle Info  ABSTRACT  \nArticle history: \nReceived Sep 17, 2021 \nRevised Feb 2, 2022 \nAccepted Feb 15, 2022 \n \n Nowadays, credit card fraud has emerged as a major problem. People are \nbecoming increasingly using credit cards to pay for their t ransactions, it has \nbecome more popular and essential in our lives. Fraudsters are dev eloping \nnew strategies and techniques over time, and it is not easy fo r humans to \nmanually check out all transactions. The cost of fraudulent transac tions is",
  "new strategies and techniques over time, and it is not easy fo r humans to \nmanually check out all transactions. The cost of fraudulent transac tions is \nsignificant and without prevention mechanisms it is rising. Fi nding the best \nmethodology to detect fraudulent transactions is a crucial asse t to the \nindustry to reduce the fraud financial loss. Artificial neural netw orks (ANN) \ntechnique is considered as one of the effective techniques that has proved its \nefficiency in detecting credit card fraud transactions with hig h precision and \nminimum cost. In this paper, we propose a multilayer percept ron (MLP) \nANN-based model solution to improve the accuracy of the detection process. \nThe performance of the methodology is measured based on the precision, \nsensitivity, specificity, accuracy, F-measure, area under curve (AUC) and \nroot mean square error ( RMSE). Moreover, we illustrate the performance \nresults of these measures with a descriptive analysis. Exp erimental results",
  "root mean square error ( RMSE). Moreover, we illustrate the performance \nresults of these measures with a descriptive analysis. Exp erimental results \nhave shown that the proposed ANN-based model is efficient an d does \nimprove the accuracy of the detection of fraudulent transactions. \nKeywords: \nArtificial neural networks \nCredit card fraud  \nMachine learning \nMultilayer perceptron online \ntransaction \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nBassam Kasasbeh \nDepartment of Computer Science, Faculty of Information Technology, Applied Science Private University \nP.O.BOX 166 Amman 11931, Jordan \nEmail: b_kasasbeh@asu.edu.jo \n \n \n1. INTRODUCTION  \nDetecting credit card fraud is becoming more and more import ant in our lives. The need to pay over \ninternet via credit cards are increasingly a demand. Altho ugh the number of credit card fraud patterns may",
  "internet via credit cards are increasingly a demand. Altho ugh the number of credit card fraud patterns may \nform a small percentage of transactions, but the cost may  be huge [1]. By preventing fraudulent transactions, \nwe can save a significant amount of money that would traditionally be lost. \nDetection is a data mining technique that relies on analyzing past records in order to find the desired \npattern. In case of fraud detection, the desired pattern is the ability to distinguish between what is fraud and \nwhat is not. In financial payments, the credit card fraud transaction can be defined as the unauthorized usage \nof credit card data to conduct a financial transaction or remove balance from the account [2]-[4]. Generally, \ncredit card fraud can be conducted by fraudulent physically (a ctual steal), or virtually. Because virtual fraud \nis more likely to happen, and riskier, as the user will not know about it till the financial institution blocks the",
  "is more likely to happen, and riskier, as the user will not know about it till the financial institution blocks the \ntransaction or the user reports abnormal behavior, means it can be undetected for long time. Therefore, fraud \ndetection is considered as an online problem where the presen ce of a fraud detection system for every \nfinancial institution is a must [5]. \nCredit card fraud detection has some difficulties. First , due to the time and cost, it is not easy for \nhumans to check all transactions and manually figure out incorrect patterns. The huge number of transactions",
  "Indonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nMultilayer perceptron artificial neural networks-based model for credit card … (Bassam Kasasbeh) \n363 \nmakes it uneasy for a human analyst to detect fraudulent patterns [6]. In real world, the massive transactions \nare scanned by automated tools, analyzed by classifiers who in  turn find and alert the suspicious ones, alerts \nare then inspected by professional investigators [6]. If we put f raud detection performance in our \nconsideration, then the massive transactions cannot be verified by investigators due to time and cost \nchallenges [7]. Second, due to some reasons the distributi on of the dataset changes over times, for example, \nCardholders may change their purchasing behavior over seasons , and fraudsters strategies and techniques \nevolve over time, this is called concept drift, which refe rs to when the dataset shift [8]. In the presence of",
  "evolve over time, this is called concept drift, which refe rs to when the dataset shift [8]. In the presence of \nconcept drift, it is difficult to decide whether a transac tion is fraudulent or original based on previous data \n[9]. In the presence of this issue, the trained detecti on models on past records will be less efficient of \ndetecting fraudulent patterns when new records arrive. Thus, i n such situation, we need to guarantee a high \naccuracy of the system. Third, it is very challenging to f ind the best strategy to detect fraudulent patterns \nsince the data are scarcely available due to confidential issues and typically, they are unbalanced [6]. Finding \nthe right strategies is a crucial asset to the industry to enhance the fraud detection process.  \nTaking all the difficulties into consideration, it is important to keep a balance between the need of \nreducing the number of fraudulent transactions to be detec ted by humans and maintaining a high accuracy of",
  "reducing the number of fraudulent transactions to be detec ted by humans and maintaining a high accuracy of \nthe system when the dataset is shifted. We aim to find t he best methodology in artificial neural networks \n(ANN) to implement a credit card detection solution that  provides high precision and that is critical to saving \ntime and money. In this paper we focus on measuring the sol ution correctness by its ability to detect frauds \ncorrectly rather than classifying a normal one as a fr aud. Neural network is one of the machine learning \nclassification techniques that has proved its efficiency i n pattern recognition areas (e.g., detection problems), \nand its fast convergence [10].  \nIn real-world, the process of collecting real credit ca rd dataset is difficult since there are some \nrestraints due to privacy and confidential issues. We considered a real dataset made by European cardholders’",
  "restraints due to privacy and confidential issues. We considered a real dataset made by European cardholders’ \ntransaction as the source domain. The dataset contains  transactions collected from European cardholders in \nSeptember 2013 and it is highly unbalanced. The dataset is unbala nced due to that the number of fraudulent \ntransactions is much smaller than the valid transactions [11]. The rest of the paper is organized as: Section 1 \ndescribes the related work, section 2 explains our methodol ogy, section 3 demonstrates our experiments, and \nsection 4 concludes the paper. \n \n \n2. RELATED WORK  \nThe problem of credit card fraud detection has been early  investigated in the literature. Many \ndifferent techniques and different approaches have been use d to handle the issues of imbalanced dataset, \nconcept drift, and verification/feedback latency as well as other issues. Credit card fraud detection is a hot",
  "concept drift, and verification/feedback latency as well as other issues. Credit card fraud detection is a hot \nresearch area in which there is always a place for impr ovement due to the high change rate in customer \nbehaviors when using their cards, as well as the fraudster ac tions change over time (concept drift). Few \nworks have explored this issue, proposed a real-time framewo rk using SOM [12] . The effectiveness of \nincremental learning model in which when new transactions a re added to the dataset, the model is updated to \nadapt to the presence of the new instances of data [6]. \nAs in this paper we choose to handle the problem using artificial neural network (ANN) we will take \nthe chance to further explore the applied ANN in the lite rature for fraud detection. One of the early systems \nof credit card fraud detection suggested by Aleskerov et al . [13] was CardWatch system. A feed forward",
  "of credit card fraud detection suggested by Aleskerov et al . [13] was CardWatch system. A feed forward \nneural network architecture was proposed as part of the syst em, in which it was trained using past records of \ncustomer to be able to detect possible frauds. Experiments  showed that fraud detection rate was 85%. On the \nother hand Brause et al. [14] used ANN to investigate the impact of training the model w ith symbolic and/or \nanalog features. The main goal was to help in minimizing the n umber of false alarms to be recognized by the \nfinancial institution as fraud or not. Their model was evaluated based on the confidence value. similarly, Guo \nand Li [15] used confidence-based ANN to be able to detect fraudule nt transactions. Their method basically \nrelied on a confidence threshold value to classify if a t ransaction is a fraud or normal. Authors suggested \nROC analysis technique to ensure the value of threshold is reasonable.",
  "relied on a confidence threshold value to classify if a t ransaction is a fraud or normal. Authors suggested \nROC analysis technique to ensure the value of threshold is reasonable.  \nOthers have used the ANN combined with other techniques to enh ance and improve the detection \nresults. According to Bahera and Panigrahi [16] suggested a methodology to reduce the misclassification rate \nusing a feed forward back propagation (FFNNBP) neural network i n a later phase of the system combined \nwith fuzzy logic. The aim of ANN was to strengthen the in itial observation of a transaction that has been \nclassified as fraud by fuzzy algorithm in the early phase. T his ensured if the suspicious transactions are \nactually fraud or not based on similarity with trained patte rns. Results showed that 93.90% of transactions \nwere correctly classified. Similarly, Wang et al. [17] used three layers feed forward back propagation neural",
  "were correctly classified. Similarly, Wang et al. [17] used three layers feed forward back propagation neural \nnetwork (BPNN) with whale optimization algorithm (WOA) proposed as (WOA-BP) where the role of WOA \nwas optimizing the weights in BPNN to enhance the convergenc e speed of the training. According to the",
  "          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 26, No. 1, April 2022: 362-373 \n364 \nevaluation with similar algorithms from the literature and based on real dataset, WOA-BP shows the smallest \nmean squared error, and the fastest convergence rate among other similar strategies in the literature.  \nThe acquiring of real-world credit card dataset is difficu lt for mainly security and privacy issues. \nDespite of the lack of data because of sensitivity, few works in the literature have explored the effectiveness \nof using ANN models experimented and evaluated on real datase ts. For example, Fu et al. [18] proposed a \nconvolutional neural network (CNN) to avoid the model overf itting. The experiment was conducted on real \ntransactions given by a commercial bank with around 26 M tr ansactions, that has almost 4K as frauds. \nAuthors had split the data for training, validation and texting based on transactions timeline.",
  "transactions given by a commercial bank with around 26 M tr ansactions, that has almost 4K as frauds. \nAuthors had split the data for training, validation and texting based on transactions timeline. \nOthers have investigated different types of deep learning net works. According to Roy et. al.  [19] \nexplored the results of training ANN, long short-term memo ry (LSTM) and gated recurrent unit (GRU) for \nthe credit card fraud detection problem. The experiment was conducted on real dataset from a financial \ninstitution, where results has shown that GRU have outperformed other models in term of accuracy. \nThe most related work to our proposed solution was the one found in [20]. In this work authors have \nproposed ANN backpropagation algorithm. The experiment was conducted on the same dataset as the one we \nare using. In addition, authors have suggested under sampling as  the sampling technique to handle the",
  "are using. In addition, authors have suggested under sampling as  the sampling technique to handle the \nimbalanced dataset issue. Performance metrics have shown that 94.2% accuracy on imbalanced dataset as the \nfalse positive (FP) was 1.3%. In comparison to our propo sed solution in this paper, we are investigating the \ncorrectness of building ANN classification model without ma nipulating the imbalance dataset in which we \nchoose the best model based on an iterative approach. \nOn the other hand, many researchers have studied the applic ation of different other machine \nlearning techniques on the fraud detection problem. This is becaus e the fraud detection problem is rich of \nissues that can be enhanced and improved by metaheuristic an d fuzzy concepts. Researchers have used \nBayesian network algorithm, Behera and Panigrahi [16] proposed a fraud detection approach that goes in \nthree steps. The main supervised learning is at the third step where a neural network learns among the",
  "three steps. The main supervised learning is at the third step where a neural network learns among the \nsuspicious transactions to determine if it was actually a fraud or not. Maes et al . [21] investigated the \napplication of Bayesian network in fraud detection with ANN to solve the uncertainty issue. In addition, \nPanigrahi et al. [22] proposed a novel approach that consists of 4 main pha ses where not only records from \npast were used to detect fraud, but also current transactions . Bayesian network was used as last check in the \nproposed model. Based on the Bayesian learning, the model wa s able to detect fraud transaction out from the \nsuspicious ones. \n Moreover, Srivastava et al. [23] showed how credit card fraud can be detected with high co verage \nand low false alarm rate using hidden Markov model (HMM). He st ated how HMM can decrease the false \npositive transactions. Many other works had used different techniques for the detection problem, for example",
  "positive transactions. Many other works had used different techniques for the detection problem, for example \nBhattacharyya et al. [24] investigated the application of support vector machine (SVM ) and random forest in \nbinary classification problem as fraud detection. Where SVM was able to map high-dimensional feature \nspace of inputs without adding computational complexity and it  performed well with under sampling \ntechnique to handle the skewed data. Similarly, Sahin and Duman [25] proposed SVM and RF for credit card \ndetection where RF outperforms the SVM for relatively sma ll amount of transactions. Other techniques \ninclude decision tree [25] , [26], genetic algorithms [27], logistic regression [28], and a ssociated rules [29] . \nTable 1 summarizes the most related work found in literature  that applied ANN to investigate the problem of \ncredit card fraud detection. \n \n \nTable 1. Three-line representation",
  "Table 1 summarizes the most related work found in literature  that applied ANN to investigate the problem of \ncredit card fraud detection. \n \n \nTable 1. Three-line representation \nWork/system Main goal/motivation ANN architecture Measurements Main Results \nCARDWATCH Used ANN as part of the whole \ndetection system \nFeed forward ANN \nwith 3 layers \n(RMSE)=0.16 set \nto detect fraud \ntransactions \n85% fraud detection rate \nFuzzy logic and \nneural network \nMinimize the misclassifications \nby fraud detection system \nFFBPNN, used at \nlater phase \nTP, FP and ROC 93.90% (TP)correctly \nclassified transactions \nDeep learning Investigate different NN ANN, ANN, LSTM \nand GRU \nAccuracy GRU outperform others \nNeural Network Decrease number of fraud \ntransactions that are \nmisclassified (FP) \nANN BP with \nscaled conjugate \ngradient \nAccuracy and FP 94.2% accuracy (FP) \n1.3% \n \n \n3. METHODOLOGY \nFraud detection is considered as an online problem and requires r eliable and flexible methods to",
  "scaled conjugate \ngradient \nAccuracy and FP 94.2% accuracy (FP) \n1.3% \n \n \n3. METHODOLOGY \nFraud detection is considered as an online problem and requires r eliable and flexible methods to \ndetect the presence of credit card fraud in real-time. Nowada ys, deep learning is the most powerful and \ninteresting machine learning technique, and artificial neural n etwork (ANN) is an efficient way to improve",
  "Indonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nMultilayer perceptron artificial neural networks-based model for credit card … (Bassam Kasasbeh) \n365 \nthe performances of fraud detection systems. The most favorable point associated with ANN is \ncomprehensibility, learning from example, tolerance to noisy, and parallelism. In this Section, Subsection 2.1 \ndescribes the dataset that was used to assess the generalization of the ANN models.  \n \n3.1.  Dataset description \nObtaining credit card fraud datasets is very difficult because banks do not publish any data related to \ncustomer's transactions. In this paper, we used a publicly avail able dataset, the Credit-card dataset [30]. The \nCredit card fraud detections dataset was obtained from a mac hine learning group (MLG) and artificial \nintelligence ( AI) workbench public platform as a part of the Université Libre de Bruxelles (ULB). This",
  "intelligence ( AI) workbench public platform as a part of the Université Libre de Bruxelles (ULB). This \ndataset was gathered in September 2013 from MasterCard transacti ons of European cardholders. The dataset \ncontains transactions created in a record period in just  two days. Wherever there are a total of 285,607 \ntransactions in total, with 0.172% of them being fraud cases.  This dataset contains 31 features, 28 of which \nhave been turned into numerical input values using principal com ponent analysis (PCA) due to a \nconfidentiality concern. These features are named as V1, V2, .. , and V28. According to [31], the feature \ncontains general information such as gender, marital status, age in months, Housing, Job, Telephone, Foreign \nworker, and identification (ID). Other banking information includes credit limit, past m onth bills, past month \npayments, account status, Wage assignments, other existing c redits, credit history, purpose, saving account,",
  "payments, account status, Wage assignments, other existing c redits, credit history, purpose, saving account, \ncredit amount, debtors, property, number of existing credit s, credit card number, and personal identification \nnumber (PIN). time, amount, and class are the other three labeled var iables that do not bind with principal \ncomponent analysis (PCA). The time variable records the  amount of time that has passed between each \ntransaction and the first transaction in the dataset in  seconds. The transaction amount is stored in the amoun t \nvariable. Class variable maps the output 1 in case of fr aud detection and 0 for normal transactions. The \ndataset statistics are summarized in Table 2. \n \n \nTable 2. Dataset summary statistics \nNumber of Features Number of Instances \n31 284807 \n28 unlabeled features obtained after \nPCA \n3 labeled features (Time, Amount, Class) 492 for Fraudulent cases. 284315 for Normal \ncases \n \n \n3.2.  Artificial neural network",
  "31 284807 \n28 unlabeled features obtained after \nPCA \n3 labeled features (Time, Amount, Class) 492 for Fraudulent cases. 284315 for Normal \ncases \n \n \n3.2.  Artificial neural network \nMachine learning techniques can extract useful patterns and info rmation from large datasets and is \nused for decision-making tasks and to evaluate future events  probabilities. The insights derived from \nMachine learning are used for fraud detection, marketing, and sc ientific discovery [32] , [33]. In this paper, \nwe used the ANN technique with multi-layer perceptron (MLP) to generate a model to detect credit card \nfraudulent transactions. ANN is a mathematical model based on the emulation of the biological neural \nsystem. This technique consists of three main stages: mult iplication, addition, and activation. The value of \neach artificial neural is multiplied by individual weights . On the middle side of the ANN is the total function",
  "each artificial neural is multiplied by individual weights . On the middle side of the ANN is the total function \nthat includes all the inputs’ weight. At the end of the AN N is the total input that has been weigh ted and \nalready went through the activation phase that is also called the transfer function [34]. \nMLP is a feed-forward ANN made up of a group of neurons linked together by linking weights. \nMLP converts a set of inputs into the desired outputs. Figure 1 shows the construction of MLP, which is \nmade up of three basic parts: an input layer, a hidden layer, and an output layer. The input layer gets the data, \nwhich it then transmits to the first hidden layer, which forwards it until it reaches the output layer [34], [35]. \n \n \n \n \nFigure 1. MLP",
  "          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 26, No. 1, April 2022: 362-373 \n366 \nEach layer is made up of a certain number of neurons. Weight and bias are used to connect neurons \nbetween layers. The following equation yields the output (Oj) of each artificial neuron j in the hidden layer: \n \n𝑂𝑗 = 𝑓(∑ 𝑤𝑖𝑥𝑖 + 𝑏𝑛\n𝑖=1 ) (1) \n \nwhere n is the number of neurons in the previous layer, w is the weight, x is the input value, b is the bias, and \nf the sigmoid activation function: \n \n𝑓(𝑥) =\n1\n1−𝑒−𝑥\n (2) \n \n3.3.  Model building \nIn this paper, we aim to find the best ANN model for fraud dete ction system. Due this end, \nvariations of the number of neurons and the hidden layer s have been tested. The number of input data and \noutput units of multiple layers of neurons, the complexi ty of the classification problem, and the number of \ntraining cases to be learned all affect the best model [36] . Therefore, an iterative process is used in this paper",
  "training cases to be learned all affect the best model [36] . Therefore, an iterative process is used in this paper \nto determine the best model. A 10-fold cross validation appro ach is developed in the iterative process to \naccess all feasible models. Algorithm 1 describes the steps towards finding the best ANN model. \n \nAlgorithm 1. Find-best-ann-model \n1:Procedure findBest(D,C) \n2: ⯈Input: D (array of n datasets),C (Configuration set of NN) \n3: ⤷c CONTAINS (L,M,N,V,S,E,H) \n4: ⯈Output B = B_1,B_2,....,B_n \n5: ⤷ Bi is the best model of dataset i \n6: For d_j  ∈D do \n7: N_c←number of neurons in the output class \n8: For NHL ←1 till 3 do  \n9: IF NHL ←1 then  \n10: N_p← number of neurons in the input layer \n11: Else  \n12: N_p← number of neurons in the previous layer \n13: End IF  \n14: #generate variations of the same hidden layer \n15: N← 〖(N〗_p+N_C)/2  \n16: V←[N-1,N,N-1]  #three hidden layers with different number of N  \n17: Foreach element in V do  \n18: T← Build NN(V_i,C)",
  "14: #generate variations of the same hidden layer \n15: N← 〖(N〗_p+N_C)/2  \n16: V←[N-1,N,N-1]  #three hidden layers with different number of N  \n17: Foreach element in V do  \n18: T← Build NN(V_i,C) \n19: Trained← Train(T)  #return F-score \n20: B_i← if(Trained is better than previous models) \n21: End For  \n22: End For  \n23: End For \n24: End Procedure (return B) \n \nAccording to Algorithm 1, the procedure starts with defining the  model configurations to ensure the \nfair of comparisons. As first step, we start by trying all the variations of neurons within one, two, and three \nhidden layers (for in line 7) as experiments show that adding more hidden layers gives no better results than \nthese layers. The process of selecting the number of neurons in the current hidden layers is done according to \nthe equation in line 13. After considering three variations of  neurons number for the hidden layer, it is now",
  "the equation in line 13. After considering three variations of  neurons number for the hidden layer, it is now \nthe time of finding the best result given the number of hidden layers, number of neurons in each hidden layer, \nand the required configurations (for loop line 15-19). Line 17 fi ned the best model in term of the F-measure \nsince other measurements terms may be misleading when dealing with imbalanced dataset (see section 2.4). \nThe following are the properties of all ANN models trained in this experiment: \n- A multilayer perceptron algorithm (MLP) neural network artificial was used. \n- The number of neurons (nodes) in the input layer was 29, and there were two output neurons (nodes). \n- For training, the backpropagation (BP) algorithm was used. \n- The sigmoid activation function was used to test each model by using (2).  \n \n3.4.  Performance evaluation \nThe metrics considered in the performance evaluation of the models in this paper are given as:",
  "3.4.  Performance evaluation \nThe metrics considered in the performance evaluation of the models in this paper are given as: \n- The confusion matrix is used to evaluate the result, a nd precision, recall, and accuracy are calculated. It \nhas two types of classes: actual and predicted.",
  "Indonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nMultilayer perceptron artificial neural networks-based model for credit card … (Bassam Kasasbeh) \n367 \n- Accuracy: The ratio of total predicted transactions correctly classified. \n- Sensitivity (Recall): is a metric that indicates how often a test accurately generates a positive result for \nobservations that have the condition being tested for. \n- Specificity: is a percentage of correctly classified abn ormal samples. Higher the specificity, better the \nmodel. \n- Precision: indicates how many of the cases that were correctly predicted turned out to be positive. \n- F-Measure: The weighted harmonic means of the two fractions  precision and recall of the test, which is \na measure of a test's accuracy. The outcome is a number  between 0.0 and 1.0, with 0.0 being the worst \nF-measure and 1.0 representing the best F-measure. \n- Area under curve (AUC): represents the degree of separability . It indicates how well the model can",
  "F-measure and 1.0 representing the best F-measure. \n- Area under curve (AUC): represents the degree of separability . It indicates how well the model can \ndistinguish between classes. \n- Root mean square error (RMSE): calculates the differences  between values predicted by a hypothetical \nmodel and the observed values. It measures the accuracy of the projected model's fit to the actual data. \n \n \n4. RESULTS AND DISCUSSION \nIn order to apply the classification, we applied Weka 3.8.5. Ex periments were conducted on an Intel \nCore i5 with 8.0 GB of RAM and 2.40 GHz processor with Windows 10 64-bits operating system. We used \n10-Fold cross-validation (CV) to partition the training dataset  into 10 equal portions and extract varied \nresults. This method uses 9 of the 10 elements to train a n eural network and the remaining part to evaluate it. \nThe identical procedure is followed for all ten parts, wi th the exception that the test set is determined using a",
  "The identical procedure is followed for all ten parts, wi th the exception that the test set is determined using a  \nsliding window and the remaining parts are used to train the n eural network. Following that, the results are \ncollated, and the averages of all folds are calculated. Th e 10-Fold CV main aspect is that it uses all of the \nrecords in the dataset alternately for training and testing. \nThe best ANN model for the fraud detection system was deter mined through an experimental study. \nAccuracy is an important metric to consider but it does n ot always give the full picture. Sometimes when \nusing only accuracy to find the best model can be misleading [37] , [38], especially with an unbalanced \ndataset. The dataset used is very unbalanced which is biased towar ds the category of non-fraud cases. For \nsuch problems, classifiers should be evaluated with additional measures.  \nThe Authors in Boughorbel  et al.  [39] showed that using accuracy as a performance measure in",
  "such problems, classifiers should be evaluated with additional measures.  \nThe Authors in Boughorbel  et al.  [39] showed that using accuracy as a performance measure in \nunbalanced datasets was insufficient. In dealing with the im balanced data, the authors found that the F-\nmeasure and area under curve (AUC) produced higher consistency than accuracy. For that, we used the 10-\nfold cross validation method to run many experiments to find the best ANN model with the best classification \nF-measure. \n \n4.1.  Results analysis with one hidden layer \nSeveral network structures were investigated to discover the optimal ANN model to use. Because \nthere are 29 neurons in the input layer and two neurons in the output layer. The number of neurons in the first \nhidden layer will be 15.5, according to Algorithm 1.  Therefore, the following models will be formed in the \nfirst hidden layer: 29-15-2, 29-14-2, and 29-16-2. Table 3 presents th e confusion matrix for the best model",
  "first hidden layer: 29-15-2, 29-14-2, and 29-16-2. Table 3 presents th e confusion matrix for the best model \nwith one hidden layer (29-14-2). This model represents 29 input variables, one hidden layer of 14 neurons \nand two output layers. The F-measure for fraud class in thi s model is 84.757%. All the remaining models \nproduced slightly low classification F-measure 84.021% for 29-16-2 model, and 82.366% for 29-14-2 model. \nThe root mean square error (RMSE) of this model by using (9)  is 0.0218. Table 3 presents the confusion \nmatrix and the result summary for this model. \n \n \nTable 3. Summary results best model with one hidden layer \n  Predicted F-measure % Precision % Sensitivity % Specificity %   Normal Fraud \nOriginal Normal 284274 41 99.975% 99.986% 99.965% 79.675% \nFraud 100 392 84.757% 79.675% 90.531% 99.986% \n \n \n \n4.2.  Results analysis with two hidden layers \nIn the first hidden layer, we obtained three different nu mbers of neurons; 14, 15 and 16. Therefore,",
  "4.2.  Results analysis with two hidden layers \nIn the first hidden layer, we obtained three different nu mbers of neurons; 14, 15 and 16. Therefore, \nby applying step 2 from our methodology, the number of neurons in the second layer will be as the following: \ni) When the first hidden layer has 14 neurons, the second hidden layer contains 14/2 neurons, which is 7 \nneurons. The network models will be 29-14-7-2, 29-14-6-2, and 29-14- 8-2. ii) When the first hidden layer",
  "          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 26, No. 1, April 2022: 362-373 \n368 \nhas 15 neurons, the second hidden layer contains 15/2 neurons , which is 7.5 neurons. The network models \nwill be 29-15-7-2, 29-15-6-2, and 29-15-8- 2. iii) When the first hidden layer has 16 neurons, the seco nd \nhidden layer contains 16/2 neurons, which is 8 neurons. The net work models will be 29-16-8-2, 29-16-7-2, \nand 29-16-9-2.  \nAfter we tested 9 different models with two hidden layers, we found that the 29-15-8-2 model has \nthe best F-measure value for fraud class 85.129% with an err or 0.0214. Also, this model has the best \nprecision, sensitivity, and specificity. Table 4 presents th e confusion matrix and the result summary for this \nmodel. \n \n \nTable 4. Summary results the best model with two hidden layers \n  Pedicted F-measure % Precision % Sensitivity % Specificity %   Normal Fraud \nOriginal Normal 284274 41 99.976% 99.986% 99.966% 99.986%",
  "Pedicted F-measure % Precision % Sensitivity % Specificity %   Normal Fraud \nOriginal Normal 284274 41 99.976% 99.986% 99.966% 99.986% \nFraud 97 395 85.129% 80.285% 90.596% 80.285% \n \n \n4.3.  Results analysis with three hidden layers \nWe obtained 9 different models when the number of hidden l ayers was two so that when a third \nhidden layer is added, 27 different models were tested. Table 5 s hows the confusion matrix and the summary \nresult for the best classification F-measure for fraud class is 83.193% is obtained when the network model is \n29-15-8-4-2. When we trained the dataset with four and five hidden layer s, the same results are always \nobtained in all possibilities with different options such as changing the learning rate. As we note from Table 6 \nthat all 492 fraud cases were classified as normal cases, due to the large gap between the number of frauds \nand normal cases in the dataset. \n \n \nTable 5. Summary results the best model with three hidden layers",
  "and normal cases in the dataset. \n \n \nTable 5. Summary results the best model with three hidden layers \n  Pedicted F-measure % Precision % Sensitivity % Specificity %   Normal Fraud \nOriginal Normal 284246 69 99.971% 99.976% 99.966% 99.976% \nFraud 98 394 82.513% 80.081% 85.097% 80.081% \n \n \nTable 6. Summary results the best model with three hidden layers \n  Pedicted F-measure % Precision % Sensitivity % Specificity %   Normal Fraud \nOriginal Normal 284315 0 99.914% 100.00% 99.827% 100.00% \nFraud 492 0 NaN 0.000% NaN 0.000% \n \n \n4.4.  Results analysis with four hidden layers \nWe started building the ANN models with one hidden layer and ended with 5 hidden layers. We can \nnotice that the best model was obtained by using two hidden la yers; the model is 29-15-8-2 and obtained \n99.950% classification F-measure. Using one hidden layer achieve s good results, but when another hidden \nlayer has been added, the best classification F-measure ha s slightly risen from 84.757% to 85.129%. Adding",
  "layer has been added, the best classification F-measure ha s slightly risen from 84.757% to 85.129%. Adding \nthree hidden layers lowered the F-measure and did not improve the results. \nThese results can be explained as: If you have too few hidden layers, you will get high training error \nand high generalization error due to under fitting. This part deals with the summary results collected during \nexperiments. Table 7 compares results between the best ANN m odel in the one hidden layer, two hidden \nlayers, and three hidden layers. Parameters chosen for comparison of results are F-measure, precision, \nsensitivity, specificity, accuracy, AUC, and RMSE. We no tice that in all cases the results are remarkably \nclose to each other, and this is expected due to the structure of the ANN. This is because that the ANN works \non learning how to detect the status of the credit card in  the normal cases (without fraud) due to the presence",
  "on learning how to detect the status of the credit card in  the normal cases (without fraud) due to the presence \nof a large number of normal transactions in the used data set. Nevertheless, there are some factors that help \nfinding the best ANN model for detecting fraud or normal t ransactions as the results show that ANN with \ntwo hidden layers, especially 29-15-8-2 model, has slightly higher accuracy from the other models. \n \n4.5.  Summary of the results \nWe started building the ANN models with one hidden layer and ended with 5 hidden layers. We can \nnotice that the best model was obtained by using two hidden la yers; the model is 29-15-8-2 and obtained",
  "Indonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nMultilayer perceptron artificial neural networks-based model for credit card … (Bassam Kasasbeh) \n369 \n99.950% classification F-measure. Using one hidden layer achieve s good results, but when another hidden \nlayer has been added, the best classification F-measure ha s slightly risen from 84.757% to 85.129%. Adding \nthree hidden layers lowered the F-measure and did not improve the results. \nThese results can be explained as: If you have too few hidden layers, you will get high training error \nand high generalization error due to under fitting. This part de als with the summary results collected during \nexperiments. Table 7 compares results between the best ANN m odel in the one hidden layer, two hidden \nlayers, and three hidden layers. Parameters chosen for comparison of results are F-measure, precision, \nsensitivity, specificity, accuracy, AUC, and RMSE. We no tice that in all cases the results are remarkably",
  "sensitivity, specificity, accuracy, AUC, and RMSE. We no tice that in all cases the results are remarkably \nclose to each other, and this is expected due to the structure of the ANN. This is because that the ANN works \non learning how to detect the status of the credit card in the normal cases (without fraud) due to the presence \nof a large number of normal transactions in the used data set. Nevertheless, there are some factors that help \nfinding the best ANN model for detecting fraud or normal t ransactions as the results show that ANN with \ntwo hidden layers, especially 29-15-8-2 model, has slightly higher accuracy from the other models. \n \n \nTable 7. Summary of results regarding multiple hidden layers \nNumber of hidden Layers 1 2 3 \nBest Network Model 29-14-2 29-15-8-2 29-15-8-4-2 \nAccuracy 99.9505% 99.9515% 99.9414 % \nRoot mean square error (RMSE) 0.0218 0.0214 0.0234 \nArea under curve (AUC) 0.8983 0.9014 0.9003 \nF-measure 99.949% 99.950% 99.940%",
  "Accuracy 99.9505% 99.9515% 99.9414 % \nRoot mean square error (RMSE) 0.0218 0.0214 0.0234 \nArea under curve (AUC) 0.8983 0.9014 0.9003 \nF-measure 99.949% 99.950% 99.940% \nPrecision 99.949% 99.950% 99.940% \nSensitivity 99.950% 99.952% 99.941% \nSpecificity 79.710% 80.319% 80.116% \n \n \nThe F-measure comparison for each model is shown in Figure 2 which showed the high \nconvergence of results between the three models in the different layers, but there was a very slight superiority \nfor the model with two hidden layers for fraud detection. In all three models, ANN with two hidden layers \nperforms better for fraud class detection, as shown in Fi gures 3-5. In comparison to one hidden layer and \nthree hidden layers, it provides the highest F-measure in a ll three models. However, ANN with three hidden \nlayers showed the lowest F-measure in all the three models  as compared to the other two models in Figure 2.",
  "layers showed the lowest F-measure in all the three models  as compared to the other two models in Figure 2. \nFigures 3-5 show the same results of ANN with two hidden lay ers for other performance measurement \nparameters (precision, sensitivity, and specificity).  \n \n \n \n \nFigure 2. F-Measure",
  "          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 26, No. 1, April 2022: 362-373 \n370 \n \n \nFigure 3. Precision \n \n \n \n \nFigure 4. Sensitivity \n \n \n \n \nFigure 5. Specificity",
  "Indonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nMultilayer perceptron artificial neural networks-based model for credit card … (Bassam Kasasbeh) \n371 \nThe error rate of all methods is shown in Figure 6 using root mean square error (RMSE). This figure \nshows that ANN with two hidden layers was the best. Accordi ng to the results of F-measure, Precision, \nSensitivity, Specificity, and RMSE in Figures 2 –6, the use of an ANN with two hidden layers model \noutperforms other ANN models in detecting fraud in credit card financial transactions.  \nFurthermore, Figure 7 reveals more about the best models chosen from each combination of hidden \nlayers. The ROC figure that plots the relationship betwe en the false positive rate and the true positive rate of  \nthe fraud class shows excellent results for the three models. As shown the value for all models starts from 0.8 \nat least, which gives an indication that any selected model will be able to detect fraud transactions with high",
  "at least, which gives an indication that any selected model will be able to detect fraud transactions with high \naccuracy. \n \n \n \n \nFigure 6. Root mean squared error \n \n \n \n \nFigure 7. ROC curve \n \n \n5. CONCLUSION \nIn this paper, we investigated the problem of fraud detection i n credit card payment systems. We \nform the problem as a binary classification problem. To so lve the problem, we choose to find the best model \nin ANN considering many variations in terms of hidden layers and the number of neurons in each one in an \niterative way. After that, we conducted the experiments on a real dataset and discussed the results in term of \nF-measure. Results show that using ANN succeeded in getting hi gh accuracy in the various layers used \nwhere the F-measure for ANN with one hidden layer the perce ntages of classification F-measure for Fraud \ncases were 84.76%, 85.13%, and 82.51% in one hidden layer, two hidden layer s, and three hidden layers,",
  "cases were 84.76%, 85.13%, and 82.51% in one hidden layer, two hidden layer s, and three hidden layers, \nrespectively. From these results, it can be concluded th at ANN is very useful in detecting fraud in credit card \ntransactions. As future work, we will further investigate the problem of imbalanced credit card dataset.",
  "          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 26, No. 1, April 2022: 362-373 \n372 \nREFERENCES \n[1] B. Lebichot, Y.-A. Le Borgne, L. He- Guelton, F. Oblé, and G. Bontempi, “Deep-learning domain adaptation techniques for credit \ncards fraud detection,” in INNS Big Data and Deep Learning conference, 2019, pp. 78–88, doi: 10.1007/978-3-030-16841-4_8. \n[2] S. Kannan and K. Somasundaram, “Selection of optimal mining algorithm for outlier detection - an efficient method to \npredict/detect money laundering crime in finance industry,” Elysium Journal of Engineering Research and Management , vol. 1, \nno. 1, pp. 30–42, 2014. \n[3] L. Seyedhossein and M. R. Hashemi, “Mining information from credit car d time series for timelier fraud detection,” in 2010 5th \nInternational Symposium on Telecommunications, IST 2010, Dec. 2010, pp. 619–624, doi: 10.1109/ISTEL.2010.5734099.",
  "International Symposium on Telecommunications, IST 2010, Dec. 2010, pp. 619–624, doi: 10.1109/ISTEL.2010.5734099. \n[4] E. W. T. Ngai, Y. Hu, Y. H. Wong, Y. Chen, and X. Sun, “The application of data mining techniques in financial fraud detection: \nA classification framework and an academic review of literature,” Decision Support Systems , vol. 50, no. 3, pp. 559 –569,  \nFeb. 2011, doi: 10.1016/j.dss.2010.08.006. \n[5] A. Thennakoon, C. Bhagyani, S. Premadasa, S. Mihiranga, and N. Kuruwitaarach chi, “Real-time credit card fraud detection using \nmachine learning,” in P roceedings of the 9th International Conference On Cloud Computing, Data Science and Engineering, \nConfluence 2019, Jan. 2019, pp. 488–493, doi: 10.1109/CONFLUENCE.2019.8776942. \n[6] A. Dal Pozzolo, O. Caelen, Y. A. Le Borgne, S. Waterschoot, a nd G. Bontempi, “Learned lessons in credit card fraud detection",
  "[6] A. Dal Pozzolo, O. Caelen, Y. A. Le Borgne, S. Waterschoot, a nd G. Bontempi, “Learned lessons in credit card fraud detection \nfrom a practitioner perspective,” Expert Systems with Applications , vol. 41, no. 10, pp. 4915 –4928, Aug. 2014, doi: \n10.1016/j.eswa.2014.02.026. \n[7] A. D. Pozzolo, G. Boracchi, O. Caelen, C. A lippi, and G. Bontempi, “Credit card fraud detection: A realistic modeli ng and a \nnovel learning strategy,” IEEE Transactions on Neural Networks and Learning Systems , vol. 29, no. 8, pp. 3784 –3797, Aug. \n2018, doi: 10.1109/TNNLS.2017.2736643. \n[8] Y. Lucas an d J. Jurgovsky, “Credit card fraud detection using machine learning: A survey,” Proceedings of the International \nConference on Intelligent Computing and Control Systems, ICICCS 2020 , Oct. 2020, pp. 1264 –1270, doi: \n10.1109/ICICCS48265.2020.9121114. \n[9] F. C arcillo, Y. A. Le Borgne, O. Caelen, Y. Kessaci, F. Oblé, and G. Bonte mpi, “Combining unsupervised and supervised",
  "10.1109/ICICCS48265.2020.9121114. \n[9] F. C arcillo, Y. A. Le Borgne, O. Caelen, Y. Kessaci, F. Oblé, and G. Bonte mpi, “Combining unsupervised and supervised \nlearning in credit card fraud detection,” Information Sciences, vol. 557, pp. 317–331, May 2021, doi: 10.1016/j.ins.2019.05.042. \n[10] U. Fiore , A. De Santis, F. Perla, P. Zanetti, and F. Palmieri, “Using generative adversarial networks for improving classification \neffectiveness in credit card fraud detection,” Information Sciences , vol. 479, pp. 448 –455, Apr. 2019, doi: \n10.1016/j.ins.2017.12.030. \n[11] I. Sadgali, N. Sael, and F. Benabbou, “Bidirectional gated recurrent unit for im proving classification in credit card fraud \ndetection,” Indonesian Journal of Electrical Engineering and Computer Science , vol. 21, no. 3, pp. 1704 –1712, Mar. 2021, doi: \n10.11591/ijeecs.v21.i3.pp1704-1712. \n[12] J. T. S. Quah and M. Sriganesh, “Real -time credit card fraud detection using computational intelligence,” Expert Systems with",
  "10.11591/ijeecs.v21.i3.pp1704-1712. \n[12] J. T. S. Quah and M. Sriganesh, “Real -time credit card fraud detection using computational intelligence,” Expert Systems with \nApplications, vol. 35, no. 4, pp. 1721–1732, Nov. 2008, doi: 10.1016/j.eswa.2007.08.093. \n[13] E. Aleskerov, B. Freisleben, and B. Rao, “CARDWATCH: A neural network base d database mining system for credit card fraud \ndetection,” in IEEE/IAFE Conference on Computational Intelligence for Financial Engineering, P roceedings (CIFEr) , 1997,  \npp. 220–226, doi: 10.1109/cifer.1997.618940. \n[14] R. Brause, T. Langsdorf, and M. Hepp, “Neural data mining for credit card fraud detection,” in Proceedings of the International \nConference on Tools with Artificial Intelligence, Jul. 1999, pp. 103–106, doi: 10.1109/tai.1999.809773. \n[15] T. Guo and G. Y. Li, “Neural data mining for credit card fraud dete ction,” in Proceedings of the 7th International Conference on",
  "[15] T. Guo and G. Y. Li, “Neural data mining for credit card fraud dete ction,” in Proceedings of the 7th International Conference on \nMachine Learning and Cybernetics, ICMLC, Jul. 2008, vol. 7, pp. 3630–3634, doi: 10.1109/ICMLC.2008.4621035. \n[16] T. K. Behera and S. Panigrahi, “Credit card fraud detection: A hybrid approach using fuzzy clustering and neural network,” in  \nProceedings - 2015 2nd IEEE International Conference on Advances in Computing and Com munication Engineering, ICACCE \n2015, May 2015, pp. 494–499, doi: 10.1109/ICACCE.2015.33. \n[17] C. Wang, Y. Wang, Z. Ye, L. Yan, W. Cai, and S. Pan, “Credit card fraud de tection based on whale algorithm optimized BP \nneural network,” in 13th International Conference on Computer Science and Education, ICCSE 2018 , Aug. 2018, pp. 614 –617, \ndoi: 10.1109/ICCSE.2018.8468855. \n[18] K. Fu, D. Cheng, Y. Tu, and L. Zhang, “Credit card fraud detection using convolutiona l neural networks,” in Lecture Notes in",
  "doi: 10.1109/ICCSE.2018.8468855. \n[18] K. Fu, D. Cheng, Y. Tu, and L. Zhang, “Credit card fraud detection using convolutiona l neural networks,” in Lecture Notes in \nComputer Science (including subseries Lecture Notes in Artificial Intell igence and Lecture Notes in Bioinformatics) , vol. 9949 \nLNCS, 2016, pp. 483–490. \n[19] A. Roy, J. Sun, R. Mahoney, L. Alonzi, S. Adams, and P. Beling, “Deep l earning detecting fraud in credit card transactions,” in \n2018 Systems and Information Engineering Design Symposium, SIEDS 2018 , Apr. 2018, pp. 129 –134, doi: \n10.1109/SIEDS.2018.8374722. \n[20] S. Georgieva, M. Markova, and V. Pavlov, “Using neural network for cr edit card fraud detection,” in AIP Conference \nProceedings, 2019, vol. 2159, p. 030013, doi: 10.1063/1.5127478. \n[21] S. Maes, K. Tuyls, B. Vanschoenwinkel, and B. Manderick, “Credit card fra ud detection using bayesian and neural networks,”",
  "Proceedings, 2019, vol. 2159, p. 030013, doi: 10.1063/1.5127478. \n[21] S. Maes, K. Tuyls, B. Vanschoenwinkel, and B. Manderick, “Credit card fra ud detection using bayesian and neural networks,” \nMaciunas RJ, editor. Interactive image-guided neurosurgery. American Association Neurological Surgeons, pp. 261–270, 1993. \n[22] S. Panigrahi, A. Kundu, S. Sural, and A. K. Majumdar, “Credit card fraud detec tion: A fusion approach using Dempster -Shafer \ntheory and Bayesian learning,” Information Fusion, vol. 10, no. 4, pp. 354–363, Oct. 2009, doi: 10.1016/j.inffus.2008.04.001. \n[23] A. Srivastava, A. Kundu, S. Sural, and A. K. Majumdar, “Credit card fraud de tection using Hidden Markov Model,” IEEE \nTransactions on Dependable and Secure Computing, vol. 5, no. 1, pp. 37–48, Jan. 2008, doi: 10.1109/TDSC.2007.70228. \n[24] S. Bhattacharyya, S. Jha, K. Tharakunnel, and J. C. Westland, “Data mining for credit card fraud: A comparative study,” Decision",
  "[24] S. Bhattacharyya, S. Jha, K. Tharakunnel, and J. C. Westland, “Data mining for credit card fraud: A comparative study,” Decision \nSupport Systems, vol. 50, no. 3, pp. 602–613, Feb. 2011, doi: 10.1016/j.dss.2010.08.008. \n[25] Y. Sahin and E. Duman, “Detecting credit card fraud by decision trees and support vector machines,” IMECS 2011 - International \nMultiConference of Engineers and Computer Scientists 2011, vol. 1, pp. 442–447, 2011. \n[26] A. C. Bahnsen, D. Aouada, and B. Otter sten, “Example -dependent cost- sensitive decision trees,” Expert Systems with \nApplications, vol. 42, no. 19, pp. 6609–6619, Nov. 2015, doi: 10.1016/j.eswa.2015.04.042. \n[27] E. Duman and M. H. Ozcelik, “Detecting credit card fraud by genetic algo rithm and scatter search,” Expert Systems with \nApplications, vol. 38, no. 10, pp. 13057–13063, Sep. 2011, doi: 10.1016/j.eswa.2011.04.110.",
  "Applications, vol. 38, no. 10, pp. 13057–13063, Sep. 2011, doi: 10.1016/j.eswa.2011.04.110. \n[28] S. Jha, M. Guillen, and J. Christopher Westland, “Employing transaction aggregati on strategy to detect credit card fraud ,” Expert \nSystems with Applications, vol. 39, no. 16, pp. 12650–12657, Nov. 2012, doi: 10.1016/j.eswa.2012.05.018. \n[29] D. Sánchez, M. A. Vila, L. Cerda, and J. M. Serrano, “Association rules  applied to credit card fraud detection,” Expert Systems \nwith Applications, vol. 36, no. 2 PART 2, pp. 3630–3640, Mar. 2009, doi: 10.1016/j.eswa.2008.02.001.",
  "Indonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nMultilayer perceptron artificial neural networks-based model for credit card … (Bassam Kasasbeh) \n373 \n[30] A. D. Pozzolo, O. Caelen, R. A. Johnson, and G. Bontempi, “Calibrating probabi lity with undersampling for unbalanced \nclassification,” in Proceedings-2015 IEEE Symposium Series on Computational Intelligence, SSCI 2015 , Dec. 2015,  \npp. 159–166, doi: 10.1109/SSCI.2015.33. \n[31] F. Itoo, Meenakshi, and S. Singh, “Comparison and analysis of logistic regres sion, Naïve Bayes and KNN machine learning \nalgorithms for credit c ard fraud detection,” International Journal of Information Technology (Singapore) , vol. 13, no. 4,  \npp. 1503–1511, Aug. 2021, doi: 10.1007/s41870-020-00430-y. \n[32] M. Alkhalili, M. H. Qutqut, and F. Almasalha, “Investigation of applying machine learning for watch-list filtering in anti-money \nlaundering,” IEEE Access, vol. 9, pp. 18481–18496, 2021, doi: 10.1109/ACCESS.2021.3052313.",
  "laundering,” IEEE Access, vol. 9, pp. 18481–18496, 2021, doi: 10.1109/ACCESS.2021.3052313. \n[33] A. Özdemir, U. Yavuz, and F. A. Dael, “Performance evaluation of different classification techniques using different datasets,” \nInternational Journal of Electrical and Computer Engineering , vol. 9, no. 5, pp. 3584 –3590, Oct. 2019, doi: \n10.11591/ijece.v9i5.pp3584-3590. \n[34] A. Krenker, J. Bester, and A. Kos, “Introduction to the artificial ne ural networks,” Artificial Neural Networks - Methodological \nAdvances and Biomedical Applications, pp. 1–18, 2011, doi: 10.5772/15751. \n[35] H. Ramchoun, M. Amine, J. Idrissi, Y. Ghanou, and M. Ettaouil, “Multilayer  perceptron: architecture optimization and training,” \nInternational Journal of Interactive Multimedia and Artificial Intelligence,  vol. 4, no. 1, p. 26, 2016, doi: \n10.9781/ijimai.2016.415. \n[36] S. Xu and L. Chen, “A novel approach for determining the optimal number of hidden layer neurons for FNN’s and its application",
  "10.9781/ijimai.2016.415. \n[36] S. Xu and L. Chen, “A novel approach for determining the optimal number of hidden layer neurons for FNN’s and its application  \nin data mining,” 5th International Conference on Information Technology and Applications, ICITA 2008, 2008, pp. 683–686. \n[37] G. Wu and E. Y. Chang, “Class -boundary alignment for imbalanced dataset learning,” ICML Workshop on Learning from \nImbalanced Data Sets II, pp. 49–56, 2003. \n[38] B. L. Sturm, “Classification accuracy is not enough: On the evaluation of music genre recognition systems,” Journal of Intelligent \nInformation Systems, vol. 41, no. 3, pp. 371–406, Dec. 2013, doi: 10.1007/s10844-013-0250-y. \n[39] S. Boughorbel, F. Jarray, and M. El- Anbari, “Optimal classifier for imbalanced data using Matthews correlation coefficient \nmetric,” PLoS ONE, vol. 12, no. 6, p. e0177678, Jun. 2017, doi: 10.1371/journal.pone.0177678. \n \n \nBIOGRAPHIES OF AUTHORS",
  "metric,” PLoS ONE, vol. 12, no. 6, p. e0177678, Jun. 2017, doi: 10.1371/journal.pone.0177678. \n \n \nBIOGRAPHIES OF AUTHORS  \n \n \nBassam Kasasbeh     received his master’s degree from University of Jordan in \nComputer Science. He is currently a lecturer of at Computer Science  department at Applied \nScience Private University. His research interests include m achine learning, Cyber security , \nInternet of things. He can be contacted at email: b_kasasbeh@asu.edu.jo. \n  \n \nBalqees AL-Dabaybah     CS Master degree graduated in 2018 from Princess \nSumaya University for technology (PSUT). My research interest i n machine learning, cloud \ncomputing, algorithms and HCI. She can be contacted at email: b_aldabaybah@asu.edu.jo. \n  \n \nHadeel Ahmad     received the B.Sc. degree in computer Science from Applied \nScience University, Jordan, in 2001, the M.Sc. degree in comp uter systems from Arab \nAcademy for banking and financial science, Jordan, in 2004. She is currently a lecturer with",
  "Science University, Jordan, in 2001, the M.Sc. degree in comp uter systems from Arab \nAcademy for banking and financial science, Jordan, in 2004. She is currently a lecturer with \nthe Faculty of Information Technology, Applied Science Unive rsity, Jordan, since October \n2018. Her research interests are in machine learning, semant ic web, text mining and \nontologies. She can be contacted at email: h_ahmad@asu.edu.jo.",
  "Credit Card Fraud Detection\nUsing Advanced Transformer Model\n1st* Chang Yu\nIndependent Researcher\nNortheastern University\nBoston, MA, 02115, USA\nEmail: chang.yu@northeastern.edu\n2nd Yongshun Xu\nIndependent Researcher\nUniversity of Massachusetts Lowell\nLowell, MA, 01850, USA\nEmail: Yongshun Xu@student.uml.edu\n2nd Jin Cao\nIndependent Researcher\nJohns Hopkins University\nBaltimore, MD, 21218, USA\nEmail: caojinscholar@gmail.com\n2nd Ye Zhang\nIndependent Researcher\nUniversity of Pittsburgh\nPittsburgh, PA, 15203, USA\nEmail: yez12@pitt.edu\n3rd Yixin Jin\nIndependent Researcher\nUniversity of Michigan, Ann Arbor\nAnn Arbor, MI 48109, USA\nEmail: jyx0621@gmail.com\n3rd Mengran Zhu\nIndependent Researcher\nMiami University\nOxford, OH, 45056, USA\nEmail: mengran.zhu0504@gmail.com\nAbstract—With the proliferation of various online and mobile\npayment systems, credit card fraud has emerged as a significant\nthreat to financial security. This study focuses on innovative",
  "Abstract—With the proliferation of various online and mobile\npayment systems, credit card fraud has emerged as a significant\nthreat to financial security. This study focuses on innovative\napplications of the latest Transformer models for more robust\nand precise fraud detection. To ensure the reliability of the\ndata, we meticulously processed the data sources, balancing the\ndataset to address the issue of data sparsity significantly. We\nalso selected highly correlated vectors to strengthen the training\nprocess. To guarantee the reliability and practicality of the\nnew Transformer model, we conducted performance comparisons\nwith several widely adopted models, including Support Vector\nMachine (SVM), Random Forest, Neural Network, Logistic\nRegression, XGBoost, and TabNet. We rigorously compared these\nmodels using metrics such as Precision, Recall, F1 Score, and\nROC AUC. Through these detailed analyses and comparisons, we\npresent to the readers a highly efficient and powerful anti-fraud",
  "models using metrics such as Precision, Recall, F1 Score, and\nROC AUC. Through these detailed analyses and comparisons, we\npresent to the readers a highly efficient and powerful anti-fraud\nmechanism with promising prospects. The results demonstrate\nthat the Transformer model not only excels in traditional appli-\ncations but also shows great potential in niche areas like fraud\ndetection, offering a substantial advancement in the field.\nIndex Terms—Credit card Fraud Detection, Transformer, pre-\nprocessing, Precision, Recall, F1-Score\nI. I NTRODUCTION\nThe emergence of information technologies such as mobile\ninternet and big data has led to the rise of internet finance,\nbringing convenience to people’s daily lives. However, along\nwith this convenience comes the phenomenon of internet\nfinancial fraud, with credit card transactions being the primary\ntarget of various fraudulent activities. The rapid development\nof internet financial services has significantly increased the",
  "financial fraud, with credit card transactions being the primary\ntarget of various fraudulent activities. The rapid development\nof internet financial services has significantly increased the\nprobability of credit card fraud incidents, resulting not only in\nimmeasurable losses for individuals and businesses but also in\nnumerous socio-economic problems.\nA major technical challenge lies in the fact that fraudu-\nlent transactions account for only a small portion of credit\ncard payments, making it challenging for machine learning\nalgorithms to detect such sensitive behavior [36]. As the\nmost powerful and capable model to date, the Transformer\nlarge model exhibits revolutionary identification capabilities,\nmaking it particularly suitable for recognizing fraudulent data\nin small samples.\nIn recent years, various machine learning techniques have\nbeen applied to tackle the problem of fraud detection. Neural\nNetworks (NN) [7], for instance, have been widely used due to",
  "in small samples.\nIn recent years, various machine learning techniques have\nbeen applied to tackle the problem of fraud detection. Neural\nNetworks (NN) [7], for instance, have been widely used due to\ntheir ability to learn complex features and relationships from\nlarge datasets [20]. NNs can automatically extract features\nfrom raw data and create a hierarchical representation, en-\nabling them to detect fraudulent activities with high accuracy\n[1].\nAnother popular approach is XGBoost, an ensemble learn-\ning method that combines multiple decision trees to create a\nrobust and accurate model [2], [5]. XGBoost has proven to be\neffective in fraud detection tasks, as it can handle imbalanced\ndatasets and capture non-linear relationships between features\n[33].\nTabNet, a more recent addition to the fraud detection toolkit,\nis a deep learning architecture specifically designed for tabular\ndata [3]. It utilizes an attentive mechanism to learn the impor-",
  "[33].\nTabNet, a more recent addition to the fraud detection toolkit,\nis a deep learning architecture specifically designed for tabular\ndata [3]. It utilizes an attentive mechanism to learn the impor-\ntance of each feature and creates interpretable explanations\nfor its predictions. TabNet has shown promising results in\nfraud detection, particularly in scenarios where interpretability\nis crucial [11], [21].\nDespite the success of these techniques, they often face\nchallenges when dealing with complex, high-dimensional data\nand long-range dependencies. This is where Transformer\nmodels, originally developed for natural language processing\ntasks, have shown immense potential [6]. Haowei’s work\n[25] has demonstrated the superiority of transformer-based\nmodels over traditional statistical approaches in accurately\npredicting heart rate time series, effectively capturing complex\ntemporal dependencies and non-linear relationships inherent",
  "models over traditional statistical approaches in accurately\npredicting heart rate time series, effectively capturing complex\ntemporal dependencies and non-linear relationships inherent\nin physiological data. Despite the success of these techniques,\nthey often face challenges when dealing with complex, high-\ndimensional data and long-range dependencies. This is where\narXiv:2406.03733v4  [cs.LG]  12 Nov 2024",
  "Transformer models, originally developed for natural language\nprocessing tasks, have shown immense potential [6], [37].\nTransformers can effectively capture intricate patterns and\nrelationships within the data, thanks to their self-attention\nmechanism. By adapting Transformers for fraud detection, we\ncan leverage their ability to handle complex data structures\nand create more accurate and robust models.\nMoreover, Transformers can be pre-trained on large, unla-\nbeled datasets and then fine-tuned for specific fraud detection\ntasks. This transfer learning approach allows the models to\nlearn general representations of the data and reduces the\nneed for extensive labeled datasets [24]. Transformers have\nthe potential to outperform traditional methods and provide\na more efficient and effective solution for fraud detection\nin various domains. Therefore, in this study, we aim to\nthoroughly explore the feasibility of applying Transformers\nto fraud detection.",
  "a more efficient and effective solution for fraud detection\nin various domains. Therefore, in this study, we aim to\nthoroughly explore the feasibility of applying Transformers\nto fraud detection.\nThe remainder of this research is written as follows. In\nSection II, we provide a comprehensive review of the related\nresearch that has informed the objectives and methodological\nchoices of the current study. This review covers key contribu-\ntions in areas such as statistical modeling, NN, and machine\nlearning approaches to credit card fraud detection [34].\nSection III then delves into the detailed methodology em-\nployed in this work, explaining the data processing, sampling\nstrategies, model architectures, and underlying principles that\nconstitute our fraud detection framework.\nMoving to Section IV , we present the results of our exten-\nsive experimental evaluation, comparing the performance of\nour proposed model against several widely-used benchmark",
  "Moving to Section IV , we present the results of our exten-\nsive experimental evaluation, comparing the performance of\nour proposed model against several widely-used benchmark\nmethods to rigorously assess the detection accuracy achieved.\nFinally, Section V concludes the paper by summarizing the\nkey findings, discussing the practical implications of our work,\nand outlining promising directions for future research in the\nfield of credit card fraud detection.\nII. R ELATED WORK\nThe detection of fraud in financial transactions has long\nbeen a pressing research challenge in the field of payment\nsystems. As the use of mobile payments, digital wallets, and\nweb-based payment methods has proliferated, accurately iden-\ntifying the small proportion of fraudulent transactions amidst\nthe vast volume of payment data has become an increasingly\ncritical issue.\nThe academic literature has accumulated a rich body of\nresearch addressing this problem. Bolton and Hand [4] pio-",
  "the vast volume of payment data has become an increasingly\ncritical issue.\nThe academic literature has accumulated a rich body of\nresearch addressing this problem. Bolton and Hand [4] pio-\nneered the use of statistical models to analyze and detect the\npresence of fraudulent activity. Building upon this foundation,\nStojanovi´c [27] subsequently employed more advanced neural\nnetwork models for fraud detection. Christoph [18] proposed\na machine learning approach involving feature vectorization\nand classification, providing an experimental framework for\nrelated studies. Furthermore, the work of Weng et al [32].\noffered valuable insights by demonstrating how data analysis\nand AI techniques can be leveraged to tackle data security\nand cybersecurity challenges, inspiring the focus of the current\nresearch.\nSimilar work has been done in other deep learning bio-\nlogical areas such as [14], [16] fine-tuning segment anything\nto obtain accurate forged regions [17] proposing a new style",
  "research.\nSimilar work has been done in other deep learning bio-\nlogical areas such as [14], [16] fine-tuning segment anything\nto obtain accurate forged regions [17] proposing a new style\nsampling module that improves the detection accuracy and\ngeneralization of the anomalous face classification task\nIn the realm of model selection, the success of large lan-\nguage models in other domains, as showcased by the research\nof Wang et al [23], [30]., has provided important guidance.\nTo ensure the efficacy of training and testing strategies, the\ndata cleaning and processing methods developed by Wei et al\n[31]. have been invaluable. Shen et al [26]s work on efficient\nclassification models has offered valuable experience in model\nselection, while Ding et al.’s [9] approach to data filtering,\nsegmentation, and augmentation has provided useful insights\nfor addressing sparse data challenges.\nIn their groundbreaking paper, Ni et al. (2023) present a",
  "segmentation, and augmentation has provided useful insights\nfor addressing sparse data challenges.\nIn their groundbreaking paper, Ni et al. (2023) present a\ncomprehensive comparison between traditional models and\ndeep learning approaches for heart rate time series forecasting\n[25].Their research demonstrates that transformer-based mod-\nels, particularly PatchTST, significantly outperform traditional\nmodels like ARIMA and Prophet in predicting heart rate\ndynamics [25].\nWhen it comes to experimental comparisons, the detailed\nevaluations of supervised learning, unsupervised learning, and\ndeep learning methods conducted by Kazeem et al [15].\nhave served as a useful reference. Additionally, the rigorous\nexperimental and analytical methodologies employed by Jha\net al [12]. have informed the meticulous implementation of\nthe current study.\nThis rich body of prior research has provided a solid foun-\ndation and has inspired novel ideas for our own exploration.",
  "et al [12]. have informed the meticulous implementation of\nthe current study.\nThis rich body of prior research has provided a solid foun-\ndation and has inspired novel ideas for our own exploration.\nBuilding upon these insights, we aim to contribute innovative\nmethods that advance the state of the art in credit card fraud\ndetection, offering more effective solutions for the industry.\nIII. M ETHODOLOGY\nIn Section III, we provide a detailed account of the method-\nology employed in this work. This includes a thorough dis-\ncussion of the data processing techniques utilized, such as\nensuring an equally distributed dataset. We also examine the\nrelevant prior research on feature engineering and dimension-\nality reduction, and analyze the impact these techniques had\non the data used in our study [35], [38]. By comprehensively\ndocumenting these methodological components, we aim to es-\ntablish transparency and enable the replication of our research\napproach.",
  "on the data used in our study [35], [38]. By comprehensively\ndocumenting these methodological components, we aim to es-\ntablish transparency and enable the replication of our research\napproach.\nThe primary focus of our methodology lies in accurately\nidentifying instances of credit card fraud in datasets where\nfraudulent samples are exceedingly rare. We conducted com-\nprehensive evaluations and explorations of various model\narchitectures [39], accompanied by detailed and thorough\ndata analysis and preprocessing. [13], [32] Additionally, we\nselected relevant loss functions and evaluation methods to\nensure the robustness and effectiveness of our approach. The",
  "study [25] has significantly influenced our research, guiding\nthe development of advanced predictive models in our work\nto handle complex temporal dependencies and non-linear\nrelationships.\nA. Dataset Introduction\nThe dataset used in this study consists of European credit\ncard transaction data. To ensure the timeliness of the ex-\nperiments and adapt to the latest payment environment, we\nutilized the most recent data available, encompassing over\n550,000 transaction records of European credit card users up\nto 2023. These data contain the latest processed variables,\nranging from V1 to V28. To enhance the informative value of\nthe data and increase the scientific validity of the comparisons,\nwe additionally incorporated European payment data from\n2013, thereby increasing the datasource size and improving\nthe meaningfulness of our experimental comparisons. Among\nthe dataset’s 284,804 transactions, 492 were identified as\nfraudulent, accounting for 0.172% of the total. All these data",
  "the meaningfulness of our experimental comparisons. Among\nthe dataset’s 284,804 transactions, 492 were identified as\nfraudulent, accounting for 0.172% of the total. All these data\noriginate from the same source and possess variables V1 to\nV28, which have been processed and converted to float values.\nB. Data Processing\nWe first resample and preprocess the dataset to address the\nissue of data imbalance. By randomly sampling and merging, a\nnew balanced dataset is generated, containing an equal number\nof fraudulent and non-fraudulent transaction samples. The\nnew data set is then shuffled to ensure the randomness and\nreliability of the samples.\n1) Equally Distributing and Correlating: To generate an\nequally distributed dataset, the initial step involves displaying\ntextual output showcasing the proportions of class distribution\nto confirm the balance of the dataset Following this, a bar plot\nis generated utilizing Seaborn’s countplot function to visually",
  "textual output showcasing the proportions of class distribution\nto confirm the balance of the dataset Following this, a bar plot\nis generated utilizing Seaborn’s countplot function to visually\ndepict the distribution of classes. This visualization illustrates\nan equal distribution of classes, thereby underscoring the\neffectiveness of the subsampling technique in achieving class\nbalance.\n2) Feature Correlation Analysi: In our analysis, we metic-\nulously compared the original imbalanced dataset with its\nsubsampled counterpart, unearthing a plethora of advantages\nstemming from data balancing. This encompassed a noticeable\nboost in model performance, mitigated susceptibility to over-\nfitting, heightened predictive stability, and augmented model\ninterpretability. The correlation efficacy visibly escalated as\nevidenced by the comparison, underscoring the tangible bene-\nfits derived from this approach. Correspond info please check\nFig1 and Fig2.",
  "evidenced by the comparison, underscoring the tangible bene-\nfits derived from this approach. Correspond info please check\nFig1 and Fig2.\n3) Outlier Detection: To better understand and handle\noutliers in the data, we first conducted a visual analysis\nof the data. Specifically, for features V14, V12, and V10,\nwe plotted the distribution of these features in fraudulent\ntransactions and fitted a normal distribution curve. Using the\nseaborn and matplotlib libraries, we created three side-by-side\nsubplots, each showing the distribution of a single feature. By\nobserving the distribution plots, we were able to preliminarily\nFig. 1. Imbalanced Correlation matrix.\nFig. 2. Sub-sampled correlation matrix.\nidentify outliers in the data. This step of visual analysis\nhelps to intuitively understand how these features manifest in\nfraudulent transactions as Fig3.\nAfter identifying potential outliers, we formally handled\nthese outliers using the Interquartile Range (IQR) method. The",
  "fraudulent transactions as Fig3.\nAfter identifying potential outliers, we formally handled\nthese outliers using the Interquartile Range (IQR) method. The\nspecific steps are as follows:\nCalculate quartiles and interquartile range: First, calculate\nthe 25th percentile (Q1) and 75th percentile (Q3) of the feature\nvalues. Then, calculate the interquartile range (IQR = Q3 -\nQ1). Determine the upper and lower bounds for outliers: Use\n1.5 times the IQR to determine the upper and lower bounds for\noutliers, i.e., the lower bound is Q1 - 1.5 * IQR, and the upper\nbound is Q3 + 1.5 * IQR. Remove outliers: Consider feature\nvalues that are lower than the lower bound or higher than\nthe upper bound as outliers and remove these outliers from\nthe dataset. For example, for feature V14, we first calculate\nits quartiles and interquartile range, then determine the upper\nand lower bounds for outliers, and finally remove outliers\nFig. 3. Outlier Detection for V14 and V12 feature.",
  "Fig. 4. Outlier Removal for V14 and V12 feature.\nthat exceed these ranges. The same method is also applied to\nfeatures V12 and V10. To verify the effect of outlier handling,\nwe again conducted a visual analysis of the processed data,\nusing box plots to show the distribution of features V14, V12,\nand V10 in fraudulent and non-fraudulent transactions. Box\nplots not only display the central tendency and dispersion of\nthe data but also visually show outliers as Fig 4.\nBy comparing the distribution plots before and after pro-\ncessing, we can observe the reduction of outliers and their\nimpact on the data distribution. This step further verifies the\neffectiveness of our outlier handling method and provides\na cleaner and more reliable dataset for subsequent model\ntraining\n4) Dimensionality Reduction: To achieve optimal dimen-\nsionality reduction for the data, three different techniques—T-\nSNE, PCA, and Truncated SVD—are employed to map the",
  "training\n4) Dimensionality Reduction: To achieve optimal dimen-\nsionality reduction for the data, three different techniques—T-\nSNE, PCA, and Truncated SVD—are employed to map the\ndata onto a two-dimensional space. This facilitates a visual\nrepresentation of the distribution of data points on scatter plots,\naiding in a better understanding of the data’s structure and\nclustering tendencies.\na) T-SNE: T-Distributed Stochastic Neighbor Embed-\nding, is a technique for reducing the dimensionality of high-\ndimensional data, particularly when the goal is to represent the\ndata in a low-dimensional space of two or three dimensions.\nUnlike linear dimensionality reduction methods, T-SNE is ca-\npable of capturing and preserving the complex nonlinear struc-\ntures and local relationships inherent in high-dimensional data.\nThe algorithm works by representing each high-dimensional\ndata point as a corresponding low-dimensional point in a way",
  "tures and local relationships inherent in high-dimensional data.\nThe algorithm works by representing each high-dimensional\ndata point as a corresponding low-dimensional point in a way\nthat ensures similar objects are represented by nearby points\nand dissimilar objects are represented by distant points with\na high degree of probability. This unique ability to maintain\nthe intricate patterns and local similarities of the original\ndata makes T-SNE an invaluable tool for data visualization\nand clustering analysis, providing researchers with a clear\nand intuitive understanding of the underlying structure and\nrelationships within the high-dimensional data.\nb) PCA: PCA (Principal Component Analysis) is a linear\ndimensionality reduction technique that transforms the orig-\ninal data into a new coordinate system where the greatest\nvariances by any projection of the data come to lie on the\nfirst coordinates (called principal components), the second",
  "inal data into a new coordinate system where the greatest\nvariances by any projection of the data come to lie on the\nfirst coordinates (called principal components), the second\nFig. 5. Comparison of T-SNE, PCA, and Truncated SVD.\nFig. 6. Detailed view of SVD results.\ngreatest variances on the second coordinates, and so on. PCA\nis efficient and widely used for reducing the dimensionality of\ndatasets while preserving as much variability as possible.\nc) Truncated SVD: Truncated SVD (Singular Value De-\ncomposition) is technique that decomposes a matrix into three\nother matrices. It is particularly useful for sparse data and is\noften used in the context of Latent Semantic Analysis (LSA)\nin natural language processing. Unlike PCA, Truncated SVD\ndoes not center the data before performing the decomposition,\nmaking it suitable for term-document matrices and other non-\ncentered data.\nCompared to PCA and Truncated SVD, T-SNE excels in\npreserving the nonlinear structures and local relationships",
  "making it suitable for term-document matrices and other non-\ncentered data.\nCompared to PCA and Truncated SVD, T-SNE excels in\npreserving the nonlinear structures and local relationships\nwithin high-dimensional data. Consequently, T-SNE offers su-\nperior advantages in data visualization and clustering analysis.\nThe specific comparison results are illustrated in Figure 5 and\nFigure 6.\nC. Model architectures\n1) Transformer Structure: The Transformer architecture\nemployed in this study primarily consists of a Self-Attention\nMechanism and a Feed-Forward Neural Network.\na) Self-Attention Mechanism: The Self-Attention Mecha-\nnism is utilized to compute attention weights for each position\nin the input sequence, capturing dependencies between differ-\nent positions. In the Transformer Encoder, the Self-Attention\nMechanism is referred to as Multi-Head Attention. Below is\nthe mathematical formulation of Multi-Head Attention:\nGiven an input sequence X = {x1, x2, . . . , xn}, we first",
  "Mechanism is referred to as Multi-Head Attention. Below is\nthe mathematical formulation of Multi-Head Attention:\nGiven an input sequence X = {x1, x2, . . . , xn}, we first\nmap the input sequence to Query ( Q), Key ( K), and Value\n(V ) spaces. By matrix multiplication and scaling, we obtain\nQuery (Q), Key ( K), and Value ( V ) tensors:\nQ = XWQ K = XWK V = XWV\nwhere WQ, WK, and WV are learnable weight matrices.",
  "Fig. 7. Structure of attention module.\nNext, we compute the attention weights ( A) by taking the\ndot product of Query and Key [28], followed by scaling and\nsoftmax normalization:\nA = softmax\n\u0012QKT\n√dk\n\u0013\nWhere dk is the dimensionality of Query and Key.\nFinally, we multiply the attention weights with the values\nand then perform a weighted sum to obtain the output of the\nSelf-Attention Mechanism:\nAttention(X) =AV\nThe Multi-Head Attention mechanism calculates the outputs\nof multiple attention heads by using independent Query, Key,\nand Value projection matrices. These outputs are concatenated\nalong the last dimension to generate the final result of the Self-\nAttention Mechanism.\nb) Operations in Transformer Encoder Layer: In the\nTransformer encoder layer, we apply the self-attention mech-\nanism to the input sequence, followed by a fully connected\nfeed-forward neural network.\nIn the self-attention mechanism, we use the input sequence\nas queries, keys, and values and compute attention weights",
  "feed-forward neural network.\nIn the self-attention mechanism, we use the input sequence\nas queries, keys, and values and compute attention weights\nfor each position with respect to other positions. Then, these\nattention weights are used to weight-sum the values of the\ninput sequence, resulting in the output of the self-attention\nmechanism.\nNext, the output of the self-attention mechanism is trans-\nformed through a feed-forward neural network, and the re-\nsult is added back to the input (using residual connections),\nfollowed by layer normalization. Mathematically, it can be\nrepresented as:\nEncoderLayer(X) =LayerNorm(X + Attention(X))\n+ FeedForward(X)\nHere, X represents the input sequence, Attention represents\nthe self-attention mechanism, and FeedForward represents the\nfeed-forward neural network.\nHere is the Structure of our implemented model. Figure 6:\nIV. E VALUATION\nIn this section, we first introduce the evaluation metrics, and\nthen present our experiment results.",
  "Here is the Structure of our implemented model. Figure 6:\nIV. E VALUATION\nIn this section, we first introduce the evaluation metrics, and\nthen present our experiment results.\nA. Evaluation Metrics\nWe use Precision, Recall, F1-score, and ROC AUC to\nassess our classification model. These metrics offer a thorough\nevaluation from different perspectives.\na) Precision: Precision calculates the rate of true posi-\ntive predictions among all positive predictions, indicating the\nmodel’s accuracy in identifying positives [10].\nb) Recall: Recall, or sensitivity, calculates the proportion\nof true positives detected among all actual positives, showing\nthe model’s ability to capture positive cases.\nc) F1-score: The F1-score provides a balanced measure\nof the model’s accuracy in predicting positives and capturing\nactual positives.\nd) ROC AUC: ROC AUC evaluates the model’s ability to\ndistinguish between classes at various thresholds. It represents",
  "of the model’s accuracy in predicting positives and capturing\nactual positives.\nd) ROC AUC: ROC AUC evaluates the model’s ability to\ndistinguish between classes at various thresholds. It represents\nthe area under the ROC curve, where a higher AUC indicates\nbetter discrimination between positive and negative instances.\nThe choice of metrics depends on the problem’s require-\nments, with some scenarios prioritizing recall and others preci-\nsion. The F1-score and ROC AUC offer balanced evaluations,\nconsidering both aspects.\ne) F1-score: F1-score is the harmonic mean of precision\nand recall, providing a balanced measure that combines both\nmetrics’ performance. It can be represented by the following\nformula:\nF1-score = 2 · Precision · Recall\nPrecision + Recall\nB. Results\nTo fairly and accurately select the best fraud detection\nmodel, we carefully curated several different classification\nmodels. The models designed for this experiment include Lo-",
  "B. Results\nTo fairly and accurately select the best fraud detection\nmodel, we carefully curated several different classification\nmodels. The models designed for this experiment include Lo-\ngistic Regression, K-Nearest Neighbors, SVC, Decision Tree\nClassifier, and Neural Network. Additionally, to ensure that\nthe models used in our experiments remain highly practical in\ncomparison to the currently used models in the same scenarios,\nwe included comparisons with the well-known and excellent\nmodel XGBoost and the deep learning-based TabNet.\nOur experiments are divided into two stages. First, we\nused the latest data from 2023 to fairly compare the overall\nperformance of all models, including Recall, Precision, F1\nScore, and ROC AUC. To further validate the effectiveness of\nour models, we used the same models and processing methods\nto compare with the European fraud data covering both 2023\nand 2013. Our aim is to verify whether our new models have",
  "our models, we used the same models and processing methods\nto compare with the European fraud data covering both 2023\nand 2013. Our aim is to verify whether our new models have\nan advantage over classic and well-known modern models, and\nwhether this advantage is up-to-date and broadly applicable\nthrough repeated comparisons.\nAs shown in Table I and Fig 8, the Transformer model\nhas demonstrated remarkable performance in this experiment,\noutperforming other models such as XGBoost and TabNet\nacross various evaluation metrics. Firstly, the Transformer",
  "TABLE I\nEXPERIMENT WITH 2023 D ATA.\nClassifier Precision Recall F1 Score ROC AUC\nLogistic Regression 0.93 0.93 0.93 0.98\nKNN 0.93 0.93 0.93 0.98\nSVM 0.91 0.91 0.91 0.99\nDecision Tree 0.93 0.93 0.93 0.93\nNeural Network 0.92 0.91 0.91 0.96\nXGBoost 0.95 0.95 0.95 0.99\nTabNet 0.93 0.93 0.93 0.98\nTransformer 0.998 0.998 0.998 0.99\nFig. 8. ROC and AUC Scores of Various Classifiers on 2023 Data.\nachieves significantly higher Precision and Recall scores of\n0.998, surpassing XGBoost (0.95), TabNet (0.93), and Neural\nNetwork (0.92 and 0.91) [29]. This indicates that the Trans-\nformer excels at correctly identifying positive samples while\nminimizing false positives and false negatives.\nMoreover, the Transformer’s F1 Score, which balances Pre-\ncision and Recall, reaches an impressive 0.998, considerably\nhigher than XGBoost (0.95), TabNet (0.93), and Neural Net-\nwork (0.91) [8]. This showcases the Transformer’s ability to\nmaintain a perfect equilibrium between Precision and Recall,",
  "higher than XGBoost (0.95), TabNet (0.93), and Neural Net-\nwork (0.91) [8]. This showcases the Transformer’s ability to\nmaintain a perfect equilibrium between Precision and Recall,\nresulting in superior overall performance.\nTo further validate the superior performance of the Trans-\nformer model in the domain of fraud detection, we conducted a\ncross-validation using data from 2013. The results demonstrate\nthat the Transformer model consistently outperforms other\nmodels on the 2013 data, achieving significantly higher scores\nin Precision, Recall, F1-score, and ROC AUC.\nTABLE II\nPERFORMANCE METRICS OF VARIOUS CLASSIFIERS ON 2013 D ATA\nModel Precision Recall F1-score ROC AUC\nKNN 0.93 0.92 0.92 0.95\nSVM 0.93 0.93 0.93 0.95\nDecision Tree 0.91 0.90 0.89 0.85\nLogistic Regression 0.96 0.96 0.96 0.95\nNeural Network 0.975 0.999 0.988 0.85\nXGBoost 0.96 0.90 0.92 0.96\nTabNet 0.59 0.77 0.67 0.48\nTransformer 0.998 0.998 0.998 0.98\nFirstly, the Transformer model attains a Precision and Recall",
  "Neural Network 0.975 0.999 0.988 0.85\nXGBoost 0.96 0.90 0.92 0.96\nTabNet 0.59 0.77 0.67 0.48\nTransformer 0.998 0.998 0.998 0.98\nFirstly, the Transformer model attains a Precision and Recall\nof 0.998 on the 2013 data, which is consistent with its perfor-\nmance on the 2023 data. This indicates that the Transformer\nFig. 9. ROC AUC Scores of Various Classifiers on 2013 Data.\nmodel maintains a high level of accuracy and recall across\ndifferent time periods, proving its excellent generalization\nability and stability [29]. In contrast, although the Neural\nNetwork achieves a high Recall (0.999) on the 2013 data, its\nPrecision (0.975) is slightly lower than that of the Transformer,\nsuggesting that it is less effective in reducing false positives.\nSecondly, the Transformer model’s F1-score on the 2013\ndata also reaches 0.998, significantly surpassing other models.\nThis further confirms the Transformer’s superiority in bal-\nancing Precision and Recall . It is worth noting that TabNet",
  "data also reaches 0.998, significantly surpassing other models.\nThis further confirms the Transformer’s superiority in bal-\nancing Precision and Recall . It is worth noting that TabNet\nperforms poorly on the 2013 data, with an F1-score of only\n0.67. This may be attributed to TabNet’s sensitivity to changes\nin data distribution, resulting in unstable performance across\ndifferent time periods.\nMoreover, the Transformer model achieves an ROC AUC\nof 0.98 on the 2013 data, which is only slightly lower than\nXGBoost’s 0.96. However, considering its performance in\nother metrics, the Transformer still demonstrates the best\noverall performance. This further validates the Transformer’s\nadvantage in capturing complex feature interactions and long-\nrange dependencies, enabling it to maintain consistently high\nperformance on data from different time periods .\nIn conclusion, the cross-validation using 2013 data further\nconfirms the superior performance of the Transformer model",
  "performance on data from different time periods .\nIn conclusion, the cross-validation using 2013 data further\nconfirms the superior performance of the Transformer model\nin the field of fraud detection. Its outstanding Precision,\nRecall, F1-score, and ROC AUC, as well as its stable perfor-\nmance across different time periods, can be attributed to its\nunique self-attention mechanism and pretraining-fine-tuning\nparadigm. This makes the Transformer model an ideal choice\nfor addressing complex fraud detection problems and provides\nstrong support for its application in other domains.\nV. C ONCLUSION\nIn this experiment, we successfully adapted and optimized\nthe Transformer model for fraud detection, achieving remark-\nable results. The experimental outcomes demonstrate that our\nTransformer model significantly outperforms other classical\nmodels, such as XGBoost, TabNet, and Neural Network,\nacross multiple evaluation metrics, including Precision, Recall,",
  "Transformer model significantly outperforms other classical\nmodels, such as XGBoost, TabNet, and Neural Network,\nacross multiple evaluation metrics, including Precision, Recall,\nF1-score, and ROC AUC. By conducting cross-validation\non data from two different time periods, 2013 and 2023,",
  "we further confirmed the Transformer model’s exceptional\ngeneralization ability and stability.\nThe significance of these results is twofold. First, it validates\nthat Transformer models not only excel in general artificial in-\ntelligence applications like ChatGPT but also possess tremen-\ndous potential in classification tasks, surpassing traditional,\nclassical approaches. Second, our innovative attempt to apply\nthe Transformer in fraud detection lays a solid foundation for\nthe future development of more optimized Transformer-based\nmodels, promising to enhance financial security measures\nfurther.\nThe success of our experiment can be attributed to our\nextensive efforts in understanding the underlying principles\nof the Transformer and adjusting its Encoder layers. Despite\nthe considerable challenge of adapting the Transformer, ini-\ntially designed for natural language processing tasks, into a\nclassification model suitable for fraud detection, our model",
  "the considerable challenge of adapting the Transformer, ini-\ntially designed for natural language processing tasks, into a\nclassification model suitable for fraud detection, our model\nultimately lived up to expectations, demonstrating outstanding\naccuracy across all evaluation metrics. This result not only\nproves the superior performance of the Transformer model in\nthe fraud detection domain but also provides strong support\nfor its application in other fields.\nIn the future research, our focus will remain on investigating\nthe potential of Transformer models in fraud detection and\nbeyond [19], [22]. Our goal is to create models that are not\nonly more efficient and accurate but also more resilient to\nvarious challenges. Moreover, we believe that the findings\nof this experiment will serve as a valuable resource for\nprofessionals in the industry, encouraging the adoption and\nadvancement of AI-driven solutions in the realm of financial\nsecurity and other critical areas.\nREFERENCES",
  "professionals in the industry, encouraging the adoption and\nadvancement of AI-driven solutions in the realm of financial\nsecurity and other critical areas.\nREFERENCES\n[1] Amira Abdallah, Mohd Aizaini Maarof, and Anazida Zainal. Fraud\ndetection system: A survey. Journal of Network and Computer Appli-\ncations, 68:90–113, 2016.\n[2] U ˘gur Akbulut, Mehmet Akif Cifci, and Zafer Aslan. Hybrid modeling\nfor stream flow estimation: integrating machine learning and federated\nlearning. Applied Sciences, 13(18):10203, 2023.\n[3] Sercan O Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular\nlearning. arXiv preprint arXiv:1908.07442 , 2021.\n[4] Richard J Bolton and David J Hand. Statistical fraud detection: A review.\nStatistical science, 17(3):235–255, 2002.\n[5] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting\nsystem. In Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , pages 785–794,\n2016.",
  "system. In Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining , pages 785–794,\n2016.\n[6] Zhengyu Chen, Jixie Ge, Heshen Zhan, Siteng Huang, and Donglin\nWang. Pareto self-supervised training for few-shot learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13663–13672, 2021.\n[7] Zhengyu Chen, Teng Xiao, and Kun Kuang. Ba-gnn: On learning\nbias-aware graph neural network. In 2022 IEEE 38th International\nConference on Data Engineering (ICDE) , pages 3012–3024. IEEE,\n2022.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805 , 2018.\n[9] Zhicheng Ding, Panfeng Li, Qikai Yang, Siyang Li, and Qingtian Gong.\nRegional style and color transfer. arXiv preprint arXiv:2404.13880 ,\n2024.\n[10] Zenggui Gao, Ruining Yang, Kai Zhao, Wenhua Yu, Zheng Liu, and",
  "Regional style and color transfer. arXiv preprint arXiv:2404.13880 ,\n2024.\n[10] Zenggui Gao, Ruining Yang, Kai Zhao, Wenhua Yu, Zheng Liu, and\nLilan Liu. Hybrid convolutional neural network approaches for recogniz-\ning collaborative actions in human–robot assembly tasks. Sustainability,\n16(1):139, 2023.\n[11] Nan Huang, Yiwen Chen, and Jianming Shen. Tabnet-based interpretable\nfraud detection for imbalanced data. In 2021 IEEE 8th International\nConference on Data Science and Advanced Analytics (DSAA) , pages\n1–6. IEEE, 2021.\n[12] Sanjeev Jha. Credit card fraud detection with discrete choice models\nand misclassified transactions . University of Illinois at Chicago, 2009.\n[13] Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, and Marco Pavone.\nLearning from teaching regularization: Generalizable correlations should\nbe easy to imitate. arXiv preprint arXiv:2402.02769 , 2024.\n[14] Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu,",
  "Learning from teaching regularization: Generalizable correlations should\nbe easy to imitate. arXiv preprint arXiv:2402.02769 , 2024.\n[14] Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu,\nLigong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, and\nDimitris N Metaxas. Apeer: Automatic prompt engineering enhances\nlarge language model reranking. arXiv preprint arXiv:2406.14449, 2024.\n[15] Oladimeji Kazeem. Fraud detection using machine learning.\n[16] Yingxin Lai, Zhiming Luo, and Zitong Yu. Detect any deepfakes:\nSegment anything meets face forgery detection and localization. In\nChinese Conference on Biometric Recognition, pages 180–190. Springer,\n2023.\n[17] Yingxin Lai, Guoqing Yang, Yifan He, Zhiming Luo, and Shaozi Li.\nSelective domain-invariant feature for generalizable deepfake detection.\nIn ICASSP 2024-2024 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 2335–2339. IEEE, 2024.",
  "In ICASSP 2024-2024 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 2335–2339. IEEE, 2024.\n[18] Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling.\nAttribute-based classification for zero-shot visual object categorization.\nIEEE Trans. Pattern Anal. Mach. Intell. , 36(3):453–465, 2014.\n[19] Guangchen Lan, Xiao-Yang Liu, Yijing Zhang, and Xiaodong Wang.\nCommunication-efficient federated learning for resource-constrained\nedge devices. IEEE Transactions on Machine Learning in Communica-\ntions and Networking , 1:210–224, 2023.\n[20] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning.\nNature, 521(7553):436–444, 2015.\n[21] Lei Li, Tianfang Zhang, Zhongfeng Kang, and Xikun Jiang. Mask-fpan:\nSemi-supervised face parsing in the wild with de-occlusion and uv gan.\nComputers & Graphics , 116:185–193, 2023.\n[22] Xinjin Li, Jinghao Chang, Tiexin Li, Wenhan Fan, Yu Ma, and Haowei\nNi. A vehicle classification method based on machine learning.",
  "Computers & Graphics , 116:185–193, 2023.\n[22] Xinjin Li, Jinghao Chang, Tiexin Li, Wenhan Fan, Yu Ma, and Haowei\nNi. A vehicle classification method based on machine learning.\nPreprints, July 2024.\n[23] Weimin Lyu, Songzhu Zheng, Haibin Ling, and Chao Chen. Backdoor\nattacks against transformers with attention enhancement. In ICLR 2023\nWorkshop on Backdoor Attacks and Defenses in Machine Learning ,\n2023.\n[24] Weimin Lyu, Songzhu Zheng, Lu Pang, Haibin Ling, and Chao Chen.\nAttention-enhancing backdoor attacks against bert-based models. In\nFindings of the Association for Computational Linguistics: EMNLP\n2023, pages 10672–10690, 2023.\n[25] Haowei Ni, Shuchen Meng, Xieming Geng, Panfeng Li, Zhuoying Li,\nXupeng Chen, Xiaotong Wang, and Shiyao Zhang. Time series modeling\nfor heart rate prediction: From arima to transformers. arXiv preprint\narXiv:2406.12199, 2024.\n[26] Xinyu Shen, Qimin Zhang, Huili Zheng, and Weiwei Qi. Harnessing",
  "for heart rate prediction: From arima to transformers. arXiv preprint\narXiv:2406.12199, 2024.\n[26] Xinyu Shen, Qimin Zhang, Huili Zheng, and Weiwei Qi. Harnessing\nxgboost for robust biomarker selection of obsessive-compulsive disorder\n(ocd) from adolescent brain cognitive development (abcd) data. Re-\nsearchGate, May 2024.\n[27] Branka Stojanovi ´c, Josip Bo ˇzi´c, Katharina Hofer-Schmitz, Kai\nNahrgang, Andreas Weber, Atta Badii, Maheshkumar Sundaram, Elliot\nJordan, and Joel Runevic. Follow the trail: Machine learning for fraud\ndetection in fintech applications. Sensors, 21(5):1594, 2021.\n[28] Tao Sun, Yongjun Xu, Zhao Zhang, Lin Wu, and Fei Wang. A\nhierarchical spatial-temporal embedding method based on enhanced\ntrajectory features for ship type classification. Sensors, 22(3):711, 2022.\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention",
  "[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Advances in Neural Information Processing Systems ,\npages 5998–6008, 2017.\n[30] Yuchen Wang, Yin-Shan Lin, Ruixin Huang, Jinyin Wang, and Sensen\nLiu. Enhancing user experience in large language models through\nhuman-centered design: Integrating theoretical insights with an exper-\nimental study to meet diverse software learning needs with a single\ndocument knowledge base. Computing and Artificial Intelligence ,\n2(1):535–535, 2024.",
  "[31] Song Wei, Andrea Coletta, Svitlana Vyetrenko, and Tucker Balch.\nIntags: Interactive agent-guided simulation. In NeurIPS 2023 Workshop\non Synthetic Data Generation with Generative AI , 2023.\n[32] Yijie Weng and Jianhao Wu. Fortifying the global data fortress: a\nmultidimensional examination of cyber security indexes and data pro-\ntection measures across 193 nations. International Journal of Frontiers\nin Engineering Technology, 6(2), 2024.\n[33] Yufeng Xie, Guixian Liu, and Chengxiang Yan. An improved xgboost\nalgorithm for imbalanced data in credit card fraud detection. In 2019\nIEEE 4th International Conference on Cloud Computing and Big Data\nAnalysis (ICCCBDA), pages 116–120. IEEE, 2019.\n[34] Wei Xu, Jianlong Chen, Zhicheng Ding, and Jinyin Wang. Text senti-\nment analysis and classification based on bidirectional gated recurrent\nunits (grus) model. arXiv preprint arXiv:2404.17123 , 2024.\n[35] Shiqi Yang, Yu Zhao, and Haoxiang Gao. Using large language models",
  "units (grus) model. arXiv preprint arXiv:2404.17123 , 2024.\n[35] Shiqi Yang, Yu Zhao, and Haoxiang Gao. Using large language models\nin real estate transactions: A few-shot learning approach, May 2024.\n[36] Jiajie Yuan, Linxiao Wu, Yulu Gong, Zhou Yu, Ziang Liu, and Shuyao\nHe. Research on intelligent aided diagnosis system of medical image\nbased on computer deep learning. arXiv preprint arXiv:2404.18419 ,\n2024.\n[37] Qimin Zhang, Weiwei Qi, Huili Zheng, and Xinyu Shen. Cu-net: a\nu-net architecture for efficient brain-tumor segmentation on brats 2019\ndataset, 2024.\n[38] Yu Zhao and Haoxiang Gao. Utilizing large language models for\ninformation extraction from real estate transactions. arXiv preprint\narXiv:2404.18043, 2024.\n[39] Yu Zhao, Haoxiang Gao, and shiqi yang. Utilizing large language models\nto analyze common law contract formation, Jun 2024."
]